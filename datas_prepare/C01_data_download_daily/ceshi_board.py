import pandas as pd
import requests
import time
import random
from typing import List, Dict
from bs4 import BeautifulSoup
from datetime import datetime
import logging

# å¯¼å…¥ä½ çš„é…ç½®
import CommonProperties.Base_Properties as base_properties
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties.DateUtility import DateUtility


class THSConceptCrawler:
    """åŒèŠ±é¡ºæ¦‚å¿µæ¿å—çˆ¬è™«ï¼ˆç”Ÿäº§çº§å®‰å…¨ç‰ˆæœ¬ï¼‰"""

    def __init__(self):
        # MySQLé…ç½®
        self.mysql_config = {
            'user': base_properties.origin_mysql_user,
            'password': base_properties.origin_mysql_password,
            'host': base_properties.origin_mysql_host,
            'database': base_properties.origin_mysql_database
        }

        # å®‰å…¨é…ç½®
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Referer': 'https://q.10jqka.com.cn/',
        })

        # å»¶è¿Ÿé…ç½®ï¼ˆå®‰å…¨ç¬¬ä¸€ï¼‰
        self.page_delay = 5.0  # é¡µé—´å»¶è¿Ÿ
        self.concept_delay = 8.0  # æ¦‚å¿µé—´å»¶è¿Ÿ
        self.batch_delay = 25.0  # æ‰¹æ¬¡é—´å»¶è¿Ÿ

        # ç®€æ´æ—¥å¿—
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(message)s',
            datefmt='%H:%M:%S'
        )
        self.logger = logging.getLogger(__name__)

    def run(self, batch_size: int = 15, test_mode: bool = False):
        """ä¸»æ‰§è¡Œå‡½æ•°"""
        print("=" * 70)
        print("åŒèŠ±é¡ºæ¦‚å¿µæ¿å—è‚¡ç¥¨çˆ¬è™« - å®‰å…¨æ¨¡å¼")
        print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"å»¶è¿Ÿé…ç½®: é¡µé—´{self.page_delay}s | æ¦‚å¿µé—´{self.concept_delay}s | æ‰¹æ¬¡é—´{self.batch_delay}s")
        print("=" * 70)

        # 1. è·å–æ¦‚å¿µæ¿å—
        print("è·å–æ¦‚å¿µæ¿å—æ•°æ®...")
        concepts_df = self._get_concepts()

        if concepts_df.empty:
            print("âŒ æœªè·å–åˆ°æ¦‚å¿µæ¿å—æ•°æ®")
            return

        if test_mode:
            concepts_df = concepts_df.head(5)  # æµ‹è¯•æ¨¡å¼åªå–5ä¸ª

        total_concepts = len(concepts_df)
        print(f"ğŸ“‹ å…± {total_concepts} ä¸ªæ¦‚å¿µæ¿å—")

        # 2. åˆ†æ‰¹å¤„ç†
        all_success = 0
        total_records = 0

        for batch_idx, batch_start in enumerate(range(0, total_concepts, batch_size), 1):
            batch_end = min(batch_start + batch_size, total_concepts)
            batch = concepts_df.iloc[batch_start:batch_end]

            print(f"\nğŸ“¦ æ‰¹æ¬¡ {batch_idx} - æ¦‚å¿µ {batch_start + 1} åˆ° {batch_end}")

            batch_data = []
            batch_success = 0

            # å¤„ç†æ‰¹æ¬¡å†…çš„æ¯ä¸ªæ¦‚å¿µ
            for idx, (_, row) in enumerate(batch.iterrows(), 1):
                code = row['board_code']
                name = row['board_name']

                # æ¦‚å¿µé—´å»¶è¿Ÿï¼ˆå¸¦éšæœºæŠ–åŠ¨ï¼‰
                if idx > 1:
                    delay = self.concept_delay * random.uniform(0.9, 1.1)
                    time.sleep(delay)

                try:
                    stocks = self._crawl_single_concept(code, name)

                    if stocks:
                        batch_data.extend(stocks)
                        batch_success += 1
                        all_success += 1
                        total_records += len(stocks)
                        print(f"  âœ“ {name[:18]:18s} ({code}): {len(stocks):3d} åª")
                    else:
                        print(f"  - {name[:18]:18s} ({code}): æ— æ•°æ®")

                except Exception as e:
                    print(f"  âœ— {name[:18]:18s} ({code}): é”™è¯¯")
                    time.sleep(10)  # é”™è¯¯åé¢å¤–ç­‰å¾…

            # 3. å†™å…¥å½“å‰æ‰¹æ¬¡æ•°æ®
            if batch_data:
                self._save_batch_to_mysql(batch_data)
                print(f"  ğŸ’¾ æ‰¹æ¬¡å†™å…¥: {len(batch_data)} æ¡è®°å½•")

            # 4. æ‰¹æ¬¡ç»Ÿè®¡
            print(f"  ğŸ“Š æ‰¹æ¬¡å®Œæˆ: {batch_success}/{len(batch)} ä¸ªæ¦‚å¿µ")
            print(f"  ğŸ“ˆ ç´¯è®¡è¿›åº¦: {all_success}/{total_concepts} ä¸ªæ¦‚å¿µ | {total_records:,} æ¡è®°å½•")

            # 5. æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆé‡è¦ï¼ï¼‰
            if batch_end < total_concepts:
                print(f"  â³ æ‰¹æ¬¡é—´éš” {self.batch_delay} ç§’...")
                time.sleep(self.batch_delay)

        # æœ€ç»ˆç»Ÿè®¡
        print("\n" + "=" * 70)
        print("ğŸ‰ çˆ¬å–ä»»åŠ¡å®Œæˆ!")
        print(f"âœ… æˆåŠŸæ¦‚å¿µ: {all_success}/{total_concepts}")
        print(f"ğŸ“Š æ€»è®°å½•æ•°: {total_records:,}")
        print(f"â° ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    def _get_concepts(self):
        """ä»MySQLè·å–æ¦‚å¿µæ¿å—"""
        try:
            return mysql_utils.data_from_mysql_to_dataframe_latest(
                user=self.mysql_config['user'],
                password=self.mysql_config['password'],
                host=self.mysql_config['host'],
                database=self.mysql_config['database'],
                table_name="ods_akshare_board_concept_name_ths",
                cols=['board_code', 'board_name']
            )
        except Exception as e:
            print(f"è·å–æ¦‚å¿µæ¿å—å¤±è´¥: {e}")
            return pd.DataFrame()

    def _crawl_single_concept(self, code: str, name: str) -> List[Dict]:
        """çˆ¬å–å•ä¸ªæ¦‚å¿µçš„æ‰€æœ‰è‚¡ç¥¨"""
        all_stocks = []

        # è·å–æ€»é¡µæ•°
        try:
            total_pages = self._get_total_pages(code)
        except:
            total_pages = 1

        if total_pages == 0:
            return []

        # é€é¡µçˆ¬å–
        for page in range(1, total_pages + 1):
            # é¡µé—´å»¶è¿Ÿ
            if page > 1:
                delay = self.page_delay * random.uniform(0.8, 1.2)
                time.sleep(delay)

            page_stocks = self._crawl_single_page(code, page)

            if not page_stocks:
                break

            # æ ¼å¼åŒ–æ•°æ®
            today = datetime.now().strftime('%Y-%m-%d')
            for stock in page_stocks:
                all_stocks.append({
                    'ymd': today,
                    'board_code': code,
                    'board_name': name,
                    'stock_code': stock.get('ä»£ç ', ''),
                    'stock_name': stock.get('åç§°', '')
                })

        return all_stocks

    def _get_total_pages(self, code: str) -> int:
        """è·å–æ€»é¡µæ•°"""
        url = f"https://q.10jqka.com.cn/gn/detail/code/{code}/"

        for attempt in range(2):  # é‡è¯•ä¸€æ¬¡
            try:
                response = self.session.get(url, timeout=15)
                response.encoding = 'gbk'

                # æ£€æŸ¥é™åˆ¶
                if any(keyword in response.text for keyword in ["è®¿é—®é™åˆ¶", "è¯·ç¨åå†è¯•", "é¢‘ç‡è¿‡å¿«"]):
                    print("    âš ï¸  æ£€æµ‹åˆ°é™åˆ¶ï¼Œç­‰å¾…15ç§’...")
                    time.sleep(15)
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')

                # æŸ¥æ‰¾åˆ†é¡µä¿¡æ¯
                page_info = soup.find('span', class_='page_info')
                if page_info:
                    text = page_info.get_text(strip=True)
                    if '/' in text:
                        return int(text.split('/')[1])

                return 1

            except Exception as e:
                if attempt == 0:
                    time.sleep(8)

        return 1

    def _crawl_single_page(self, code: str, page: int) -> List[Dict]:
        """çˆ¬å–å•é¡µ"""
        if page == 1:
            url = f"https://q.10jqka.com.cn/gn/detail/code/{code}/"
        else:
            url = f"https://q.10jqka.com.cn/gn/detail/code/{code}/page/{page}/"

        try:
            response = self.session.get(url, timeout=15)
            response.encoding = 'gbk'

            if "æš‚æ— æˆä»½è‚¡æ•°æ®" in response.text:
                return []

            soup = BeautifulSoup(response.text, 'html.parser')
            table = soup.find('table', class_='m-table m-pager-table')

            if not table:
                return []

            stocks = []
            tbody = table.find('tbody')
            if tbody:
                for row in tbody.find_all('tr'):
                    cols = row.find_all('td')
                    if len(cols) >= 3:
                        stocks.append({
                            'ä»£ç ': cols[1].get_text(strip=True),
                            'åç§°': cols[2].get_text(strip=True)
                        })

            return stocks

        except:
            return []

    def _save_batch_to_mysql(self, data: List[Dict]):
        """ä¿å­˜æ‰¹æ¬¡æ•°æ®åˆ°MySQL"""
        if not data:
            return

        try:
            df = pd.DataFrame(data)
            df = df.drop_duplicates(['ymd', 'board_code', 'stock_code'])

            mysql_utils.data_from_dataframe_to_mysql(
                user=self.mysql_config['user'],
                password=self.mysql_config['password'],
                host=self.mysql_config['host'],
                database=self.mysql_config['database'],
                df=df,
                table_name="ods_akshare_stock_board_concept_maps_ths",
                merge_on=['ymd', 'board_code']
            )

        except Exception as e:
            print(f"âš ï¸  å†™å…¥MySQLå¤±è´¥: {e}")
            # å°è¯•ä¿å­˜å¤‡ä»½
            try:
                backup_file = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                df.to_csv(backup_file, index=False, encoding='utf-8-sig')
                print(f"    æ•°æ®å·²å¤‡ä»½åˆ°: {backup_file}")
            except:
                print("    å¤‡ä»½å¤±è´¥")


def main():
    """ä¸»å‡½æ•° - å®Œæ•´çˆ¬å–"""
    crawler = THSConceptCrawler()
    crawler.run(batch_size=15, test_mode=False)


def test():
    """æµ‹è¯•å‡½æ•° - åªçˆ¬å–å‰å‡ ä¸ªæ¦‚å¿µ"""
    crawler = THSConceptCrawler()

    # æµ‹è¯•ç”¨æ›´çŸ­çš„å»¶è¿Ÿ
    crawler.page_delay = 3.0
    crawler.concept_delay = 5.0
    crawler.batch_delay = 10.0

    print("ğŸ§ª æµ‹è¯•æ¨¡å¼å¯åŠ¨ï¼ˆåªçˆ¬å–å‰5ä¸ªæ¦‚å¿µï¼‰")
    crawler.run(batch_size=5, test_mode=True)


def custom():
    """è‡ªå®šä¹‰é…ç½®"""
    crawler = THSConceptCrawler()

    # è‡ªå®šä¹‰å»¶è¿Ÿï¼ˆæ ¹æ®ç½‘ç»œæƒ…å†µè°ƒæ•´ï¼‰
    crawler.page_delay = 6.0  # é¡µé—´å»¶è¿Ÿ
    crawler.concept_delay = 10.0  # æ¦‚å¿µé—´å»¶è¿Ÿ
    crawler.batch_delay = 30.0  # æ‰¹æ¬¡é—´å»¶è¿Ÿ

    # è‡ªå®šä¹‰æ‰¹æ¬¡å¤§å°
    batch_size = 12

    print(f"âš™ï¸  è‡ªå®šä¹‰æ¨¡å¼: æ‰¹æ¬¡{batch_size}ä¸ªæ¦‚å¿µ | å»¶è¿Ÿ{crawler.concept_delay}s")
    crawler.run(batch_size=batch_size, test_mode=False)


if __name__ == "__main__":
    print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. å®Œæ•´çˆ¬å–ï¼ˆå®‰å…¨æ¨¡å¼ï¼Œæ¨èï¼‰")
    print("2. æµ‹è¯•æ¨¡å¼ï¼ˆåªçˆ¬5ä¸ªæ¦‚å¿µï¼‰")
    print("3. è‡ªå®šä¹‰æ¨¡å¼")
    print("4. è¶…å®‰å…¨æ¨¡å¼ï¼ˆæœ€ä¿å®ˆï¼‰")

    choice = input("è¯·è¾“å…¥é€‰æ‹© (1/2/3/4): ").strip()

    if choice == '1':
        main()
    elif choice == '2':
        test()
    elif choice == '3':
        custom()
    elif choice == '4':
        # è¶…å®‰å…¨æ¨¡å¼
        crawler = THSConceptCrawler()
        crawler.page_delay = 8.0
        crawler.concept_delay = 15.0
        crawler.batch_delay = 40.0
        print("ğŸ›¡ï¸  è¶…å®‰å…¨æ¨¡å¼å¯åŠ¨ï¼ˆæœ€ä¿å®ˆé…ç½®ï¼‰")
        crawler.run(batch_size=10, test_mode=False)
    else:
        print("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å¼")
        main()