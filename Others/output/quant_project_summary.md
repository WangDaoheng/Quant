# é‡åŒ–å·¥ç¨‹V1.0 ä»£ç æ¢³ç†æ–‡æ¡£
*ç”Ÿæˆæ—¶é—´: 2026-01-12 15:30:10*

## é¡¹ç›®ç»Ÿè®¡ä¿¡æ¯
- é¡¹ç›®æ ¹ç›®å½•: F:\Quant\Backtrader_PJ1
- æ€»æ–‡ä»¶æ•°: 45
- Pythonæ–‡ä»¶æ•°: 40
- SQLæ–‡ä»¶æ•°: 4
- Shellæ–‡ä»¶æ•°: 1
- æœ‰æ•ˆç›®å½•æ•°: 14

# Backtrader_PJ1 é¡¹ç›®ç›®å½•ç»“æ„
*ç”Ÿæˆæ—¶é—´: 2026-01-12 15:30:10*

ğŸ“ Backtrader_PJ1/
    ğŸ“„ main-doubao.py
    ğŸ“„ main.py
    ğŸ“ backtest/
        ğŸ“„ __init__.py
        ğŸ“„ backtest_engine.py
        ğŸ“„ factor_driven_strategy.py
        ğŸ“„ performance_analysis.py
        ğŸ“„ simple_strategy.py
    ğŸ“ CommonProperties/
        ğŸ“„ Base_Properties.py
        ğŸ“„ Base_utils.py
        ğŸ“„ DateUtility.py
        ğŸ“„ Mysql_Utils.py
        ğŸ“„ __init__.py
        ğŸ“„ set_config.py
    ğŸ“ dashboard/
        ğŸ“„ __init__.py
        ğŸ“„ strategy_dashboard.py
    ğŸ“ datas_prepare/
        ğŸ“„ __init__.py
        ğŸ“„ run_data_prepare.sh
        ğŸ“„ setup_data_prepare.py
        ğŸ“ C00_SQL/
            ğŸ“„ DW_mysql_tables_nopart.sql
            ğŸ“„ MART_mysql_tables_nopart.sql
            ğŸ“„ __init__.py
            ğŸ“„ create_mysql_tables.sql
            ğŸ“„ create_mysql_tables_nopart.sql
        ğŸ“ C01_data_download_daily/
            ğŸ“„ __init__.py
            ğŸ“„ download_insight_data_afternoon.py
            ğŸ“„ download_insight_data_afternoon_of_history.py
            ğŸ“„ download_vantage_data_afternoon.py
        ğŸ“ C02_data_merge/
            ğŸ“„ __init__.py
            ğŸ“„ merge_insight_data_afternoon.py
        ğŸ“ C03_data_DWD/
            ğŸ“„ __init__.py
            ğŸ“„ calculate_DWD_datas.py
        ğŸ“ C04_data_MART/
            ğŸ“„ __init__.py
            ğŸ“„ calculate_MART_datas.py
        ğŸ“ C06_data_transfer/
            ğŸ“„ __init__.py
            ğŸ“„ get_example_tables.py
            ğŸ“„ put_df_to_mysql.py
            ğŸ“„ transfer_between_local_and_originMySQL.py
    ğŸ“ monitor/
        ğŸ“„ __init__.py
        ğŸ“„ alert_system.py
        ğŸ“„ realtime_monitor.py
    ğŸ“ review/
        ğŸ“„ __init__.py
        ğŸ“„ daily_review.py
    ğŸ“ strategy/
        ğŸ“„ __init__.py
        ğŸ“„ factor_library.py
        ğŸ“„ strategy_engine.py

# é¡¹ç›®ä»£ç å†…å®¹

--------------------------------------------------------------------------------
## main-doubao.py

```python
import logging
from backtest import StockBacktestEngine, PerformanceAnalyzer
from monitor.realtime_monitor import RealtimeMonitor
from monitor.alert_system import AlertSystem
from review.daily_review import DailyReview
from dashboard.strategy_dashboard import StrategyDashboard
from CommonProperties.DateUtility import DateUtility

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('quant_strategy.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def main():
    # 1. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
    engine = StockBacktestEngine()
    alert_system = AlertSystem()

    # 2. å›æµ‹å‚æ•°é…ç½®
    start_date = DateUtility.first_day_of_month_before_n_months(6)
    end_date = DateUtility.today()
    initial_cash = 100000
    initial_stock_codes = ['600000', '000001', '601318', '002594', '300059']

    # 3. è¿è¡Œå›æµ‹ï¼ˆå› å­é©±åŠ¨ç­–ç•¥ï¼‰
    logger.info("======= å¼€å§‹å› å­é©±åŠ¨ç­–ç•¥å›æµ‹ =======")
    factor_perf = engine.run_backtest(
        stock_codes=initial_stock_codes,
        start_date=start_date,
        end_date=end_date,
        initial_cash=initial_cash,
        strategy_type='factor_driven'
    )

    if not factor_perf:
        logger.error("å›æµ‹å¤±è´¥ï¼Œç»ˆæ­¢ç¨‹åº")
        return

    # 4. ç”Ÿæˆå›æµ‹æŠ¥å‘Š
    analyzer = PerformanceAnalyzer()
    factor_report = analyzer.generate_report(factor_perf, "å› å­é©±åŠ¨ç­–ç•¥", start_date, end_date)
    logger.info("\n======= å› å­é©±åŠ¨ç­–ç•¥å›æµ‹æŠ¥å‘Š =======\n" + factor_report)

    # 5. åˆå§‹åŒ–Cerebroå®ä¾‹ï¼ˆç”¨äºç›‘æ§/å¤ç›˜ï¼‰
    # cerebro = engine.run_backtest.__self__.cerebro  # å®é™…éœ€ä»å›æµ‹å¼•æ“ä¸­è·å–çœŸå®Cerebroå®ä¾‹
    cerebro = engine.cerebro  # å›æµ‹å¼•æ“ä¸­å·²ä¿å­˜äº†cerebroå®ä¾‹
    # 6. å®æ—¶ç›‘æ§
    monitor = RealtimeMonitor(engine, initial_stock_codes)
    # å•æ¬¡ç›‘æ§ï¼ˆéå¾ªç¯ï¼‰
    factor_alerts = monitor.monitor_factor_signals()
    position_alerts = monitor.monitor_position_performance(cerebro)
    price_alerts = monitor.monitor_price_volatility()

    # è§¦å‘é¢„è­¦
    if factor_alerts or position_alerts or price_alerts:
        alert_system.trigger_alert('all', {
            'factor': factor_alerts,
            'position': position_alerts,
            'price': price_alerts
        })

    # 7. æ¯æ—¥å¤ç›˜
    review = DailyReview(engine, cerebro, 'factor_driven')
    review_report = review.generate_daily_review_report()
    logger.info("\n======= æ¯æ—¥å¤ç›˜æŠ¥å‘Š =======\n" + review_report)

    # 8. ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨ç›˜
    dashboard = StrategyDashboard(engine, factor_perf, 'factor_driven')
    dashboard_path = dashboard.generate_dashboard(cerebro)
    logger.info(f"å¯è§†åŒ–ä»ªè¡¨ç›˜è·¯å¾„ï¼š{dashboard_path}")

    # 9. å¯åŠ¨å®æ—¶ç›‘æ§ï¼ˆå¯é€‰ï¼Œæ³¨é‡Šæ‰åˆ™åªè¿è¡Œä¸€æ¬¡ï¼‰
    # monitor.run_monitor(cerebro, interval=3600)  # 1å°æ—¶ç›‘æ§ä¸€æ¬¡

    logger.info("======= é‡åŒ–ç­–ç•¥åˆ†ææµç¨‹å®Œæˆ =======")


if __name__ == "__main__":
    main()


```

--------------------------------------------------------------------------------
## main.py

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é‡åŒ–ç­–ç•¥ä¸»ç¨‹åºå…¥å£ - ä¼˜åŒ–ç‰ˆ
ä¿®å¤äº†å›æµ‹å‘¨æœŸæ˜¾ç¤ºbugï¼Œå¢å¼ºäº†é”™è¯¯å¤„ç†

ä¸»è¦åŠŸèƒ½ï¼š
1. è¿è¡Œå› å­é©±åŠ¨ç­–ç•¥å›æµ‹
2. å®æ—¶ç›‘æ§ç­–ç•¥ä¿¡å·
3. ç”Ÿæˆæ¯æ—¥å¤ç›˜æŠ¥å‘Š
4. åˆ›å»ºå¯è§†åŒ–ä»ªè¡¨ç›˜
"""

import logging
import traceback
from datetime import datetime, timedelta
from typing import Optional, Dict, Any

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from backtest import StockBacktestEngine, PerformanceAnalyzer
from monitor.realtime_monitor import RealtimeMonitor
from monitor.alert_system import AlertSystem
from review.daily_review import DailyReview
from dashboard.strategy_dashboard import StrategyDashboard
from CommonProperties.DateUtility import DateUtility


# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================
def setup_logging():
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    log_date_format = '%Y-%m-%d %H:%M:%S'

    # åˆ›å»ºæ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(log_format, datefmt=log_date_format)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆæŒ‰æ—¥æœŸæ»šåŠ¨ï¼‰
    try:
        file_handler = logging.FileHandler(
            f'quant_strategy_{datetime.now().strftime("%Y%m%d")}.log',
            encoding='utf-8'
        )
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.INFO)
    except Exception as e:
        print(f"åˆ›å»ºæ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        file_handler = None

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)

    # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)

    # ç§»é™¤å¯èƒ½å­˜åœ¨çš„æ—§å¤„ç†å™¨
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)

    # æ·»åŠ æ–°å¤„ç†å™¨
    root_logger.addHandler(console_handler)
    if file_handler:
        root_logger.addHandler(file_handler)

    return root_logger


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================
def main():
    """ä¸»ç¨‹åºå…¥å£"""
    # 1. åˆå§‹åŒ–æ—¥å¿—
    logger = setup_logging()
    logger.info("=" * 60)
    logger.info("ğŸš€ é‡åŒ–ç­–ç•¥åˆ†æç³»ç»Ÿå¯åŠ¨")
    logger.info(f"å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)

    # ç”¨äºå­˜å‚¨å›æµ‹ç»“æœçš„å˜é‡
    factor_perf: Optional[Dict[str, Any]] = None
    engine: Optional[StockBacktestEngine] = None
    cerebro = None

    try:
        # 2. åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
        logger.info("ğŸ“¦ åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶...")
        engine = StockBacktestEngine()
        alert_system = AlertSystem()

        # 3. å›æµ‹å‚æ•°é…ç½®
        logger.info("âš™ï¸ é…ç½®å›æµ‹å‚æ•°...")
        start_date = DateUtility.first_day_of_month_before_n_months(6)  # 6ä¸ªæœˆå‰
        end_date = DateUtility.today()  # ä»Šå¤©

        # éªŒè¯æ—¥æœŸæ ¼å¼
        if not (start_date.isdigit() and len(start_date) == 8):
            raise ValueError(f"å¼€å§‹æ—¥æœŸæ ¼å¼é”™è¯¯: {start_date}")
        if not (end_date.isdigit() and len(end_date) == 8):
            raise ValueError(f"ç»“æŸæ—¥æœŸæ ¼å¼é”™è¯¯: {end_date}")

        initial_cash = 100000  # åˆå§‹èµ„é‡‘10ä¸‡å…ƒ
        initial_stock_codes = ['600000', '000001', '601318', '002594', '300059']  # æµ‹è¯•è‚¡ç¥¨æ± 

        logger.info(f"å›æµ‹å‘¨æœŸ: {start_date} ~ {end_date}")
        logger.info(f"åˆå§‹èµ„é‡‘: {initial_cash:,}å…ƒ")
        logger.info(f"è‚¡ç¥¨æ± : {initial_stock_codes}")

        # 4. è¿è¡Œå›æµ‹ï¼ˆå› å­é©±åŠ¨ç­–ç•¥ï¼‰
        logger.info("=" * 60)
        logger.info("ğŸ“ˆ å¼€å§‹å› å­é©±åŠ¨ç­–ç•¥å›æµ‹")
        logger.info("=" * 60)

        factor_perf = engine.run_backtest(
            stock_codes=initial_stock_codes,
            start_date=start_date,
            end_date=end_date,
            initial_cash=initial_cash,
            strategy_type='factor_driven'
        )

        if not factor_perf:
            logger.error("âŒ å›æµ‹å¤±è´¥ï¼Œç»ˆæ­¢ç¨‹åº")
            return

        # 5. ç”Ÿæˆå›æµ‹æŠ¥å‘Š
        logger.info("ğŸ“Š ç”Ÿæˆå›æµ‹æŠ¥å‘Š...")
        analyzer = PerformanceAnalyzer()
        factor_report = analyzer.generate_report(
            backtest_result=factor_perf,
            strategy_name="å› å­é©±åŠ¨ç­–ç•¥",
            start_date=start_date,
            end_date=end_date
        )

        logger.info("\n" + "=" * 60)
        logger.info("ğŸ“‹ å› å­é©±åŠ¨ç­–ç•¥å›æµ‹æŠ¥å‘Š")
        logger.info("=" * 60)

        # é€è¡Œè¾“å‡ºæŠ¥å‘Šï¼Œé¿å…æ—¥å¿—æˆªæ–­
        for line in factor_report.split('\n'):
            logger.info(line)

        # 6. è·å–Cerebroå®ä¾‹ç”¨äºç›‘æ§/å¤ç›˜
        if hasattr(engine, 'get_cerebro'):
            cerebro = engine.get_cerebro()
        elif hasattr(engine, 'cerebro'):
            cerebro = engine.cerebro
        else:
            logger.warning("âš ï¸ æ— æ³•è·å–Cerebroå®ä¾‹ï¼Œè·³è¿‡ç›‘æ§å’Œå¤ç›˜")
            cerebro = None

        # 7. å®æ—¶ç›‘æ§ï¼ˆå¦‚æœCerebroå¯ç”¨ï¼‰
        if cerebro:
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ‘ï¸ å¼€å§‹å®æ—¶ç›‘æ§")
            logger.info("=" * 60)

            monitor = RealtimeMonitor(engine, initial_stock_codes)

            # å•æ¬¡ç›‘æ§ï¼ˆéå¾ªç¯ï¼‰
            logger.info("ğŸ” ç›‘æ§å› å­ä¿¡å·...")
            factor_alerts = monitor.monitor_factor_signals()

            logger.info("ğŸ” ç›‘æ§æŒä»“ç»©æ•ˆ...")
            position_alerts = monitor.monitor_position_performance(cerebro)

            logger.info("ğŸ” ç›‘æ§ä»·æ ¼æ³¢åŠ¨...")
            price_alerts = monitor.monitor_price_volatility()

            # è§¦å‘é¢„è­¦
            if factor_alerts or position_alerts or price_alerts:
                logger.warning("ğŸš¨ æ£€æµ‹åˆ°é¢„è­¦ä¿¡å·ï¼Œè§¦å‘é¢„è­¦ç³»ç»Ÿ")
                alert_system.trigger_alert('all', {
                    'factor': factor_alerts,
                    'position': position_alerts,
                    'price': price_alerts
                })
            else:
                logger.info("âœ… æ— é¢„è­¦ä¿¡å·ï¼Œç›‘æ§æ­£å¸¸")
        else:
            logger.info("â­ï¸ è·³è¿‡å®æ—¶ç›‘æ§ï¼ˆCerebroä¸å¯ç”¨ï¼‰")

        # 8. æ¯æ—¥å¤ç›˜ï¼ˆå¦‚æœCerebroå¯ç”¨ï¼‰
        if cerebro:
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“ ç”Ÿæˆæ¯æ—¥å¤ç›˜æŠ¥å‘Š")
            logger.info("=" * 60)

            review = DailyReview(engine, cerebro, 'factor_driven')
            review_report = review.generate_daily_review_report()

            logger.info("ğŸ“„ å¤ç›˜æŠ¥å‘Šæ‘˜è¦:")
            # åªè¾“å‡ºæŠ¥å‘Šçš„å‰å‡ è¡Œä½œä¸ºæ‘˜è¦
            lines = review_report.split('\n')[:15]
            for line in lines:
                logger.info(line)

            if len(review_report.split('\n')) > 15:
                logger.info("... (å®Œæ•´æŠ¥å‘Šå·²ä¿å­˜è‡³æ–‡ä»¶)")
        else:
            logger.info("â­ï¸ è·³è¿‡æ¯æ—¥å¤ç›˜ï¼ˆCerebroä¸å¯ç”¨ï¼‰")

        # 9. ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨ç›˜ï¼ˆå¦‚æœå›æµ‹ç»“æœå¯ç”¨ï¼‰
        if factor_perf and cerebro:
            logger.info("\n" + "=" * 60)
            logger.info("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–ä»ªè¡¨ç›˜")
            logger.info("=" * 60)

            dashboard = StrategyDashboard(engine, factor_perf, 'factor_driven')
            dashboard_path = dashboard.generate_dashboard(cerebro)

            if dashboard_path:
                logger.info(f"âœ… ä»ªè¡¨ç›˜å·²ç”Ÿæˆ: {dashboard_path}")
                logger.info(f"ğŸ’¡ è¯·ç”¨æµè§ˆå™¨æ‰“å¼€æŸ¥çœ‹: file://{dashboard_path}")
            else:
                logger.error("âŒ ä»ªè¡¨ç›˜ç”Ÿæˆå¤±è´¥")
        else:
            logger.info("â­ï¸ è·³è¿‡ä»ªè¡¨ç›˜ç”Ÿæˆï¼ˆæ•°æ®ä¸è¶³ï¼‰")

        # 10. æ˜¾ç¤ºå…³é”®ç»©æ•ˆæŒ‡æ ‡
        logger.info("\n" + "=" * 60)
        logger.info("ğŸ¯ å…³é”®ç»©æ•ˆæŒ‡æ ‡æ±‡æ€»")
        logger.info("=" * 60)

        if factor_perf:
            metrics = [
                ("æ€»æ”¶ç›Šç‡", f"{factor_perf.get('æ€»æ”¶ç›Šç‡', 0):.2f}%"),
                ("å¹´åŒ–æ”¶ç›Šç‡", f"{factor_perf.get('å¹´åŒ–æ”¶ç›Šç‡', 0):.2f}%"),
                ("å¤æ™®æ¯”ç‡", f"{factor_perf.get('å¤æ™®æ¯”ç‡', 0):.2f}"),
                ("æœ€å¤§å›æ’¤", f"{factor_perf.get('æœ€å¤§å›æ’¤', 0):.2f}%"),
                ("èƒœç‡", f"{factor_perf.get('èƒœç‡', 0):.2f}%"),
                ("ç›ˆäºæ¯”", f"{factor_perf.get('ç›ˆäºæ¯”', 0):.2f}"),
                ("æœ€ç»ˆèµ„é‡‘", f"{factor_perf.get('æœ€ç»ˆèµ„é‡‘', 0):,.2f}å…ƒ"),
            ]

            for name, value in metrics:
                logger.info(f"  {name:<10} : {value}")

            # ç®€å•è¯„ä¼°
            total_return = factor_perf.get('æ€»æ”¶ç›Šç‡', 0)
            max_drawdown = factor_perf.get('æœ€å¤§å›æ’¤', 100)

            if total_return > 20 and max_drawdown < 15:
                logger.info("ğŸŒŸ ç­–ç•¥è¡¨ç°ä¼˜ç§€ï¼")
            elif total_return > 10 and max_drawdown < 20:
                logger.info("ğŸ‘ ç­–ç•¥è¡¨ç°è‰¯å¥½")
            elif total_return > 0:
                logger.info("ğŸ¤” ç­–ç•¥è¡¨ç°ä¸€èˆ¬ï¼Œæœ‰å¾…ä¼˜åŒ–")
            else:
                logger.info("âš ï¸ ç­–ç•¥äºæŸï¼Œéœ€è¦é‡æ–°è¯„ä¼°")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… é‡åŒ–ç­–ç•¥åˆ†ææµç¨‹å®Œæˆ")
        logger.info("=" * 60)

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åºæ‰§è¡Œ")
    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
        logger.error("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        logger.error(traceback.format_exc())

        # å°è¯•ä¿å­˜éƒ¨åˆ†ç»“æœ
        try:
            if factor_perf:
                logger.info("\nğŸ’¾ å°è¯•ä¿å­˜å·²ç”Ÿæˆçš„å›æµ‹ç»“æœ...")
                # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜åˆ°æ–‡ä»¶çš„é€»è¾‘
                pass
        except:
            pass

        logger.error("âŒ ç¨‹åºå¼‚å¸¸ç»ˆæ­¢")
    finally:
        # æ¸…ç†èµ„æº
        logger.info("ğŸ§¹ æ¸…ç†èµ„æº...")
        # å¯ä»¥æ·»åŠ èµ„æºæ¸…ç†é€»è¾‘ï¼Œå¦‚å…³é—­æ•°æ®åº“è¿æ¥ç­‰


# ============================================================================
# ç¨‹åºå…¥å£
# ============================================================================
if __name__ == "__main__":
    # è®°å½•å¯åŠ¨ä¿¡æ¯
    print("=" * 60)
    print("ğŸ¯ é‡åŒ–ç­–ç•¥åˆ†æç³»ç»Ÿ v1.0")
    print(f"ğŸ“… å¯åŠ¨æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    print("")

    # è¿è¡Œä¸»ç¨‹åº
    main()

    # ç¨‹åºç»“æŸ
    print("")
    print("=" * 60)
    print("ğŸ ç¨‹åºæ‰§è¡Œå®Œæ¯•")
    print("=" * 60)
```

--------------------------------------------------------------------------------
## backtest\__init__.py

```python
from .backtest_engine import StockBacktestEngine
from .simple_strategy import SimpleStrategy
from .factor_driven_strategy import FactorDrivenStrategy
from .performance_analysis import PerformanceAnalyzer

__all__ = [
    'StockBacktestEngine',
    'SimpleStrategy',
    'FactorDrivenStrategy',
    'PerformanceAnalyzer'
]
```

--------------------------------------------------------------------------------
## backtest\backtest_engine.py

```python
import backtrader as bt
import pandas as pd
import logging
from CommonProperties import Mysql_Utils
from CommonProperties.Base_utils import timing_decorator
from Others.strategy.factor_library import FactorLibrary
from backtest.simple_strategy import SimpleStrategy
from backtest.factor_driven_strategy import FactorDrivenStrategy

# å¤ç”¨ä½ çš„æ—¥å¿—é…ç½®
logger = logging.getLogger(__name__)


class StockBacktestEngine:
    """å›æµ‹å¼•æ“æ ¸å¿ƒï¼šå®Œå…¨å¤ç”¨ç°æœ‰MySQLå·¥å…·ç±»å’Œè£…é¥°å™¨"""

    def __init__(self):
        # å¤ç”¨è¿œç¨‹MySQLé…ç½®ï¼ˆä»ä½ çš„Base_Propertiesè¯»å–ï¼‰
        self.user = Mysql_Utils.origin_user
        self.password = Mysql_Utils.origin_password
        self.host = Mysql_Utils.origin_host
        self.database = Mysql_Utils.origin_database
        # åˆå§‹åŒ–å› å­åº“
        self.factor_lib = FactorLibrary()
        # æå‰åˆå§‹åŒ–cerebroï¼ˆä½†è¦æ³¨æ„çº¿ç¨‹å®‰å…¨ï¼‰
        self.cerebro = None

    @timing_decorator
    def _prepare_feed(self, stock_code, start_date, end_date):
        """
        å‡†å¤‡Backtraderæ•°æ®é¦ˆé€
        """
        try:
            # ä½¿ç”¨factor_libè·å–Kçº¿æ•°æ®
            kline_df = self.factor_lib.get_stock_kline_data(
                stock_code=stock_code,
                start_date=start_date,
                end_date=end_date
            )

            if kline_df.empty:
                logger.warning(f"è‚¡ç¥¨[{stock_code}]åœ¨{start_date}-{end_date}æ— æ•°æ®")
                return None

            # æ•°æ®æ ¼å¼è½¬æ¢
            kline_df['ymd'] = pd.to_datetime(kline_df['ymd'])
            kline_df = kline_df.set_index('ymd')
            kline_df.index.name = 'datetime'

            # ç¡®ä¿åˆ—åæ­£ç¡®
            kline_df = kline_df.rename(columns={
                'open': 'open',
                'high': 'high',
                'low': 'low',
                'close': 'close',
                'volume': 'volume'
            })

            # è½¬æ¢ä¸ºBacktraderæ•°æ®æ ¼å¼
            feed = bt.feeds.PandasData(dataname=kline_df)
            return feed
        except Exception as e:
            logger.error(f"å‡†å¤‡{stock_code}æ•°æ®å¤±è´¥ï¼š{str(e)}")
            return None


    @timing_decorator
    def get_factor_value(self, stock_code, date, factor_type='pb'):
        """
        æŸ¥è¯¢æŒ‡å®šè‚¡ç¥¨/æ—¥æœŸçš„å› å­ä¿¡å·
        """
        try:
            date_str = date.strftime('%Y%m%d')

            # æ¸…ç†è‚¡ç¥¨ä»£ç æ ¼å¼
            stock_code_clean = stock_code.split('.')[0] if '.' in stock_code else stock_code

            if factor_type == 'pb':
                # PBå› å­
                pb_df = self.factor_lib.pb_factor(start_date=date_str, end_date=date_str)
                if not pb_df.empty:
                    # ç²¾ç¡®åŒ¹é…è‚¡ç¥¨ä»£ç 
                    pb_df_filtered = pb_df[pb_df['stock_code'] == stock_code_clean]
                    if not pb_df_filtered.empty:
                        return bool(pb_df_filtered['pb_signal'].iloc[0])
                return False

            elif factor_type == 'zt':
                # æ¶¨åœå› å­
                zt_df = self.factor_lib.zt_factor(start_date=date_str, end_date=date_str)
                if not zt_df.empty:
                    zt_df_filtered = zt_df[zt_df['stock_code'] == stock_code_clean]
                    if not zt_df_filtered.empty:
                        return bool(zt_df_filtered['zt_signal'].iloc[0])
                return False

            elif factor_type == 'shareholder':
                # ç­¹ç å› å­
                shareholder_df = self.factor_lib.shareholder_factor(start_date=date_str, end_date=date_str)
                if not shareholder_df.empty:
                    shareholder_df_filtered = shareholder_df[shareholder_df['stock_code'] == stock_code_clean]
                    if not shareholder_df_filtered.empty:
                        return bool(shareholder_df_filtered['shareholder_signal'].iloc[0])
                return False

            else:
                logger.warning(f"ä¸æ”¯æŒçš„å› å­ç±»å‹ï¼š{factor_type}")
                return False
        except Exception as e:
            logger.error(f"æŸ¥è¯¢{stock_code}@{date_str}çš„{factor_type}å› å­å¤±è´¥ï¼š{str(e)}")
            return False

    @timing_decorator
    def update_datas(self, cerebro, new_stock_codes, start_date, end_date, current_date):
        """
        åŠ¨æ€æ›´æ–°è‚¡ç¥¨æ•°æ®ï¼ˆé€‚é…æœˆåº¦è°ƒä»“ï¼‰
        :param cerebro: Backtrader Cerebroå®ä¾‹
        :param new_stock_codes: æ–°é€‰è‚¡åˆ—è¡¨
        :param start_date: å›æµ‹å¼€å§‹æ—¥æœŸ
        :param end_date: å›æµ‹ç»“æŸæ—¥æœŸ
        :param current_date: å½“å‰è°ƒä»“æ—¥æœŸï¼ˆYYYYMMDDï¼‰
        :return: æœ‰æ•ˆè‚¡ç¥¨åˆ—è¡¨
        """
        # æ¸…ç©ºæ—§æ•°æ®
        cerebro.datas.clear()
        valid_codes = []
        for code in new_stock_codes[:5]:  # é™åˆ¶æ•°é‡ï¼Œæå‡å›æµ‹é€Ÿåº¦
            feed = self._prepare_feed(code, current_date, end_date)
            if feed:
                cerebro.adddata(feed, name=code)
                valid_codes.append(code)
        logger.info(f"åŠ¨æ€åŠ è½½æ–°è‚¡ç¥¨æ•°æ®ï¼š{valid_codes}")
        return valid_codes

    @timing_decorator
    def run_backtest(self,
                     stock_codes,
                     start_date,
                     end_date,
                     initial_cash=100000,
                     strategy_type='simple',
                     stock_selection_func=None):
        """
        æ‰§è¡Œå›æµ‹ä¸»é€»è¾‘
        :param stock_codes: åˆå§‹é€‰è‚¡åˆ—è¡¨
        :param start_date: å›æµ‹å¼€å§‹æ—¥æœŸï¼ˆYYYYMMDDï¼‰
        :param end_date: å›æµ‹ç»“æŸæ—¥æœŸï¼ˆYYYYMMDDï¼‰
        :param initial_cash: åˆå§‹èµ„é‡‘ï¼ˆé»˜è®¤10ä¸‡ï¼‰
        :param strategy_type: ç­–ç•¥ç±»å‹ï¼ˆsimple/factor_drivenï¼‰
        :param stock_selection_func: åŠ¨æ€é€‰è‚¡å‡½æ•°ï¼ˆä»…dynamic_poolç­–ç•¥éœ€è¦ï¼‰
        :return: ç»©æ•ˆæŒ‡æ ‡å­—å…¸
        """
        # 1. åˆå§‹åŒ–Backtraderæ ¸å¿ƒå¼•æ“
        self.cerebro = bt.Cerebro()  # ä¿å­˜cerebroå®ä¾‹ä¾›å¤–éƒ¨è°ƒç”¨
        self.cerebro.broker.setcash(initial_cash)  # è®¾ç½®åˆå§‹èµ„é‡‘
        self.cerebro.broker.setcommission(commission=0.0003)  # ä½£é‡‘ï¼šåƒåˆ†ä¹‹0.3
        self.cerebro.broker.set_coc(True)  # ä»¥æ”¶ç›˜ä»·æˆäº¤ï¼ˆè´´è¿‘å®ç›˜ï¼‰

        # 2. åŠ è½½åˆå§‹è‚¡ç¥¨æ•°æ®
        valid_codes = []
        for code in stock_codes[:5]:
            feed = self._prepare_feed(code, start_date, end_date)
            if feed:
                self.cerebro.adddata(feed, name=code)
                valid_codes.append(code)

        if not valid_codes:
            logger.error("æ— æœ‰æ•ˆè‚¡ç¥¨æ•°æ®ï¼Œç»ˆæ­¢å›æµ‹")
            return None

        # 3. åŠ è½½ç­–ç•¥ï¼ˆæ ¸å¿ƒï¼šä¼ é€’å‚æ•°ï¼‰
        if strategy_type == 'simple':
            self.cerebro.addstrategy(SimpleStrategy)
            logger.info("åŠ è½½ç®€æ˜“è°ƒä»“ç­–ç•¥")

        elif strategy_type == 'factor_driven':
            # ä¼ é€’å›æµ‹å¼•æ“å®ä¾‹ç»™å› å­ç­–ç•¥
            self.cerebro.addstrategy(
                FactorDrivenStrategy,
                backtest_engine=self  # å…³é”®ï¼šæŠŠå¼•æ“å®ä¾‹ä¼ ç»™ç­–ç•¥
            )
            logger.info("åŠ è½½å› å­é©±åŠ¨ç­–ç•¥")

        else:
            logger.error(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»å‹ï¼š{strategy_type}")
            return None

        # 4. æ·»åŠ ç»©æ•ˆåˆ†æå™¨ï¼ˆå«èƒœç‡/å¤æ™®æ¯”ç‡/æœ€å¤§å›æ’¤ï¼‰
        self.cerebro.addanalyzer(bt.analyzers.SharpeRatio, _name='sharpe', riskfreerate=0.03)
        self.cerebro.addanalyzer(bt.analyzers.Returns, _name='returns', tann=252)
        self.cerebro.addanalyzer(bt.analyzers.DrawDown, _name='drawdown')
        self.cerebro.addanalyzer(bt.analyzers.TradeAnalyzer, _name='trade_analyzer')
        self.cerebro.addanalyzer(bt.analyzers.SQN, _name='sqn')

        # 5. è¿è¡Œå›æµ‹
        logger.info(f"å¼€å§‹å›æµ‹ï¼š{start_date} ~ {end_date}ï¼Œåˆå§‹èµ„é‡‘ï¼š{initial_cash}å…ƒ")
        results = self.cerebro.run()
        if not results:
            logger.error("å›æµ‹æ‰§è¡Œå¤±è´¥")
            return None
        strat = results[0]

        # 6. æå–ç»©æ•ˆæŒ‡æ ‡
        perf = self._extract_performance_metrics(strat, initial_cash, self.cerebro, start_date, end_date)
        logger.info(f"å›æµ‹å®Œæˆï¼Œæœ€ç»ˆèµ„é‡‘ï¼š{perf['æœ€ç»ˆèµ„é‡‘']}å…ƒ")
        return perf

    def _extract_performance_metrics(self, strat, initial_cash, cerebro, start_date, end_date):
        """æå–æ ‡å‡†åŒ–ç»©æ•ˆæŒ‡æ ‡ï¼ˆå«èƒœç‡ï¼‰"""
        # åŸºç¡€æ”¶ç›ŠæŒ‡æ ‡
        final_cash = round(cerebro.broker.getvalue(), 2)
        returns_ana = strat.analyzers.returns.get_analysis()
        sharpe_ana = strat.analyzers.sharpe.get_analysis()
        drawdown_ana = strat.analyzers.drawdown.get_analysis()

        # äº¤æ˜“èƒœç‡æŒ‡æ ‡
        trade_ana = strat.analyzers.trade_analyzer.get_analysis()
        sqn_ana = strat.analyzers.sqn.get_analysis()

        # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
        total_return = round((final_cash - initial_cash) / initial_cash * 100, 2)
        annual_return = round(returns_ana.get('rnorm', 0) * 100, 2)
        sharpe_ratio = round(sharpe_ana.get('sharperatio', 0), 2)
        max_drawdown = round(drawdown_ana.get('max', {}).get('drawdown', 0), 2)

        # èƒœç‡/ç›ˆäºæ¯”è®¡ç®—ï¼ˆå®¹é”™ï¼‰
        try:
            total_trades = trade_ana.total.closed
            winning_trades = trade_ana.won.total if hasattr(trade_ana, 'won') else 0
            losing_trades = trade_ana.lost.total if hasattr(trade_ana, 'lost') else 0

            win_rate = round(winning_trades / total_trades * 100, 2) if total_trades > 0 else 0
            avg_win = trade_ana.won.pnl.average if winning_trades > 0 else 0
            avg_loss = abs(trade_ana.lost.pnl.average) if losing_trades > 0 else 1
            profit_loss_ratio = round(avg_win / avg_loss, 2)
        except Exception as e:
            logger.warning(f"è®¡ç®—èƒœç‡å¤±è´¥ï¼š{str(e)}")
            total_trades = 0
            win_rate = 0
            profit_loss_ratio = 0

        # å°è£…ç»“æœ
        return {
            # åŸºç¡€ä¿¡æ¯
            'åˆå§‹èµ„é‡‘': initial_cash,
            'æœ€ç»ˆèµ„é‡‘': final_cash,
            'å›æµ‹å‘¨æœŸ': f"{start_date} ~ {end_date}",
            # æ”¶ç›ŠæŒ‡æ ‡
            'æ€»æ”¶ç›Šç‡': total_return,
            'å¹´åŒ–æ”¶ç›Šç‡': annual_return,
            'å¤æ™®æ¯”ç‡': sharpe_ratio,
            'æœ€å¤§å›æ’¤': max_drawdown,
            # èƒœç‡æŒ‡æ ‡
            'æ€»äº¤æ˜“æ¬¡æ•°': total_trades,
            'èƒœç‡': win_rate,
            'ç›ˆäºæ¯”': profit_loss_ratio,
            'ç­–ç•¥è´¨é‡å¾—åˆ†(SQN)': round(sqn_ana.get('sqn', 0), 2)
        }
```

--------------------------------------------------------------------------------
## backtest\factor_driven_strategy.py

```python
import backtrader as bt
import logging
from typing import Optional
from CommonProperties.Base_utils import timing_decorator
# å¯¼å…¥å¼•æ“ç±»ï¼Œè®©IDEèƒ½è¯†åˆ«ç±»å‹
from backtest.backtest_engine import StockBacktestEngine

logger = logging.getLogger(__name__)

class FactorDrivenStrategy(bt.Strategy):
    """
    å› å­é©±åŠ¨ç­–ç•¥ï¼šæ¯æ—¥æŸ¥è¯¢PB/æ¶¨åœ/ç­¹ç å› å­ï¼ŒåŠ¨æ€å†³å®šä¹°å–
    é€‚ç”¨äºéªŒè¯å› å­çš„å®é™…äº¤æ˜“ä»·å€¼
    """
    # å£°æ˜å‚æ•° + ç±»å‹æ³¨è§£ï¼ˆè§£å†³IDEè·³è½¬é—®é¢˜ï¼‰
    params = (
        ('backtest_engine', Optional[StockBacktestEngine], None),
    )

    @timing_decorator
    def next(self):
        # æ¯ä¸ªäº¤æ˜“æ—¥æ‰§è¡Œä¸€æ¬¡
        current_date = self.datas[0].datetime.date(0) if self.datas else None
        if not current_date:
            return

        # 1. æ ¡éªŒå›æµ‹å¼•æ“å‚æ•°æ˜¯å¦ä¼ é€’æˆåŠŸ
        engine: StockBacktestEngine = self.p.backtest_engine
        if not engine:
            logger.error("å›æµ‹å¼•æ“å®ä¾‹æœªä¼ é€’ï¼Œæ— æ³•æŸ¥è¯¢å› å­")
            return

        # 2. éå†æ‰€æœ‰è‚¡ç¥¨ï¼Œé€åªåˆ¤æ–­å› å­ä¿¡å·
        for data in self.datas:
            stock_code = data._name
            if not stock_code:
                continue

            # 3. æŸ¥è¯¢å½“æ—¥å› å­ä¿¡å·
            pb_signal = engine.get_factor_value(stock_code, current_date, 'pb')
            zt_signal = engine.get_factor_value(stock_code, current_date, 'zt')
            shareholder_signal = engine.get_factor_value(stock_code, current_date, 'shareholder')

            # 4. ç”Ÿæˆä¹°å–ä¿¡å·ï¼ˆä¸‰ä¸ªå› å­åŒæ—¶æ»¡è¶³æ‰ä¹°å…¥ï¼‰
            buy_signal = pb_signal and zt_signal and shareholder_signal
            sell_signal = not buy_signal

            # 5. è·å–å½“å‰æŒä»“ï¼Œé¿å…é‡å¤äº¤æ˜“
            current_pos = self.getposition(data).size

            # 6. æ‰§è¡Œä¹°å…¥
            if buy_signal and current_pos == 0:
                # ç­‰æƒåˆ†é…ä»“ä½ï¼š90%ç°é‡‘ / è‚¡ç¥¨æ•°é‡ / æ”¶ç›˜ä»·
                total_cash = self.broker.getcash() * 0.9
                position_size = total_cash / len(self.datas) / data.close[0]
                self.buy(data, size=position_size)
                logger.info(
                    f"[{current_date}] ä¹°å…¥ {stock_code} | "
                    f"PBï¼š{pb_signal} | æ¶¨åœï¼š{zt_signal} | ç­¹ç ï¼š{shareholder_signal} | "
                    f"ä¹°å…¥æ•°é‡ï¼š{position_size:.0f}è‚¡"
                )

            # 7. æ‰§è¡Œå–å‡º
            elif sell_signal and current_pos > 0:
                self.close(data)
                logger.info(
                    f"[{current_date}] å–å‡º {stock_code} | "
                    f"PBï¼š{pb_signal} | æ¶¨åœï¼š{zt_signal} | ç­¹ç ï¼š{shareholder_signal} | "
                    f"æŒä»“æ•°é‡ï¼š{current_pos}è‚¡"
                )
```

--------------------------------------------------------------------------------
## backtest\performance_analysis.py

```python
import logging
from CommonProperties.Base_utils import timing_decorator

logger = logging.getLogger(__name__)

class PerformanceAnalyzer:
    """ç»©æ•ˆåˆ†æå·¥å…·ï¼šç”Ÿæˆæ ‡å‡†åŒ–å›æµ‹æŠ¥å‘Šï¼ˆå«èƒœç‡/å› å­æ•ˆæœåˆ†æï¼‰"""
    @staticmethod
    @timing_decorator
    def generate_report(backtest_result, strategy_name, start_date, end_date):
        """
        ç”Ÿæˆç»“æ„åŒ–å›æµ‹æŠ¥å‘Š
        :param backtest_result: å›æµ‹ç»©æ•ˆå­—å…¸
        :param strategy_name: ç­–ç•¥åç§°
        :param start_date: å¼€å§‹æ—¥æœŸ
        :param end_date: ç»“æŸæ—¥æœŸ
        :return: æ ¼å¼åŒ–æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        if not backtest_result:
            return "âŒ å›æµ‹å¤±è´¥ï¼Œæ— æœ‰æ•ˆç»©æ•ˆæ•°æ®"

        # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
        report = f"""
# ğŸ“ˆ {strategy_name} å›æµ‹æŠ¥å‘Š
## ğŸ•’ å›æµ‹å‘¨æœŸ
{start_date} ~ {end_date}

## ğŸ’° æ ¸å¿ƒæ”¶ç›ŠæŒ‡æ ‡
| æŒ‡æ ‡         | æ•°å€¼       | è¯´æ˜                     |
|--------------|------------|--------------------------|
| åˆå§‹èµ„é‡‘     | {backtest_result['åˆå§‹èµ„é‡‘']} å…ƒ | -                        |
| æœ€ç»ˆèµ„é‡‘     | {backtest_result['æœ€ç»ˆèµ„é‡‘']} å…ƒ | å›æµ‹ç»“æŸåè´¦æˆ·æ€»èµ„é‡‘     |
| æ€»æ”¶ç›Šç‡     | {backtest_result['æ€»æ”¶ç›Šç‡']} % | ç´¯è®¡æ”¶ç›Šï¼ˆå«æ‰‹ç»­è´¹ï¼‰|
| å¹´åŒ–æ”¶ç›Šç‡   | {backtest_result['å¹´åŒ–æ”¶ç›Šç‡']} % | æŒ‰252ä¸ªäº¤æ˜“æ—¥å¹´åŒ–        |
| å¤æ™®æ¯”ç‡     | {backtest_result['å¤æ™®æ¯”ç‡']} | é£é™©è°ƒæ•´åæ”¶ç›Šï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰|
| æœ€å¤§å›æ’¤     | {backtest_result['æœ€å¤§å›æ’¤']} % | æœ€å¤§æµ®äºæ¯”ä¾‹ï¼ˆè¶Šä½è¶Šå¥½ï¼‰|

## ğŸ¯ äº¤æ˜“èƒœç‡æŒ‡æ ‡
| æŒ‡æ ‡         | æ•°å€¼       | è¯´æ˜                     |
|--------------|------------|--------------------------|
| æ€»äº¤æ˜“æ¬¡æ•°   | {backtest_result['æ€»äº¤æ˜“æ¬¡æ•°']} | å®Œæ•´ä¹°å–æ¬¡æ•°             |
| èƒœç‡         | {backtest_result['èƒœç‡']} % | ç›ˆåˆ©äº¤æ˜“å æ¯”             |
| ç›ˆäºæ¯”       | {backtest_result['ç›ˆäºæ¯”']} | å¹³å‡ç›ˆåˆ©/å¹³å‡äºæŸ        |
| ç­–ç•¥è´¨é‡å¾—åˆ† | {backtest_result['ç­–ç•¥è´¨é‡å¾—åˆ†(SQN)']} | >1.6ä¼˜ç§€ / <0.5è¾ƒå·®      |

## ğŸ“ ç­–ç•¥ä¼˜åŒ–å»ºè®®
{PerformanceAnalyzer._generate_suggestion(backtest_result)}
        """
        return report

    @staticmethod
    def _generate_suggestion(backtest_result):
        """æ ¹æ®ç»©æ•ˆç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        suggestions = []

        # æ”¶ç›Šç‡ç»´åº¦
        if backtest_result['å¹´åŒ–æ”¶ç›Šç‡'] > 15:
            suggestions.append("âœ… å¹´åŒ–æ”¶ç›Šç‡>15%ï¼Œç­–ç•¥æ”¶ç›Šèƒ½åŠ›ä¼˜ç§€")
        elif backtest_result['å¹´åŒ–æ”¶ç›Šç‡'] < 5:
            suggestions.append("âš ï¸ å¹´åŒ–æ”¶ç›Šç‡<5%ï¼Œå»ºè®®ä¼˜åŒ–å› å­ç»„åˆæˆ–è°ƒä»“é¢‘ç‡")

        # é£é™©ç»´åº¦
        if backtest_result['æœ€å¤§å›æ’¤'] > 20:
            suggestions.append("âš ï¸ æœ€å¤§å›æ’¤>20%ï¼Œå»ºè®®æ·»åŠ æ­¢æŸè§„åˆ™ï¼ˆå¦‚äºæŸ8%æ­¢æŸï¼‰")
        else:
            suggestions.append("âœ… æœ€å¤§å›æ’¤<20%ï¼Œé£é™©æ§åˆ¶è‰¯å¥½")

        # èƒœç‡ç»´åº¦
        if backtest_result['èƒœç‡'] > 60:
            suggestions.append("âœ… èƒœç‡>60%ï¼Œå› å­æ‹©æ—¶èƒ½åŠ›ä¼˜ç§€")
        elif backtest_result['èƒœç‡'] < 40:
            suggestions.append("âš ï¸ èƒœç‡<40%ï¼Œå»ºè®®æé«˜å› å­ç­›é€‰ä¸¥æ ¼åº¦ï¼ˆå¦‚PBåˆ†ä½æ•°ä»0.3â†’0.2ï¼‰")

        # ç›ˆäºæ¯”ç»´åº¦
        if backtest_result['ç›ˆäºæ¯”'] > 2:
            suggestions.append("âœ… ç›ˆäºæ¯”>2ï¼Œå•æ¬¡ç›ˆåˆ©è¦†ç›–å¤šæ¬¡äºæŸï¼Œç¨³å®šæ€§é«˜")
        elif backtest_result['ç›ˆäºæ¯”'] < 1:
            suggestions.append("âš ï¸ ç›ˆäºæ¯”<1ï¼Œå»ºè®®ä¼˜åŒ–å–å‡ºè§„åˆ™ï¼ˆå¦‚ç›ˆåˆ©10%æ­¢ç›ˆï¼‰")

        # å¤æ™®æ¯”ç‡ç»´åº¦
        if backtest_result['å¤æ™®æ¯”ç‡'] > 1.5:
            suggestions.append("âœ… å¤æ™®æ¯”ç‡>1.5ï¼Œé£é™©æ”¶ç›Šæ¯”ä¼˜ç§€ï¼Œå¯å®ç›˜éªŒè¯")
        elif backtest_result['å¤æ™®æ¯”ç‡'] < 0.5:
            suggestions.append("âš ï¸ å¤æ™®æ¯”ç‡<0.5ï¼Œå»ºè®®æ›´æ¢å› å­ç»„åˆï¼ˆå¦‚æ·»åŠ åŒ—å‘èµ„é‡‘å› å­ï¼‰")

        return "\n".join(suggestions) if suggestions else "ğŸ“Œ ç­–ç•¥è¡¨ç°ä¸­æ€§ï¼Œå»ºè®®æŒç»­è·Ÿè¸ª"
```

--------------------------------------------------------------------------------
## backtest\simple_strategy.py

```python
import backtrader as bt
import logging
from CommonProperties.Base_utils import timing_decorator

logger = logging.getLogger(__name__)


class SimpleStrategy(bt.Strategy):
    """
    ç®€æ˜“è°ƒä»“ç­–ç•¥ï¼šæ¯æœˆè°ƒä»“ä¸€æ¬¡ï¼Œå–å‡ºæ‰€æœ‰æŒä»“åä¹°å…¥ç¬¬ä¸€åªè‚¡ç¥¨
    é€‚ç”¨äºå¿«é€ŸéªŒè¯é€‰è‚¡ç»“æœçš„æ•´ä½“æ”¶ç›Š
    """

    @timing_decorator
    def next(self):
        # æ¯æœˆç¬¬ä¸€ä¸ªäº¤æ˜“æ—¥è°ƒä»“ï¼ˆ20ä¸ªäº¤æ˜“æ—¥â‰ˆ1ä¸ªæœˆï¼‰
        if len(self) % 20 == 0:
            current_date = self.datas[0].datetime.date(0) if self.datas else None
            if not current_date:
                return
            logger.info(f"[{current_date}] å¼€å§‹æœˆåº¦è°ƒä»“")

            # 1. å–å‡ºæ‰€æœ‰æŒä»“
            for data in self.datas:
                if self.getposition(data).size > 0:
                    self.close(data)
                    logger.info(f"[{current_date}] å–å‡º {data._name}")

            # 2. ä¹°å…¥ç¬¬ä¸€åªè‚¡ç¥¨ï¼ˆ90%ä»“ä½ï¼‰
            if self.datas:
                # è®¡ç®—ä¹°å…¥æ•°é‡ï¼š(å¯ç”¨ç°é‡‘Ã—90%) / å½“å‰æ”¶ç›˜ä»·
                total_cash = self.broker.getcash() * 0.9
                position_size = total_cash / self.datas[0].close[0]
                self.buy(self.datas[0], size=position_size)
                logger.info(
                    f"[{current_date}] ä¹°å…¥ {self.datas[0]._name} | "
                    f"å¯ç”¨ç°é‡‘ï¼š{self.broker.getcash():.2f}å…ƒ | "
                    f"ä¹°å…¥æ•°é‡ï¼š{position_size:.0f}è‚¡ | ä»“ä½å æ¯”ï¼š90%"
                )
```

--------------------------------------------------------------------------------
## CommonProperties\Base_Properties.py

```python




######################################################################

######################  insight è´¦å·ä¿¡æ¯  #############################
user = "USER019331L1"
password = "F_Y+.3mtc4tU"


######################     å½“ä¸‹æ•°æ®ç›®å½•     #############################
dir_insight_base = r'F:\QDatas\insight_A'
dir_vantage_base = r'F:\QDatas\vantage'


######################     å†å²æ•°æ®ç›®å½•     #############################
dir_history_insight_base = r'F:\QDatas\history\insight_A'
dir_history_vantage_base = r'F:\QDatas\history\vantage'


######################     mergeæ•°æ®ç›®å½•     #############################
dir_merge_insight_base = r'F:\QDatas\merge\insight_A'
dir_merge_vantage_base = r'F:\QDatas\merge\vantage'





######################  æœ¬åœ° mysql è´¦å·ä¿¡æ¯  #############################
local_mysql_user = 'root'
local_mysql_password = "123456"
local_mysql_database = 'quant'
local_mysql_host = 'localhost'

######################  è¿œç¨‹ mysql è´¦å·ä¿¡æ¯  #############################
origin_mysql_user = "root"
# origin_mysql_password = "000000"
origin_mysql_password = "WZHwzh123!!!"
origin_mysql_host = "117.72.162.13"
origin_mysql_database = "quant"


######################  äº¬ä¸œäº‘ æ—¥å¿—æ–‡ä»¶ ç•™å­˜åœ°å€  #############################

log_file_linux_path = r"/opt/Logs"
log_file_window_path = r"F:\QDatas\logs"



######################  ä¸ªäºº é…ç½® ç•™å­˜åœ°å€  #############################

personal_linux_path = r"/opt/ss_property"
personal_window_path = r"F:\QDatas\ss_property"
personal_property_file = r"personal_property.txt"




```

--------------------------------------------------------------------------------
## CommonProperties\Base_utils.py

```python
import os
import sys
from datetime import datetime,date
import time
import traceback
import inspect
from functools import wraps
import shutil
import pandas as pd
import logging
import requests
import platform
import json

from CommonProperties.set_config import setup_logging_config


def save_out_filename(filehead, file_type):
    """
    @:param filehead       æ–‡ä»¶è¯´æ˜
    @:param file_type      æ–‡ä»¶ç±»å‹

    æ‹¼æ¥è¾“å‡ºæ–‡ä»¶çš„æ–‡ä»¶å
    """
    timestamp = datetime.now().strftime("%Y%m%d%H")
    output_filename = f"{filehead}_{timestamp}.{file_type}"
    # print("æ­£åœ¨æ‰“å°æ–‡ä»¶:{{{}}}".format(save_out_filename))
    return output_filename


def get_latest_filename(filename_dir):
    """
    è¿”å›æ—¶é—´æˆ³æœ€æ–°çš„filename   file_name: stocks_codes_all_2024070818.txt
    :return:
    """
    file_names = os.listdir(filename_dir)

    latest_date = ''
    latest_file_name = ''

    # éå†æ–‡ä»¶ååˆ—è¡¨
    for file_name in file_names:
        try:
            # ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³éƒ¨åˆ†
            timestamp = file_name.split('_')[-1].split('.')[0]

            # æ£€æŸ¥æ—¶é—´æˆ³æ˜¯å¦æ˜¯æœ€æ–°çš„
            if timestamp > latest_date:
                latest_date = timestamp
                latest_file_name = file_name
        except Exception as e:
            logging.error(r"   åœ¨å¤„ç†æ–‡ä»¶ {} æ—¶é‡åˆ°é—®é¢˜:{}".format(file_name, e))

    return latest_file_name



def collect_stock_items(input_list):
    """
    å¯¹stocks çš„listä¸­æ¯ä¸ªå…ƒç´ æŒ‰ç…§å‰ä¸‰ä½åšåˆ†ç±»æ±‡æ€»
    :param input_list:
    :return:
    """

    result_dict = {}

    for item in input_list:
        prefix = item[:3]
        if prefix not in result_dict:
            result_dict[prefix] = [item]
        else:
            result_dict[prefix].append(item)

    return result_dict




def timing_decorator(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        # è·å–å½“å‰å‡½æ•°æ‰€åœ¨çš„æ–‡ä»¶åå’Œå‡½æ•°å
        current_frame = inspect.currentframe()
        caller_frame = inspect.getouterframes(current_frame, 2)
        file_name = os.path.basename(caller_frame[1].filename)

        # åœ¨å‡½æ•°æ‰§è¡Œå‰æ‰“å°å¼€å§‹æ—¥å¿—
        logging.info(f"æ–‡ä»¶: {file_name} å‡½æ•°: {func.__name__} å¼€å§‹æ‰§è¡Œ...")

        start_time = time.time()
        try:
            result = func(*args, **kwargs)
        except Exception as e:
            logging.error(f"Error in function {func.__name__}:")
            traceback.print_exc()  # æ‰“å°è¯¦ç»†çš„å †æ ˆè¿½è¸ªä¿¡æ¯
            raise e  # é‡æ–°æŠ›å‡ºå¼‚å¸¸ï¼Œä¿æŒåŸå§‹è¡Œä¸º
        end_time = time.time()
        execution_time = end_time - start_time

        # åœ¨å‡½æ•°æ‰§è¡Œåæ‰“å°æ‰§è¡Œæ—¶é—´æ—¥å¿—
        logging.info(f"æ–‡ä»¶: {file_name} å‡½æ•°: {func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.2f} ç§’")
        return result
    return wrapper


def copy_and_rename_file(src_file_path, dest_dir, new_name):
    """
    å°†æ–‡ä»¶å¤åˆ¶åˆ°å¦ä¸€ä¸ªç›®å½•å¹¶é‡å‘½å
    :param src_file_path: æºæ–‡ä»¶è·¯å¾„
    :param dest_dir: ç›®æ ‡ç›®å½•
    :param new_name: æ–°æ–‡ä»¶å
    """
    # æ£€æŸ¥ç›®æ ‡ç›®å½•æ˜¯å¦å­˜åœ¨ï¼Œä¸å­˜åœ¨åˆ™åˆ›å»º
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)

    # ç›®æ ‡æ–‡ä»¶è·¯å¾„
    dest_file_path = os.path.join(dest_dir, new_name)

    # å¤åˆ¶æ–‡ä»¶å¹¶é‡å‘½å
    shutil.copy(src_file_path, dest_file_path)
    logging.info(f"æ–‡ä»¶å·²å¤åˆ¶å¹¶é‡å‘½åä¸º: {dest_file_path}")


def process_in_batches(df, batch_size, processing_function, **kwargs):
    """
    é€šç”¨çš„æ‰¹æ¬¡å¤„ç†å‡½æ•°ã€‚

    Args:
        df (pd.DataFrame): è¦å¤„ç†çš„æ•°æ®ã€‚
        batch_size (int): æ¯ä¸ªæ‰¹æ¬¡çš„å¤§å°ã€‚
        processing_function (callable): å¤„ç†æ¯ä¸ªæ‰¹æ¬¡çš„å‡½æ•°ã€‚
        **kwargs: ä¼ é€’ç»™å¤„ç†å‡½æ•°çš„å‚æ•°ã€‚

    Returns:
        pd.DataFrame: å¤„ç†åçš„æ€» DataFrameã€‚
    """
    def get_batches(df, batch_size):
        for start in range(0, len(df), batch_size):
            yield df[start:start + batch_size]

    total_batches = (len(df) + batch_size - 1) // batch_size
    total_df = pd.DataFrame()

    for i, batch_df in enumerate(get_batches(df, batch_size), start=1):
        sys.stdout.write(f"\rå½“å‰æ‰§è¡Œ {processing_function.__name__} çš„ ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_batches} ä¸ªæ‰¹æ¬¡")
        sys.stdout.flush()
        time.sleep(0.01)

        # ç›´æ¥è°ƒç”¨å¤„ç†å‡½æ•°ï¼Œåªä¼ é€’ **kwargs
        result = processing_function(**kwargs)
        total_df = pd.concat([total_df, result], ignore_index=True)

    sys.stdout.write("\n")
    return total_df


def get_with_retries(url, headers=None, timeout=10, max_retries=3, backoff_factor=1):
    """
    Args:
        url:
        headers:
        timeout:
        max_retries:      æœ€å¤§é‡è¯•æ¬¡æ•°
        backoff_factor:

    Returns:

    """
    retries = 0
    while retries < max_retries:
        try:
            response = requests.get(url, headers=headers, timeout=timeout)
            if response.status_code == 200:
                return response
            else:
                logging.error(f"Error: {response.status_code} - {response.text}")
        except requests.exceptions.RequestException as e:
            logging.error(f"è¯·æ±‚å¤±è´¥æŠ¥é”™: {e}")

        retries += 1
        sleep_time = backoff_factor * (2 ** retries)
        logging.info(f" {sleep_time} ç§’åå¼€å±•é‡è¯•...")
        time.sleep(sleep_time)

    logging.error(f"åœ¨ç»å† {max_retries} æ¬¡å°è¯•åè¿˜æ˜¯ä¸èƒ½æ•è·æ•°æ®")
    return None


def convert_ymd_format(df, column='ymd'):
    """
    å°† ymd åˆ—ç»Ÿä¸€è½¬æ¢ä¸º %Y-%m-%d æ ¼å¼
    Args:
        df: è¾“å…¥çš„ DataFrame
        column: éœ€è¦è½¬æ¢çš„åˆ—åï¼Œé»˜è®¤ä¸º 'ymd'
    Returns:
        df: è½¬æ¢åçš„ DataFrame
    """
    # æ£€æŸ¥ ymd åˆ—çš„æ ¼å¼
    sample_value = df[column].dropna().iloc[0] if not df[column].dropna().empty else None
    print(type(sample_value))

    # å¤„ç† sample_value æ˜¯ datetime.date ç±»å‹çš„æƒ…å†µ
    if isinstance(sample_value, date):
        df.loc[:, column] = df[column].apply(
            lambda x: x.strftime('%Y-%m-%d') if pd.notnull(x) else None
        )
        return df

    # å°† sample_value è½¬æ¢ä¸ºå­—ç¬¦ä¸²
    if sample_value is not None:
        sample_value = str(sample_value)

    # å¦‚æœ sample_value æ˜¯å­—ç¬¦ä¸²ä¸”æ ¼å¼ä¸º %Y%m%dï¼Œåˆ™è¿›è¡Œè½¬æ¢
    if sample_value is not None and len(sample_value) == 8 and sample_value.isdigit():
        df.loc[:, column] = df[column].apply(
            lambda x: pd.to_datetime(str(x), format='%Y%m%d').strftime('%Y-%m-%d')
            if pd.notnull(x) else None
        )
    # å¦‚æœ sample_value å·²ç»æ˜¯ %Y-%m-%d æ ¼å¼ï¼Œåˆ™ä¸éœ€è¦è½¬æ¢
    elif sample_value is not None and len(sample_value) == 10 and sample_value[4] == '-' and sample_value[7] == '-':
        pass  # å·²ç»æ˜¯ç›®æ ‡æ ¼å¼ï¼Œæ— éœ€è½¬æ¢
    # å¦‚æœ sample_value æ˜¯ datetime ç±»å‹ï¼Œåˆ™ç›´æ¥æ ¼å¼åŒ–ä¸º %Y-%m-%d
    elif isinstance(sample_value, pd.Timestamp):
        df.loc[:, column] = df[column].apply(
            lambda x: x.strftime('%Y-%m-%d') if pd.notnull(x) else None
        )
    else:
        raise ValueError(f"æ— æ³•è¯†åˆ«çš„ ymd åˆ—æ ¼å¼: {sample_value}")

    return df


# è°ƒç”¨æ—¥å¿—é…ç½®
# setup_logging_config()



```

--------------------------------------------------------------------------------
## CommonProperties\DateUtility.py

```python
from datetime import datetime, timedelta
import calendar


class DateUtility:
    """
    æ—¥æœŸå·¥å…·ç±»ï¼šç»Ÿä¸€åç§»è§„åˆ™ï¼ˆ0=å½“å‰å‘¨æœŸï¼Œæ­£æ•°=å¾€ånå‘¨æœŸï¼Œè´Ÿæ•°=å¾€å‰nå‘¨æœŸï¼‰
    è¾“å‡ºæ ¼å¼ï¼šæ‰€æœ‰æ—¥æœŸå‡è¿”å› YYYYMMDD å­—ç¬¦ä¸²
    """
    @staticmethod
    def today():
        """è·å–ä»Šæ—¥æ—¥æœŸ"""
        return datetime.today().strftime('%Y%m%d')


    @staticmethod
    def next_day(n=0):
        """
        è·å–åç§»nå¤©çš„æ—¥æœŸ
        :param n: å¤©æ•°åç§»é‡ï¼Œ0=ä»Šæ—¥ï¼Œæ­£æ•°=å¾€ånå¤©ï¼Œè´Ÿæ•°=å¾€å‰nå¤©
        """
        next_date = datetime.today() + timedelta(days=n)
        return next_date.strftime('%Y%m%d')


    @staticmethod
    def is_monday():
        """åˆ¤æ–­ä»Šæ—¥æ˜¯å¦æ˜¯å‘¨ä¸€"""
        today = datetime.today()
        return today.weekday() == 0  # æ˜ŸæœŸä¸€çš„weekday()è¿”å›å€¼æ˜¯0

    @staticmethod
    def is_friday():
        """åˆ¤æ–­ä»Šæ—¥æ˜¯å¦æ˜¯å‘¨äº”"""
        today = datetime.today()
        return today.weekday() == 4  # æ˜ŸæœŸäº”çš„weekday()è¿”å›å€¼æ˜¯4

    @staticmethod
    def is_weekend():
        """åˆ¤æ–­ä»Šæ—¥æ˜¯å¦æ˜¯å‘¨æœ«ï¼ˆå‘¨å…­/å‘¨æ—¥ï¼‰"""
        today = datetime.today()
        # åœ¨å¤§å¤šæ•°å›½å®¶ï¼Œå‘¨æœ«æ˜¯å‘¨å…­å’Œå‘¨æ—¥ï¼Œå³weekday()è¿”å›5ï¼ˆå‘¨å…­ï¼‰æˆ–6ï¼ˆå‘¨æ—¥ï¼‰
        return today.weekday() >= 5


    @staticmethod
    def first_day_of_week(n=0):
        """
        è·å–æŒ‡å®šåç§»å‘¨çš„ç¬¬ä¸€å¤©ï¼ˆå‘¨ä¸€ï¼‰
        :param n: å‘¨åç§»é‡ï¼Œ0=æœ¬å‘¨ï¼Œ1=ä¸‹å‘¨ï¼Œ-1=ä¸Šå‘¨
        """
        today = datetime.today()
        offset_days = -today.weekday() + n * 7
        start_of_week = today + timedelta(days=offset_days)
        return start_of_week.strftime('%Y%m%d')


    @staticmethod
    def last_day_of_week(n=0):
        """
        è·å–æŒ‡å®šåç§»å‘¨çš„æœ€åä¸€å¤©ï¼ˆå‘¨æ—¥ï¼‰
        :param n: å‘¨åç§»é‡ï¼Œ0=æœ¬å‘¨ï¼Œ1=ä¸‹å‘¨ï¼Œ-1=ä¸Šå‘¨
        """
        today = datetime.today()
        offset_days = (6 - today.weekday()) + n * 7
        last_day = today + timedelta(days=offset_days)
        return last_day.strftime('%Y%m%d')


    @staticmethod
    def first_day_of_month(n=0):
        """
        è·å–æŒ‡å®šåç§»æœˆçš„ç¬¬ä¸€å¤©
        :param n: æœˆåç§»é‡ï¼Œ0=æœ¬æœˆï¼Œ1=ä¸‹æœˆï¼Œ-1=ä¸Šæœˆ
        """
        today = datetime.today()
        month = today.month - 1 + n  ## å…ˆè½¬0-11æœˆï¼ˆä¾¿äºè®¡ç®—ï¼‰
        year = today.year + month // 12
        month = month % 12 + 1  ## è½¬å›1-12æœˆ
        first_day = datetime(year, month, 1)
        return first_day.strftime('%Y%m%d')

    @staticmethod
    def last_day_of_month(n=0):
        """
        è·å–æŒ‡å®šåç§»æœˆçš„æœ€åä¸€å¤©
        :param n: æœˆåç§»é‡ï¼Œ0=æœ¬æœˆï¼Œ1=ä¸‹æœˆï¼Œ-1=ä¸Šæœˆ
        """
        today = datetime.today()
        month = today.month - 1 + n
        year = today.year + month // 12
        month = month % 12 + 1
        last_day = calendar.monthrange(year, month)[1]  # è·å–å½“æœˆæœ€åä¸€å¤©
        last_day_date = datetime(year, month, last_day)
        return last_day_date.strftime('%Y%m%d')

    # å­£åº¦ç›¸å…³
    @staticmethod
    def first_day_of_quarter(n=0):
        """
        è·å–æŒ‡å®šåç§»å­£åº¦çš„ç¬¬ä¸€å¤©ï¼ˆå­£é¦–ï¼š1/4/7/10æœˆï¼‰
        :param n: å­£åº¦åç§»é‡ï¼Œ0=æœ¬å­£åº¦ï¼Œ1=ä¸‹å­£åº¦ï¼Œ-1=ä¸Šå­£åº¦
        """
        today = datetime.today()
        current_quarter = (today.month - 1) // 3 + 1
        target_quarter = current_quarter + n

        year = today.year + (target_quarter - 1) // 4
        quarter_month = ((target_quarter - 1) % 4) * 3 + 1
        first_day = datetime(year, quarter_month, 1)
        return first_day.strftime('%Y%m%d')

    @staticmethod
    def last_day_of_quarter(n=0):
        """
        è·å–æŒ‡å®šåç§»å­£åº¦çš„æœ€åä¸€å¤©ï¼ˆå­£æœ«ï¼š3/6/9/12æœˆï¼‰
        :param n: å­£åº¦åç§»é‡ï¼Œ0=æœ¬å­£åº¦ï¼Œ1=ä¸‹å­£åº¦ï¼Œ-1=ä¸Šå­£åº¦
        """
        today = datetime.today()
        current_quarter = (today.month - 1) // 3 + 1
        target_quarter = current_quarter + n

        year = today.year + (target_quarter - 1) // 4
        quarter_month = ((target_quarter - 1) % 4) * 3 + 3
        last_day = calendar.monthrange(year, quarter_month)[1]

        last_day_date = datetime(year, quarter_month, last_day)
        return last_day_date.strftime('%Y%m%d')


    @staticmethod
    def first_day_of_year(n=0):
        """
        è·å–æŒ‡å®šåç§»å¹´çš„ç¬¬ä¸€å¤©
        :param n: å¹´åç§»é‡ï¼Œ0=æœ¬å¹´ï¼Œ1=ä¸‹ä¸€å¹´ï¼Œ-1=ä¸Šä¸€å¹´
        """
        today = datetime.today()
        first_day = datetime(today.year + n, 1, 1)
        return first_day.strftime('%Y%m%d')


    @staticmethod
    def last_day_of_year(n=0):
        """
        è·å–æŒ‡å®šåç§»å¹´çš„æœ€åä¸€å¤©
        :param n: å¹´åç§»é‡ï¼Œ0=æœ¬å¹´ï¼Œ1=ä¸‹ä¸€å¹´ï¼Œ-1=ä¸Šä¸€å¹´
        """
        today = datetime.today()
        last_day = datetime(today.year + n, 12, 31)
        return last_day.strftime('%Y%m%d')


# æµ‹è¯•
if __name__ == "__main__":
    date_utility = DateUtility()
    print("ä»Šæ—¥æ—¥æœŸ:", date_utility.today())
    print("å½“å‰æ˜¯å¦æ˜¯å‘¨æœ«:", date_utility.is_weekend())
    print("-----------------------------------------------")
    print("æœ¬å‘¨ç¬¬ä¸€å¤©æ—¥æœŸ:", date_utility.first_day_of_week())
    print("æœ¬æœˆç¬¬1å¤©æ—¥æœŸ:", date_utility.first_day_of_month())
    print("æœ¬å­£åº¦ç¬¬ä¸€å¤©æ—¥æœŸ:", date_utility.first_day_of_quarter())
    print("æœ¬å¹´ç¬¬ä¸€å¤©æ—¥æœŸ:", date_utility.first_day_of_year())



```

--------------------------------------------------------------------------------
## CommonProperties\Mysql_Utils.py

```python

import pymysql
from sqlalchemy import create_engine, text
from sqlalchemy.exc import SQLAlchemyError
from sqlalchemy.orm import sessionmaker
import gc
from datetime import datetime, timedelta
from typing import List, Optional
import traceback  # ç”¨äºæ‰“å°è¯¦ç»†çš„é”™è¯¯å †æ ˆ

import pandas as pd
import numpy as np
import logging
import platform

import CommonProperties.Base_Properties as base_properties
import CommonProperties.Base_utils as base_utils
from CommonProperties.set_config import setup_logging_config


###################  mysql é…ç½®   ######################
local_user = base_properties.local_mysql_user
local_password = base_properties.local_mysql_password
local_database = base_properties.local_mysql_database
local_host = base_properties.local_mysql_host

origin_user = base_properties.origin_mysql_user
origin_password = base_properties.origin_mysql_password
origin_database = base_properties.origin_mysql_database
origin_host = base_properties.origin_mysql_host



def check_data_written(total_rows, table_name, engine):
    """
    ç”¨äºæŸ¥è¯¢mysqlå†™å…¥çš„æ•°æ®æ¡æ•°æ˜¯å¦å®Œæ•´
    Args:
        total_rows: è¦éªŒè¯çš„è¡¨çš„ç†è®ºä¸Šçš„è¡Œæ•°
        table_name: è¦éªŒè¯çš„è¡¨çš„åç§°
        engine:     æŸ¥è¯¢å¼•æ“
    Returns:  True æ¡æ•°éªŒè¯åŒ¹é…  / False  æ¡æ•°éªŒè¯ä¸åŒ¹é…
    """

    try:
        # åˆ›å»ºæ•°æ®åº“è¿æ¥
        connection = engine.raw_connection()
        cursor = connection.cursor()

        # æŸ¥è¯¢è¡¨ä¸­å†™å…¥çš„æ•°æ®æ€»æ•°
        check_query = f"SELECT COUNT(*) FROM {table_name}"
        cursor.execute(check_query)
        result = cursor.fetchone()[0]

        # å…³é—­è¿æ¥
        cursor.close()
        connection.close()

        return result == total_rows
    except Exception as e:
        logging.error(f"æ£€æŸ¥æ•°æ®å†™å…¥æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False


def data_from_dataframe_to_mysql(user, password, host, database='quant', df=pd.DataFrame(), table_name='', merge_on=[]):
    """
    æŠŠ dataframe ç±»å‹æ•°æ®å†™å…¥ mysql è¡¨é‡Œé¢, åŒæ—¶è°ƒç”¨äº†
    Args:
        df:
        table_name:
        database:
    Returns:
    """
    db_url = f'mysql+pymysql://{user}:{password}@{host}:3306/{database}'
    engine = create_engine(db_url)

    # å¯¹è¾“å…¥çš„dfçš„ç©ºå€¼åšå¤„ç†
    df = df.replace({np.nan: None})

    # ç¡®ä¿ df ä¸­çš„å­—æ®µåˆ—é¡ºåºä¸è¡¨ä¸­çš„åˆ—é¡ºåºä¸€è‡´
    columns = df.columns.tolist()

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡å¤æ•°æ®ï¼Œå¹¶å°†å…¶å»é™¤
    df.drop_duplicates(subset=merge_on, keep='first', inplace=True)

    total_rows = df.shape[0]
    if total_rows == 0:
        logging.info(f"æ‰€æœ‰æ•°æ®å·²å­˜åœ¨ï¼Œæ— éœ€æ’å…¥æ–°çš„æ•°æ®åˆ° {host} çš„ {table_name} è¡¨ä¸­ã€‚")
        return

    # ä½¿ç”¨ INSERT IGNORE æ¥å»é‡
    insert_sql = f"""
    INSERT IGNORE INTO {table_name} ({', '.join(columns)})
    VALUES ({', '.join([f':{col}' for col in columns])});
    """

    # è½¬æ¢ df ä¸ºä¸€ä¸ªå¯ä»¥ä¼ é€’ç»™ executemany çš„å­—å…¸åˆ—è¡¨
    values = df.to_dict('records')

    with engine.connect() as connection:
        transaction = connection.begin()
        try:
            connection.execute(text(insert_sql), values)
            transaction.commit()
            logging.info(f"æˆåŠŸæ’å…¥ {total_rows} è¡Œæ•°æ®åˆ° {host} çš„ {table_name} è¡¨ä¸­ã€‚")
        except Exception as e:
            transaction.rollback()
            logging.error(f"å†™å…¥ {host} çš„è¡¨ï¼š{table_name} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            raise


def data_from_mysql_to_dataframe(user, password, host, database='quant', table_name='', start_date=None, end_date=None, cols=None):
    """
    ä» MySQL è¡¨ä¸­è¯»å–æ•°æ®åˆ° DataFrameï¼ŒåŒæ—¶è¿›è¡Œæœ€ç»ˆçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å’Œæ—¥å¿—è®°å½•
    Args:
        table_name: MySQL è¡¨å
        database: æ•°æ®åº“åç§°
        start_date: èµ·å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        cols: è¦é€‰æ‹©çš„å­—æ®µåˆ—è¡¨

    Returns:
        df: è¯»å–åˆ°çš„ DataFrame
    """

    db_url = f'mysql+pymysql://{user}:{password}@{host}:3306/{database}'
    engine = create_engine(db_url)

    # æ„å»º SELECT è¯­å¥
    if cols:
        selected_cols = ', '.join(cols)
    else:
        selected_cols = '*'

    # æ„å»º WHERE æ¡ä»¶
    where_conditions = []
    if start_date:
        where_conditions.append(f"ymd >= '{start_date}'")
    if end_date:
        where_conditions.append(f"ymd <= '{end_date}'")

    where_clause = " AND ".join(where_conditions)

    # è¯»å– MySQL è¡¨ä¸­çš„è®°å½•æ€»æ•°
    query_total = f"SELECT COUNT(*) FROM {table_name}"
    if where_clause:
        query_total += f" WHERE {where_clause}"
    total_rows = pd.read_sql(query_total, engine).iloc[0, 0]

    # è¯»å–æ•°æ®çš„æ‰¹é‡å¤§å°
    chunk_size = 10000
    chunks = []

    try:
        for offset in range(0, total_rows, chunk_size):
            query = f"SELECT {selected_cols} FROM {table_name}"
            if where_clause:
                query += f" WHERE {where_clause}"
            query += f" LIMIT {chunk_size} OFFSET {offset}"
            chunk = pd.read_sql(query, engine)
            chunks.append(chunk)

        df = pd.concat(chunks, ignore_index=True)

        # æœ€ç»ˆçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
        if df.shape[0] == total_rows:
            logging.info(f"{host} çš„ mysqlè¡¨ï¼š{table_name} æ•°æ®è¯»å–æˆåŠŸä¸”æ— é—æ¼ï¼Œå…± {total_rows} è¡Œã€‚")
        else:
            logging.warning(f"{table_name} æ•°æ®è¯»å–å¯èƒ½æœ‰é—®é¢˜ï¼Œé¢„æœŸè®°å½•æ•°ä¸º {total_rows}ï¼Œå®é™…è¯»å–è®°å½•æ•°ä¸º {df.shape[0]}ã€‚")

    except Exception as e:
        logging.error(f"ä»è¡¨ï¼š{table_name} è¯»å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        df = pd.DataFrame()  # è¿”å›ä¸€ä¸ªç©ºçš„ DataFrame ä»¥é˜²å‡ºé”™æ—¶æ²¡æœ‰è¿”å›æ•°æ®

    return df


def data_from_mysql_to_dataframe_latest(user, password, host, database='quant', table_name='', cols=None):
    """
    ä» MySQL è¡¨ä¸­è¯»å–æœ€æ–°ä¸€å¤©çš„æ•°æ®åˆ° DataFrameï¼ŒåŒæ—¶è¿›è¡Œæœ€ç»ˆçš„æ•°æ®å®Œæ•´æ€§æ£€æŸ¥å’Œæ—¥å¿—è®°å½•
    Args:
        table_name: MySQL è¡¨å
        database: æ•°æ®åº“åç§°
        cols: è¦é€‰æ‹©çš„å­—æ®µåˆ—è¡¨

    Returns:
        df: è¯»å–åˆ°çš„ DataFrame
    """

    db_url = f'mysql+pymysql://{user}:{password}@{host}:3306/{database}'
    engine = create_engine(db_url)

    try:
        # è·å–æœ€æ–°çš„ ymd æ—¥æœŸ
        query_latest_ymd = f"SELECT MAX(ymd) FROM {table_name}"
        latest_ymd = pd.read_sql(query_latest_ymd, engine).iloc[0, 0]

        if latest_ymd is not None:
            # æ„å»º SELECT è¯­å¥
            if cols:
                selected_cols = ', '.join(cols)
            else:
                selected_cols = '*'

            # æŸ¥è¯¢æœ€æ–°ä¸€å¤©çš„æ•°æ®
            query = f"SELECT {selected_cols} FROM {table_name} WHERE ymd = '{latest_ymd}'"
            df = pd.read_sql(query, engine)

            logging.info(f"    mysqlè¡¨ï¼š{table_name} æœ€æ–°ä¸€å¤©({latest_ymd})çš„æ•°æ®è¯»å–æˆåŠŸï¼Œå…± {df.shape[0]} è¡Œã€‚")
        else:
            logging.warning(f"    {table_name} è¡¨ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ ymd æ•°æ®ã€‚")
            df = pd.DataFrame()  # è¿”å›ç©ºçš„ DataFrame

    except Exception as e:
        logging.error(f"    ä»è¡¨ï¼š{table_name} è¯»å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        df = pd.DataFrame()  # è¿”å›ä¸€ä¸ªç©ºçš„ DataFrame ä»¥é˜²å‡ºé”™æ—¶æ²¡æœ‰è¿”å›æ•°æ®

    return df


def create_partition_if_not_exists(engine, partition_name, year, month):
    next_month = month + 1
    next_year = year
    if next_month > 12:
        next_month = 1
        next_year += 1

    partition_value = next_year * 100 + next_month

    with engine.connect() as conn:
        query = text(f"""
        ALTER TABLE your_table ADD PARTITION (
            PARTITION {partition_name} VALUES LESS THAN ({partition_value})
        );
        """)
        conn.execute(query)


def upsert_table(user, password, host, database, source_table, target_table, columns):
    """
    ä½¿ç”¨ source_table ä¸­çš„æ•°æ®æ¥æ›´æ–°æˆ–æ’å…¥åˆ° target_table ä¸­ï¼ˆæç®€ç‰ˆï¼‰
    æ ¸å¿ƒåŠŸèƒ½ï¼šå­˜åœ¨åˆ™æ›´æ–°ï¼Œä¸å­˜åœ¨åˆ™æ’å…¥ï¼›å†²çªæ—¶å¿½ç•¥ï¼ˆé¿å…ä¸­æ–­ï¼‰

    :param user: æ•°æ®åº“ç”¨æˆ·å
    :param password: æ•°æ®åº“å¯†ç 
    :param host: æ•°æ®åº“ä¸»æœºIP
    :param database: æ•°æ®åº“åç§°ï¼ˆé»˜è®¤ä¸º quantï¼‰
    :param source_table: æºè¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param target_table: ç›®æ ‡è¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param columns: éœ€è¦æ›´æ–°æˆ–æ’å…¥çš„åˆ—ååˆ—è¡¨ï¼ˆåˆ—è¡¨ï¼‰
    """
    # 1. æ„å»ºæ•°æ®åº“è¿æ¥ï¼ˆåŸä»£ç é€»è¾‘ï¼Œæ— charsetå‚æ•°ï¼Œè§£å†³TypeErroré”™è¯¯ï¼‰
    db_url = f'mysql+pymysql://{user}:{password}@{host}:3306/{database}'
    engine = create_engine(db_url)

    # 2. æ„å»ºåˆ—åã€æ›´æ–°è¯­å¥ã€æŸ¥è¯¢è¯­å¥ï¼ˆåŸä»£ç é€»è¾‘ï¼Œä¿æŒä¸å˜ï¼‰
    columns_str = ", ".join(columns)
    update_str = ", ".join([f"{col} = VALUES({col})" for col in columns])
    select_str = ", ".join(columns)

    # 3. æ„å»ºSQLè¯­å¥ï¼ˆå…³é”®ä¿®æ”¹ï¼šæ·»åŠ IGNOREï¼Œè§£å†³å”¯ä¸€é”®å†²çªä¸­æ–­é—®é¢˜ï¼‰
    # åŸä»£ç ï¼šINSERT INTO {target_table} ({columns_str})
    sql = f"""
    INSERT IGNORE INTO {target_table} ({columns_str})
    SELECT {select_str}
    FROM {source_table}
    ON DUPLICATE KEY UPDATE
    {update_str};
    """

    # 4. æ‰§è¡ŒSQLè¯­å¥ï¼ˆåŸä»£ç é€»è¾‘ï¼Œä¿æŒä¸å˜ï¼‰
    with engine.connect() as connection:
        # æ·»åŠ äº‹åŠ¡æäº¤ï¼ˆåŸä»£ç ç¼ºå°‘ï¼Œè¡¥å……åä¿®æ”¹æ‰ä¼šç”Ÿæ•ˆï¼‰
        with connection.begin():
            connection.execute(text(sql))
    # å¯é€‰ï¼šå…³é—­å¼•æ“ï¼ˆéå¿…é¡»ï¼Œä½†å…»æˆå¥½ä¹ æƒ¯ï¼‰
    engine.dispose()


def cross_server_upsert_all(source_user, source_password, source_host, source_database,
                            target_user, target_password, target_host, target_database,
                            source_table, target_table):
    """
    è·¨æœåŠ¡å™¨è¿ç§»æ•°æ®ï¼Œå¹¶åœ¨ç›®æ ‡æœåŠ¡å™¨ä¸Šå®ç°æ•°æ®çš„å¹¶é›†ã€‚
    è¿™æ˜¯ä¸€ç§è¿½åŠ å–å¹¶é›†çš„æ–¹å¼

    :param source_user:      æºæœåŠ¡å™¨çš„æ•°æ®åº“ç”¨æˆ·å
    :param source_password:  æºæœåŠ¡å™¨çš„æ•°æ®åº“å¯†ç 
    :param source_host:      æºæœåŠ¡å™¨çš„ä¸»æœºåœ°å€
    :param source_database:  æºæœåŠ¡å™¨çš„æ•°æ®åº“åç§°
    :param target_user:      ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“ç”¨æˆ·å
    :param target_password:  ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“å¯†ç 
    :param target_host:      ç›®æ ‡æœåŠ¡å™¨çš„ä¸»æœºåœ°å€
    :param target_database:  ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“åç§°
    :param source_table:     æºè¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param target_table:     ç›®æ ‡è¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param columns:          éœ€è¦æ›´æ–°æˆ–æ’å…¥çš„åˆ—ååˆ—è¡¨ï¼ˆåˆ—è¡¨ï¼‰
    """

    # æºæœåŠ¡å™¨è¿æ¥
    source_db_url = f'mysql+pymysql://{source_user}:{source_password}@{source_host}:3306/{source_database}'
    source_engine = create_engine(source_db_url)

    # ç›®æ ‡æœåŠ¡å™¨è¿æ¥
    target_db_url = f'mysql+pymysql://{target_user}:{target_password}@{target_host}:3306/{target_database}'
    target_engine = create_engine(target_db_url)

    # ä»æºæœåŠ¡å™¨è¯»å–æ•°æ®
    df = pd.read_sql_table(source_table, source_engine)

    # åŠ¨æ€è·å–åˆ—å
    columns = df.columns.tolist()

    # åœ¨ç›®æ ‡æœåŠ¡å™¨åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ’å…¥æ•°æ®
    temp_table_name = 'temp_source_data'
    df.to_sql(name=temp_table_name, con=target_engine, if_exists='replace', index=False)

    # æ„å»ºåˆ—åéƒ¨åˆ†
    columns_str = ", ".join(columns)

    # æ„å»º ON DUPLICATE KEY UPDATE éƒ¨åˆ†
    update_str = ", ".join([f"{col} = VALUES({col})" for col in columns])

    # æ„å»º SELECT éƒ¨åˆ†
    select_str = ", ".join(columns)

    # æ„å»ºå®Œæ•´çš„ SQL è¯­å¥
    sql = f"""
    INSERT INTO {target_table} ({columns_str})
    SELECT {select_str}
    FROM {temp_table_name}
    ON DUPLICATE KEY UPDATE 
    {update_str};
    """

    # åœ¨ç›®æ ‡æœåŠ¡å™¨ä¸Šæ‰§è¡Œåˆå¹¶æ“ä½œ
    with target_engine.connect() as connection:
        connection.execute(text(sql))
        connection.execute(f"DROP TABLE {temp_table_name};")

    print(f"æ•°æ®å·²ä» {source_table} è¿ç§»å¹¶åˆå¹¶åˆ° {target_table}ã€‚")



def cross_server_upsert_ymd(source_user, source_password, source_host, source_database,
                            target_user, target_password, target_host, target_database,
                            source_table, target_table, start_date, end_date):
    """
    è·¨æœåŠ¡å™¨è¿ç§»æ•°æ®ï¼Œå¹¶åœ¨ç›®æ ‡æœåŠ¡å™¨ä¸Šå®ç°æ•°æ®çš„å¹¶é›†ã€‚
    è¿™æ˜¯ä¸€ç§è¿½åŠ å–å¹¶é›†çš„æ–¹å¼

    :param source_user:      æºæœåŠ¡å™¨çš„æ•°æ®åº“ç”¨æˆ·å
    :param source_password:  æºæœåŠ¡å™¨çš„æ•°æ®åº“å¯†ç 
    :param source_host:      æºæœåŠ¡å™¨çš„ä¸»æœºåœ°å€
    :param source_database:  æºæœåŠ¡å™¨çš„æ•°æ®åº“åç§°
    :param target_user:      ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“ç”¨æˆ·å
    :param target_password:  ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“å¯†ç 
    :param target_host:      ç›®æ ‡æœåŠ¡å™¨çš„ä¸»æœºåœ°å€
    :param target_database:  ç›®æ ‡æœåŠ¡å™¨çš„æ•°æ®åº“åç§°
    :param source_table:     æºè¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param target_table:     ç›®æ ‡è¡¨åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
    :param columns:          éœ€è¦æ›´æ–°æˆ–æ’å…¥çš„åˆ—ååˆ—è¡¨ï¼ˆåˆ—è¡¨ï¼‰
    """

    # æºæœåŠ¡å™¨è¿æ¥
    source_db_url = f'mysql+pymysql://{source_user}:{source_password}@{source_host}:3306/{source_database}'
    source_engine = create_engine(source_db_url)

    # ç›®æ ‡æœåŠ¡å™¨è¿æ¥
    target_db_url = f'mysql+pymysql://{target_user}:{target_password}@{target_host}:3306/{target_database}'
    target_engine = create_engine(target_db_url)

    # # ä»æºæœåŠ¡å™¨è¯»å–æ•°æ®
    # df = pd.read_sql_table(source_table, source_engine)

    # ä»æºæœåŠ¡å™¨è¯»å–æ•°æ®ï¼Œé™åˆ¶ ymd åœ¨ [start_date, end_date] å†…
    query = f"""
    SELECT * FROM {source_table}
    WHERE ymd BETWEEN '{start_date}' AND '{end_date}'
    """
    df = pd.read_sql_query(query, source_engine)

    # åŠ¨æ€è·å–åˆ—å
    columns = df.columns.tolist()

    # åœ¨ç›®æ ‡æœåŠ¡å™¨åˆ›å»ºä¸´æ—¶è¡¨å¹¶æ’å…¥æ•°æ®
    temp_table_name = 'temp_source_data'
    df.to_sql(name=temp_table_name, con=target_engine, if_exists='replace', index=False)

    # æ„å»ºåˆ—åéƒ¨åˆ†
    columns_str = ", ".join(columns)
    # æ„å»º ON DUPLICATE KEY UPDATE éƒ¨åˆ†
    update_str = ", ".join([f"{col} = VALUES({col})" for col in columns])
    # æ„å»º SELECT éƒ¨åˆ†
    select_str = ", ".join(columns)

    # æ„å»ºå®Œæ•´çš„ SQL è¯­å¥
    sql = f"""
    INSERT INTO {target_table} ({columns_str})
    SELECT {select_str}
    FROM {temp_table_name}
    ON DUPLICATE KEY UPDATE 
    {update_str};
    """

    # åœ¨ç›®æ ‡æœåŠ¡å™¨ä¸Šæ‰§è¡Œåˆå¹¶æ“ä½œ
    with target_engine.connect() as connection:
        connection.execute(text(sql))
        connection.execute(f"DROP TABLE {temp_table_name};")

    print(f"æ•°æ®å·²ä» {source_table} è¿ç§»å¹¶åˆå¹¶åˆ° {target_table}ã€‚")


def full_replace_migrate(source_host, source_db_url, target_host, target_db_url, table_name, chunk_size=10000):
    """
    å°†æœ¬åœ° MySQL æ•°æ®åº“ä¸­çš„è¡¨æ•°æ®å¯¼å…¥åˆ°è¿œç¨‹ MySQL æ•°æ®åº“ä¸­ã€‚
    æ•´ä½“æš´åŠ›è¿ç§»ï¼Œå…¨åˆ å…¨æ’

    Args:
        source_host   (str): æºç«¯ ä¸»æœº
        source_db_url (str): æºç«¯ MySQL æ•°æ®åº“çš„è¿æ¥ URL
        target_host   (str): ç›®æ ‡ ä¸»æœº
        target_db_url (str): ç›®æ ‡ MySQL æ•°æ®åº“çš„è¿æ¥ URL
        table_name    (str): è¦è¿ç§»çš„è¡¨å
        chunk_size    (int): æ¯æ¬¡è¯»å–å’Œå†™å…¥çš„æ•°æ®å—å¤§å°ï¼Œé»˜è®¤ 10000 è¡Œ
    """
    # åˆ›å»ºæºç«¯æ•°æ®åº“çš„SQLAlchemyå¼•æ“
    source_engine = create_engine(source_db_url)
    # åˆ›å»ºç›®æ ‡æ•°æ®åº“çš„SQLAlchemyå¼•æ“
    target_engine = create_engine(target_db_url)

    try:
        # 1. æ¸…ç©ºç›®æ ‡è¡¨ï¼ˆä½¿ç”¨textè¯­å¥ï¼Œé¿å…SQLæ³¨å…¥ï¼Œä¸”å•ç‹¬æ‰§è¡Œï¼‰
        # ä¸ä½¿ç”¨Sessionï¼Œç›´æ¥ç”¨engineæ‰§è¡Œï¼Œé¿å…äº‹åŠ¡éšå¼æäº¤é—®é¢˜
        with target_engine.connect() as target_conn:
            # å¼€å¯äº‹åŠ¡æ‰§è¡ŒTRUNCATE
            with target_conn.begin():
                target_conn.execute(text(f"TRUNCATE TABLE {table_name}"))
            print(f"æˆåŠŸæ¸…ç©ºç›®æ ‡è¡¨ {table_name}ã€‚")

        # 2. åˆ†æ‰¹è¯»å–æºæ•°æ®å¹¶æ’å…¥ç›®æ ‡åº“
        offset = 0
        while True:
            # åˆ†æ‰¹è¯»å–æ•°æ®ï¼šä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢ï¼ˆè™½ç„¶LIMIT/OFFSETæ— æ³•å‚æ•°åŒ–ï¼Œä½†ç”¨textå°è£…æ›´è§„èŒƒï¼‰
            # æ³¨æ„ï¼štable_nameå¦‚æœæ˜¯å¤–éƒ¨ä¼ å…¥ï¼Œéœ€åšåˆæ³•æ€§æ ¡éªŒï¼Œé¿å…SQLæ³¨å…¥
            query = text(f"SELECT * FROM {table_name} LIMIT :chunk_size OFFSET :offset")
            # ç”¨pandasè¯»å–æ•°æ®ï¼Œç›´æ¥ä½¿ç”¨engineï¼Œæ— éœ€Session
            chunk = pd.read_sql(
                query,
                con=source_engine,
                params={"chunk_size": chunk_size, "offset": offset}  # å‚æ•°åŒ–ä¼ é€’æ•°å€¼ï¼Œé¿å…æ³¨å…¥
            )

            if chunk.empty:
                break

            # æ‰¹é‡æ’å…¥ç›®æ ‡æ•°æ®åº“
            chunk.to_sql(
                name=table_name,
                con=target_engine,
                if_exists='append',
                index=False,
                chunksize=chunk_size  # å†åˆ†å—å†™å…¥ï¼Œæå‡å¤§æ•°é‡æ’å…¥æ€§èƒ½
            )
            print(f"æˆåŠŸå†™å…¥ç¬¬ {offset // chunk_size + 1} å—æ•°æ®åˆ°{target_host} mysqlåº“ã€‚")

            # æ›´æ–°åç§»é‡
            offset += chunk_size

            # é‡Šæ”¾å†…å­˜
            del chunk
            gc.collect()

        print(f"è¡¨ {table_name} æ•°æ®è¿ç§»å®Œæˆã€‚")

    except Exception as e:
        # æ‰“å°è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯å’Œå †æ ˆï¼Œæ–¹ä¾¿å®šä½é—®é¢˜
        print(f"æ•°æ®è¿ç§»è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        print("é”™è¯¯å †æ ˆä¿¡æ¯ï¼š")
        traceback.print_exc()




def get_stock_codes_latest(df):
    """
    è¿™æ˜¯ä¸ºäº†å–æœ€æ–°çš„ stock_code, é¦–å…ˆé»˜è®¤ä»ç±»å˜é‡é‡Œé¢è·å– stock_code(df), å¦‚æœdfä¸ºç©ºï¼Œå°±ä»mysqlé‡Œé¢å»å–æœ€æ–°çš„
    Args:
        df:
    Returns:
    """

    if df is None or df.empty:

        if platform.system() == "Windows":
            user = local_user
            password = local_password
            host = local_host
            database = local_database
        else:
            user = origin_user
            password = origin_password
            host = origin_host
            database = origin_database

        stock_code_df = data_from_mysql_to_dataframe_latest(user=user,
                                                            password=password,
                                                            host=host,
                                                            database=database,
                                                            table_name='ods_stock_code_daily_insight')

        mysql_stock_code_list = stock_code_df['htsc_code'].tolist()
        logging.info("    ä» æœ¬åœ°Mysqlåº“ é‡Œè¯»å–æœ€æ–°çš„è‚¡ç¥¨ä»£ç ")
    else:
        mysql_stock_code_list = df['htsc_code'].tolist()
        logging.info("    ä» self.stock_code é‡Œè¯»å–æœ€æ–°çš„è‚¡ç¥¨ä»£ç ")

    return mysql_stock_code_list


def execute_sql_statements(user, password, host, database, sql_statements):
    """
    è¿æ¥åˆ°æ•°æ®åº“å¹¶æ‰§è¡Œç»™å®šçš„ SQL è¯­å¥åˆ—è¡¨ã€‚

    å‚æ•°:
    user (str): æ•°æ®åº“ç”¨æˆ·åã€‚
    password (str): æ•°æ®åº“å¯†ç ã€‚
    host (str): æ•°æ®åº“ä¸»æœºåœ°å€ã€‚
    database (str): æ•°æ®åº“åç§°ã€‚
    sql_statements (list): åŒ…å« SQL è¯­å¥çš„åˆ—è¡¨ã€‚
    """
    # åˆ›å»ºæ•°æ®åº“è¿æ¥ URL
    db_url = f'mysql+pymysql://{user}:{password}@{host}:3306/{database}'

    # åˆ›å»ºæ•°æ®åº“å¼•æ“ï¼Œè®¾ç½®è¿æ¥æ± 
    engine = create_engine(db_url, pool_size=10, max_overflow=20, pool_recycle=3600)

    try:
        # ä½¿ç”¨è¿æ¥æ± æ‰§è¡Œ SQL è¯­å¥
        with engine.connect() as connection:
            transaction = connection.begin()  # å¼€å§‹äº‹åŠ¡
            for statement in sql_statements:
                # ä½¿ç”¨ text() æ¥é˜²æ­¢ SQL æ³¨å…¥
                connection.execute(text(statement))
            transaction.commit()  # æäº¤äº‹åŠ¡

    except SQLAlchemyError as e:
        # æ•è·æ•°æ®åº“ç›¸å…³çš„é”™è¯¯
        print(f"Error executing SQL: {e}")
    finally:
        # ç¡®ä¿è¿æ¥è¢«æ­£ç¡®å…³é—­
        engine.dispose()



```

--------------------------------------------------------------------------------
## CommonProperties\__init__.py

```python

```

--------------------------------------------------------------------------------
## CommonProperties\set_config.py

```python
# set_config.py
import logging
import colorlog
from logging.handlers import RotatingFileHandler
import os
import platform
from datetime import datetime

import json
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from email.mime.base import MIMEBase
from email import encoders

import CommonProperties.Base_Properties as base_properties
from CommonProperties.Base_Properties import log_file_linux_path, log_file_window_path, personal_linux_path, personal_window_path, personal_property_file


def get_platform_is_window():
    """
    åˆ¤æ–­å½“å‰æ“ä½œç³»ç»Ÿæ˜¯window è¿˜æ˜¯ å…¶ä»–
    Returns: True  æ˜¯windowå¹³å°
             Flase æ˜¯å…¶ä»–å¹³å°
    """
    if platform.system() == "Windows":
        return True
    else:
        return False


def read_json_file(filepath):
    """
    å¯¹ json æ–‡ä»¶çš„å¤„ç†, è¿”å›ä¸€ä¸ªdict
    Args:
        filepath:  æ–‡ä»¶è·¯å¾„
    Returns:
    """
    # è¯»å–æ–‡ä»¶
    with open(filepath, 'r', encoding='utf-8') as file:
        data = file.read()

    # è§£æ JSON æ•°æ®
    json_data = json.loads(data)

    # è¾“å‡ºè§£æç»“æœ
    return json_data


def read_personal_property():
    """
    è¯»å–ç§äººé…ç½®æ–‡ä»¶
    Returns:
    """
    if  get_platform_is_window():
        personal_window_file = os.path.join(personal_window_path, personal_property_file)
        personal_property_dict = read_json_file(personal_window_file)

    else:
        personal_linux_file = os.path.join(personal_linux_path, personal_property_file)
        personal_property_dict = read_json_file(personal_linux_file)

    return personal_property_dict


def read_logfile():
    """
    è¯»å–æ—¥å¿—æ–‡ä»¶çš„åœ°å€
    Returns:
    """
    # è·å–å½“å‰æ—¥æœŸå¹¶ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    current_date = datetime.now().strftime('%Y-%m-%d')

    if get_platform_is_window():
        log_file = f"log_windows_{current_date}.txt"
        log_filedir = os.path.join(log_file_window_path, log_file)

    else:
        log_file = f"log_linux_{current_date}.txt"
        log_filedir = os.path.join(log_file_linux_path, log_file)

    return log_filedir


def setup_logging_config():
    """
    æ—¥å¿—é…ç½®æ¨¡å—   é…ç½®logger, ä½¿å¾—æ—¥å¿—æ—¢èƒ½å¤Ÿåœ¨æ§åˆ¶å°æ‰“å°,åˆèƒ½è¾“å‡ºåˆ°.logçš„æ—¥å¿—æ–‡ä»¶ä¸­
    Returns:
    """
    # è·å–å¹¶é…ç½® root logger
    logger = logging.getLogger()

    if not logger.hasHandlers():
        # é…ç½®æ§åˆ¶å°æ—¥å¿—å¤„ç†å™¨
        console_handler = colorlog.StreamHandler()

        # è·å–å½“å‰æ—¥æœŸå¹¶ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
        current_date = datetime.now().strftime('%Y-%m-%d')

        # æ ¹æ®æ“ä½œç³»ç»Ÿç±»å‹è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
        if platform.system() == "Windows":
            log_file_window_filename = f'log_windows_{current_date}.txt'
            log_file_window = os.path.join(log_file_window_path, log_file_window_filename)
            log_file_path = log_file_window  # Windows ä¸‹çš„è·¯å¾„
        else:
            log_file_linux_filename = f'log_linux_{current_date}.txt'
            log_file_linux = os.path.join(log_file_linux_path, log_file_linux_filename)
            log_file_path = log_file_linux    # Linux ä¸‹çš„è·¯å¾„

        # é…ç½®æ–‡ä»¶æ—¥å¿—å¤„ç†å™¨ï¼ˆæ»šåŠ¨æ—¥å¿—ï¼‰
        file_handler = RotatingFileHandler(log_file_path, maxBytes=1000000, backupCount=3, mode='a')

        # è®¾ç½®å½©è‰²æ—¥å¿—çš„æ ¼å¼ï¼ŒåŒ…å«æ—¶é—´ã€æ—¥å¿—çº§åˆ«å’Œæ¶ˆæ¯å†…å®¹
        console_formatter = colorlog.ColoredFormatter(
            '%(log_color)s%(asctime)s - %(levelname)s - %(message)s',
            log_colors={
                'DEBUG': 'cyan',
                'INFO': 'green',  # å°† INFO çº§åˆ«è®¾ä¸ºç»¿è‰²
                'WARNING': 'yellow',
                'ERROR': 'red',
                'CRITICAL': 'bold_red',
            }
        )

        # è®¾ç½®æ–‡ä»¶æ—¥å¿—æ ¼å¼
        file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        # å°†æ ¼å¼åº”ç”¨åˆ°å¤„ç†å™¨
        console_handler.setFormatter(console_formatter)
        file_handler.setFormatter(file_formatter)

        # æ·»åŠ å¤„ç†å™¨åˆ° logger
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)

    # è®¾ç½®æ—¥å¿—çº§åˆ«
    logger.setLevel(logging.INFO)


def send_log_via_email():

    personal_property_dict = read_personal_property()

    # å‘ä»¶äººä¿¡æ¯
    sender_email = personal_property_dict['sender_email']
    sender_password = personal_property_dict['sender_password']

    # æ”¶ä»¶äººä¿¡æ¯
    receiver_email = personal_property_dict['receiver_email']

    # è·å–å½“å‰æ—¥æœŸå¹¶ç”Ÿæˆæ—¥å¿—æ–‡ä»¶å
    current_date = datetime.now().strftime('%Y-%m-%d')

    # æ„å»ºé‚®ä»¶
    msg = MIMEMultipart()
    msg['From'] = sender_email
    msg['To'] = receiver_email
    msg['Subject'] = f"python å‘é€quanté¡¹ç›® {current_date} çš„æ—¥å¿—æ–‡ä»¶"

    # é‚®ä»¶æ­£æ–‡
    body = f"{current_date} æ—¥çš„æ—¥å¿—æ–‡ä»¶è¯·è§é™„ä»¶"
    msg.attach(MIMEText(body, 'plain'))

    # æ·»åŠ é™„ä»¶
    logging.info("é‚®ä»¶å¼€å§‹å‘é€........")
    filename = read_logfile()
    attachment = open(filename, "rb")
    part = MIMEBase('application', 'octet-stream')
    part.set_payload((attachment).read())
    encoders.encode_base64(part)
    part.add_header('Content-Disposition', "attachment; filename= %s" % os.path.basename(filename))
    msg.attach(part)

    # è¿æ¥åˆ°SMTPæœåŠ¡å™¨
    server = smtplib.SMTP_SSL('smtp.139.com', 465)
    server.login(sender_email, sender_password)

    # å‘é€é‚®ä»¶
    text = msg.as_string()
    server.sendmail(sender_email, receiver_email, text)

    # å…³é—­è¿æ¥
    server.quit()
    logging.info("é‚®ä»¶å‘é€æˆåŠŸï¼")



if __name__ == '__main__':

    # send_log_via_email()
    sender_email = '19801322932@139.com'
    sender_password = '04b78b87377067e47800'

    try:
        server = smtplib.SMTP_SSL('smtp.139.com', 465)
        server.login(sender_email, sender_password)
        print("ç™»å½•æˆåŠŸï¼")
        server.quit()
    except smtplib.SMTPAuthenticationError as e:
        print(f"ç™»å½•å¤±è´¥: {e}")



```

--------------------------------------------------------------------------------
## dashboard\__init__.py

```python
from .strategy_dashboard import StrategyDashboard

__all__ = ['StrategyDashboard']
```

--------------------------------------------------------------------------------
## dashboard\strategy_dashboard.py

```python
import logging
import os
import warnings
import matplotlib.pyplot as plt
import plotly.graph_objects as go
import plotly.express as px
import pandas as pd
from datetime import datetime
from CommonProperties.Base_utils import timing_decorator

warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

logger = logging.getLogger(__name__)


class StrategyDashboard:
    """
    å¯è§†åŒ–é¢æ¿ï¼š
    1. æ”¶ç›Šæ›²çº¿å¯è§†åŒ–
    2. å› å­æ•ˆæœå¯è§†åŒ–
    3. é£é™©æŒ‡æ ‡å¯è§†åŒ–
    4. ç”Ÿæˆäº¤äº’å¼ä»ªè¡¨ç›˜
    """

    def __init__(self, backtest_engine, backtest_result, strategy_type):
        self.engine = backtest_engine
        self.backtest_result = backtest_result
        self.strategy_type = strategy_type
        self.dashboard_dir = "dashboard_plots"

    @timing_decorator
    def plot_equity_curve(self, cerebro, save_fig=True):
        """ç»˜åˆ¶æ”¶ç›Šæ›²çº¿"""
        logger.info("ç»˜åˆ¶æ”¶ç›Šæ›²çº¿")
        # æå–è´¦æˆ·ä»·å€¼å†å²
        equity_data = []
        strat = cerebro.runstrats[0][0] if cerebro.runstrats else None
        if not strat:
            return None

        # æ¨¡æ‹Ÿæ”¶ç›Šæ›²çº¿ï¼ˆå®é™…éœ€ä»Backtraderè·å–ï¼‰
        dates = []
        values = []
        for i, data in enumerate(strat.datas[0].datetime):
            dates.append(datetime.fromordinal(int(data)))
            # æ¨¡æ‹Ÿè´¦æˆ·ä»·å€¼å˜åŒ–ï¼ˆä»…æ¼”ç¤ºï¼Œå®é™…éœ€æ›¿æ¢ä¸ºçœŸå®å›æµ‹æ•°æ®ï¼‰
            base_value = self.backtest_result['åˆå§‹èµ„é‡‘']
            values.append(base_value * (1 + (i % 100) / 1000 * (1 if i < 50 else -0.5)))

            # ç»˜åˆ¶Matplotlibå›¾
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(dates, values, label='è´¦æˆ·æ€»èµ„äº§', color='#1f77b4', linewidth=2)
        ax.axhline(y=self.backtest_result['åˆå§‹èµ„é‡‘'], color='red', linestyle='--', label='åˆå§‹èµ„é‡‘')
        ax.set_title(f'{self.strategy_type} æ”¶ç›Šæ›²çº¿', fontsize=14, fontweight='bold')
        ax.set_xlabel('æ—¥æœŸ', fontsize=12)
        ax.set_ylabel('è´¦æˆ·ä»·å€¼ï¼ˆå…ƒï¼‰', fontsize=12)
        ax.legend()
        ax.grid(True, alpha=0.3)

        # ä¿å­˜å›¾ç‰‡
        if save_fig:
            os.makedirs(self.dashboard_dir, exist_ok=True)
            fig_path = f"{self.dashboard_dir}/equity_curve_{self.strategy_type}.png"
            plt.savefig(fig_path, dpi=300, bbox_inches='tight')
            logger.info(f"æ”¶ç›Šæ›²çº¿å·²ä¿å­˜è‡³ï¼š{fig_path}")
        plt.close(fig)  # å…³é—­ç”»å¸ƒé‡Šæ”¾å†…å­˜

        return fig

    @timing_decorator
    def plot_factor_effectiveness(self, save_fig=True):
        """ç»˜åˆ¶å› å­æ•ˆæœå¯¹æ¯”å›¾"""
        logger.info("ç»˜åˆ¶å› å­æ•ˆæœå¯¹æ¯”å›¾")
        if self.strategy_type != 'factor_driven':
            logger.warning("éå› å­é©±åŠ¨ç­–ç•¥ï¼Œè·³è¿‡å› å­æ•ˆæœç»˜å›¾")
            return None

        # æ¨¡æ‹Ÿå› å­æ•ˆæœæ•°æ®ï¼ˆå®é™…éœ€ä»å¤ç›˜æ¨¡å—/æ•°æ®åº“è·å–ï¼‰
        factor_data = {
            'å› å­ç±»å‹': ['PBå› å­', 'æ¶¨åœå› å­', 'ç­¹ç å› å­', 'ç»„åˆå› å­'],
            'ç›ˆåˆ©èƒœç‡': [
                self.backtest_result.get('pb_win_rate', 65),
                self.backtest_result.get('zt_win_rate', 58),
                self.backtest_result.get('shareholder_win_rate', 62),
                self.backtest_result.get('combo_win_rate', 75)
            ],
            'å¹³å‡æ”¶ç›Š': [2.5, 3.2, 1.8, 4.5]
        }
        factor_df = pd.DataFrame(factor_data)

        # ç»˜åˆ¶Plotlyäº¤äº’å¼å›¾
        fig = go.Figure()
        # èƒœç‡æŸ±çŠ¶å›¾
        fig.add_trace(go.Bar(
            x=factor_df['å› å­ç±»å‹'],
            y=factor_df['ç›ˆåˆ©èƒœç‡'],
            name='ç›ˆåˆ©èƒœç‡ï¼ˆ%ï¼‰',
            yaxis='y1',
            marker_color='#2ecc71'
        ))
        # å¹³å‡æ”¶ç›ŠæŠ˜çº¿å›¾
        fig.add_trace(go.Scatter(
            x=factor_df['å› å­ç±»å‹'],
            y=factor_df['å¹³å‡æ”¶ç›Š'],
            name='å¹³å‡æ”¶ç›Šï¼ˆ%ï¼‰',
            yaxis='y2',
            line=dict(color='#e74c3c', width=3)
        ))

        # å¸ƒå±€è®¾ç½®
        fig.update_layout(
            title=f'{self.strategy_type} å› å­æ•ˆæœå¯¹æ¯”',
            xaxis_title='å› å­ç±»å‹',
            yaxis=dict(
                title='ç›ˆåˆ©èƒœç‡ï¼ˆ%ï¼‰',
                titlefont=dict(color='#2ecc71'),
                tickfont=dict(color='#2ecc71'),
                range=[0, 100]
            ),
            yaxis2=dict(
                title='å¹³å‡æ”¶ç›Šï¼ˆ%ï¼‰',
                titlefont=dict(color='#e74c3c'),
                tickfont=dict(color='#e74c3c'),
                overlaying='y',
                side='right',
                range=[0, 5]
            ),
            width=1000,
            height=600,
            legend=dict(x=0.02, y=0.98)
        )

        # ä¿å­˜HTMLæ–‡ä»¶
        if save_fig:
            os.makedirs(self.dashboard_dir, exist_ok=True)
            html_path = f"{self.dashboard_dir}/factor_effectiveness_{self.strategy_type}.html"
            fig.write_html(html_path)
            logger.info(f"å› å­æ•ˆæœå›¾å·²ä¿å­˜è‡³ï¼š{html_path}")

        return fig

    @timing_decorator
    def plot_risk_metrics(self, save_fig=True):
        """ç»˜åˆ¶é£é™©æŒ‡æ ‡é›·è¾¾å›¾"""
        logger.info("ç»˜åˆ¶é£é™©æŒ‡æ ‡é›·è¾¾å›¾")
        # é£é™©æŒ‡æ ‡æ•°æ®ï¼ˆå½’ä¸€åŒ–å¤„ç†ï¼‰
        risk_metrics = {
            'æŒ‡æ ‡': ['å¹´åŒ–æ”¶ç›Šç‡', 'å¤æ™®æ¯”ç‡', 'èƒœç‡', 'ç›ˆäºæ¯”', 'æœ€å¤§å›æ’¤ï¼ˆåå‘ï¼‰'],
            'å®é™…å€¼': [
                min(self.backtest_result['å¹´åŒ–æ”¶ç›Šç‡'] / 20, 1),  # 20%å¹´åŒ–=æ»¡åˆ†
                min(self.backtest_result['å¤æ™®æ¯”ç‡'] / 2, 1),  # å¤æ™®2=æ»¡åˆ†
                min(self.backtest_result['èƒœç‡'] / 100, 1),  # 100%èƒœç‡=æ»¡åˆ†
                min(self.backtest_result['ç›ˆäºæ¯”'] / 3, 1),  # ç›ˆäºæ¯”3=æ»¡åˆ†
                max(1 - (self.backtest_result['æœ€å¤§å›æ’¤'] / 30), 0)  # 30%å›æ’¤=0åˆ†
            ],
            'ä¼˜ç§€å€¼': [1, 1, 1, 1, 1]  # ä¼˜ç§€åŸºå‡†
        }
        risk_df = pd.DataFrame(risk_metrics)

        # ç»˜åˆ¶Plotlyé›·è¾¾å›¾
        fig = go.Figure()
        fig.add_trace(go.Scatterpolar(
            r=risk_df['å®é™…å€¼'],
            theta=risk_df['æŒ‡æ ‡'],
            fill='toself',
            name='å®é™…å€¼',
            marker_color='#3498db'
        ))
        fig.add_trace(go.Scatterpolar(
            r=risk_df['ä¼˜ç§€å€¼'],
            theta=risk_df['æŒ‡æ ‡'],
            fill='toself',
            name='ä¼˜ç§€åŸºå‡†',
            marker_color='#95a5a6',
            opacity=0.3
        ))

        fig.update_layout(
            title=f'{self.strategy_type} é£é™©æ”¶ç›ŠæŒ‡æ ‡é›·è¾¾å›¾',
            polar=dict(
                radialaxis=dict(
                    visible=True,
                    range=[0, 1],
                    tickvals=[0, 0.2, 0.4, 0.6, 0.8, 1],
                    ticktext=['0', '0.2', '0.4', '0.6', '0.8', '1']
                )
            ),
            showlegend=True,
            width=800,
            height=800
        )

        # ä¿å­˜HTMLæ–‡ä»¶
        if save_fig:
            os.makedirs(self.dashboard_dir, exist_ok=True)
            html_path = f"{self.dashboard_dir}/risk_metrics_{self.strategy_type}.html"
            fig.write_html(html_path)
            logger.info(f"é£é™©æŒ‡æ ‡å›¾å·²ä¿å­˜è‡³ï¼š{html_path}")

        return fig

    @timing_decorator
    def generate_dashboard(self, cerebro, save_fig=True):
        """ç”Ÿæˆå®Œæ•´å¯è§†åŒ–ä»ªè¡¨ç›˜"""
        logger.info("======= ç”Ÿæˆç­–ç•¥å¯è§†åŒ–ä»ªè¡¨ç›˜ =======")
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.dashboard_dir, exist_ok=True)

        # ç”Ÿæˆå„ç±»å‹å›¾è¡¨
        self.plot_equity_curve(cerebro, save_fig)
        if self.strategy_type == 'factor_driven':
            self.plot_factor_effectiveness(save_fig)
        self.plot_risk_metrics(save_fig)

        # å®šä¹‰å› å­å›¾è¡¨çš„æ¡ä»¶æ¸²æŸ“ç‰‡æ®µ
        if self.strategy_type == 'factor_driven':
            factor_chart_html = f"""
            <div class="chart-container">
                <div class="chart-title">å› å­æ•ˆæœå¯¹æ¯”</div>
                <iframe src="factor_effectiveness_{self.strategy_type}.html" class="iframe-container"></iframe>
            </div>
            """
        else:
            factor_chart_html = "<!-- éå› å­ç­–ç•¥ï¼Œéšè—å› å­æ•ˆæœå›¾è¡¨ -->"

        # ç”Ÿæˆå®Œæ•´çš„ä»ªè¡¨ç›˜HTMLé¡µé¢
        dashboard_html = f"""
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <title>{self.strategy_type} ç­–ç•¥ä»ªè¡¨ç›˜</title>
            <style>
                body {{ 
                    font-family: "Microsoft YaHei", Arial, sans-serif; 
                    margin: 0; 
                    padding: 20px; 
                    background-color: #f5f7fa;
                }}
                .dashboard-title {{ 
                    text-align: center; 
                    font-size: 28px; 
                    font-weight: bold; 
                    margin-bottom: 30px; 
                    color: #2c3e50;
                }}
                .chart-container {{ 
                    margin: 30px auto; 
                    padding: 20px; 
                    background: white;
                    border-radius: 12px; 
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                    max-width: 1200px;
                }}
                .metrics-summary {{ 
                    display: flex; 
                    justify-content: space-around; 
                    flex-wrap: wrap; 
                    margin: 20px auto;
                    max-width: 1200px;
                }}
                .metric-card {{ 
                    padding: 20px; 
                    background: white;
                    border-radius: 10px; 
                    width: 180px; 
                    text-align: center; 
                    margin: 10px;
                    box-shadow: 0 2px 8px rgba(0,0,0,0.08);
                    transition: transform 0.2s;
                }}
                .metric-card:hover {{
                    transform: translateY(-5px);
                }}
                .metric-value {{ 
                    font-size: 24px; 
                    font-weight: bold; 
                    color: #2c3e50;
                    margin: 10px 0;
                }}
                .metric-label {{ 
                    font-size: 14px; 
                    color: #7f8c8d;
                }}
                .chart-title {{
                    font-size: 18px;
                    font-weight: 600;
                    color: #34495e;
                    margin-bottom: 15px;
                    border-left: 4px solid #3498db;
                    padding-left: 10px;
                }}
                .iframe-container {{
                    width: 100%;
                    height: 600px;
                    border: none;
                    border-radius: 8px;
                }}
                .img-container {{
                    width: 100%;
                    border-radius: 8px;
                    max-height: 600px;
                    object-fit: contain;
                }}
            </style>
        </head>
        <body>
            <div class="dashboard-title">{self.strategy_type} ç­–ç•¥å¯è§†åŒ–ä»ªè¡¨ç›˜</div>

            <!-- æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€» -->
            <div class="metrics-summary">
                <div class="metric-card">
                    <div class="metric-label">å¹´åŒ–æ”¶ç›Šç‡</div>
                    <div class="metric-value">{self.backtest_result['å¹´åŒ–æ”¶ç›Šç‡']}%</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">å¤æ™®æ¯”ç‡</div>
                    <div class="metric-value">{self.backtest_result['å¤æ™®æ¯”ç‡']}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">èƒœç‡</div>
                    <div class="metric-value">{self.backtest_result['èƒœç‡']}%</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">æœ€å¤§å›æ’¤</div>
                    <div class="metric-value">{self.backtest_result['æœ€å¤§å›æ’¤']}%</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">ç›ˆäºæ¯”</div>
                    <div class="metric-value">{self.backtest_result['ç›ˆäºæ¯”']}</div>
                </div>
                <div class="metric-card">
                    <div class="metric-label">ç­–ç•¥è´¨é‡å¾—åˆ†</div>
                    <div class="metric-value">{self.backtest_result['ç­–ç•¥è´¨é‡å¾—åˆ†(SQN)']}</div>
                </div>
            </div>

            <!-- æ”¶ç›Šæ›²çº¿ -->
            <div class="chart-container">
                <div class="chart-title">æ”¶ç›Šæ›²çº¿</div>
                <img src="equity_curve_{self.strategy_type}.png" class="img-container" alt="æ”¶ç›Šæ›²çº¿">
            </div>

            <!-- å› å­æ•ˆæœå¯¹æ¯”ï¼ˆä»…å› å­ç­–ç•¥æ˜¾ç¤ºï¼‰ -->
            {factor_chart_html}

            <!-- é£é™©æŒ‡æ ‡é›·è¾¾å›¾ -->
            <div class="chart-container">
                <div class="chart-title">é£é™©æ”¶ç›ŠæŒ‡æ ‡é›·è¾¾å›¾</div>
                <iframe src="risk_metrics_{self.strategy_type}.html" class="iframe-container"></iframe>
            </div>

            <!-- åº•éƒ¨ä¿¡æ¯ -->
            <div style="text-align: center; margin-top: 50px; color: #95a5a6; font-size: 14px;">
                ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} | æ•°æ®æ¥æºï¼šQuanté‡åŒ–ç³»ç»Ÿ
            </div>
        </body>
        </html>
        """

        # ä¿å­˜ä»ªè¡¨ç›˜ä¸»é¡µé¢
        dashboard_path = f"{self.dashboard_dir}/strategy_dashboard_{self.strategy_type}.html"
        with open(dashboard_path, 'w', encoding='utf-8') as f:
            f.write(dashboard_html)

        logger.info(f"âœ… å®Œæ•´ä»ªè¡¨ç›˜å·²ä¿å­˜è‡³ï¼š{dashboard_path}")
        logger.info(f"ğŸ“Œ å¯ç›´æ¥ç”¨æµè§ˆå™¨æ‰“å¼€è¯¥æ–‡ä»¶æŸ¥çœ‹å¯è§†åŒ–ç»“æœ")

        return dashboard_path
```

--------------------------------------------------------------------------------
## datas_prepare\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\run_data_prepare.sh

```bash
#!/bin/bash
# å®šä¹‰æ—¥å¿—æ–‡ä»¶åï¼ˆæ ¼å¼ï¼šrun_log_YYYY-MM-DD.txtï¼‰
LOG_FILE="/opt/run_logs/run_log_$(date +%Y-%m-%d).txt"
# æ‰§è¡ŒPythonè„šæœ¬ï¼Œå¹¶å°†è¾“å‡ºï¼ˆstdoutå’Œstderrï¼‰å†™å…¥æ—¥å¿—æ–‡ä»¶
python3 /opt/Quant/datas_prepare/setup_data_prepare.py >> "$LOG_FILE" 2>&1
```

--------------------------------------------------------------------------------
## datas_prepare\setup_data_prepare.py

```python
# -*- coding: utf-8 -*-
import sys
import os

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•ï¼ˆ/opt/quants/Quant/datas_prepareï¼‰
script_dir = os.path.dirname(os.path.abspath(__file__))
# è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆ/opt/quants/Quant/ï¼Œå³ script_dir çš„çˆ¶ç›®å½•ï¼‰
project_root = os.path.dirname(script_dir)

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ° Python æœç´¢è·¯å¾„
if project_root not in sys.path:
    sys.path.insert(0, project_root)


from datas_prepare.C01_data_download_daily.download_insight_data_afternoon import SaveInsightData
from datas_prepare.C01_data_download_daily.download_insight_data_afternoon_of_history import SaveInsightHistoryData
from datas_prepare.C01_data_download_daily.download_vantage_data_afternoon import SaveVantageData

from datas_prepare.C02_data_merge.merge_insight_data_afternoon import MergeInsightData
from datas_prepare.C03_data_DWD.calculate_DWD_datas import CalDWD
from datas_prepare.C04_data_MART.calculate_MART_datas import CalDMART


import CommonProperties.set_config as set_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯   è¿è¡Œæ•´ä¸ª DataPrepare å·¥ä½œ
# ä¸»è¦åŠŸèƒ½æ¨¡å—ï¼š
#   1. æ•°æ®ä¸‹è½½        01_data_download
#      å½“æ—¥æ•°æ®ä¸‹è½½
#         download_insight_data_afternoon.py
#         download_vantage_data_afternoon.py
#      å†å²æ•°æ®ä¸‹è½½
#         download_insight_data_afternoon_of_history
#
#   2. æ•°æ®merge      C02_data_merge
#         merge_insight_data_afternoon.py
#
# ************************************************************************



class RunDataPrepare:

    def __init__(self):
        self.save_insight_now = SaveInsightData()
        self.save_insight_history = SaveInsightHistoryData()
        self.save_vantage_now = SaveVantageData()
        self.merge_insight = MergeInsightData()
        self.dwd_cal = CalDWD()
        self.dmart_cal = CalDMART()


    def send_logfile_email(self):
        """
        èšåˆåå‘é€é‚®ä»¶çš„æœåŠ¡
        Returns:

        """
        set_config.send_log_via_email()


    def setup(self):

        #  ä¸‹è½½ insight å½“æ—¥æ•°æ®
        self.save_insight_now.setup()

        #  åˆå¹¶ insight å½“æ—¥è·‘æ‰¹çš„æ•°æ®è‡³å†å²æ•°æ®ä¸­
        self.merge_insight.setup()

        #  æ‰§è¡Œ DWDå±‚é€»è¾‘
        self.dwd_cal.setup()

        #  æ‰§è¡Œ MARTå±‚é€»è¾‘
        self.dmart_cal.setup()

        #  ä¸‹è½½ vantage å½“æ—¥æ•°æ®
        # self.save_vantage_now.setup()

        #  ä¸‹è½½å†å²æ•°æ®
        # self.save_insight_history.setup()

        #  å‘é€é‚®ä»¶
        self.send_logfile_email()


if __name__ == '__main__':
    run_data_prepare = RunDataPrepare()
    run_data_prepare.setup()


```

--------------------------------------------------------------------------------
## datas_prepare\C00_SQL\DW_mysql_tables_nopart.sql

```sql

--1.1
------------------  dwd_ashare_stock_base_info   è‚¡ç¥¨åŸºæœ¬ä¿¡æ¯å¤§å®½è¡¨
create table quant.dwd_ashare_stock_base_info (
     ymd              DATE               --æ—¥æœŸ
    ,stock_code       varchar(50)        --ä»£ç 
    ,stock_name       varchar(50)        --åç§°
    ,close            double             --æœ€æ–°æ”¶ç›˜ä»·
    ,market_value     double             --æµé€šå¸‚å€¼(äº¿)
    ,total_value      double             --æ€»å¸‚å€¼(äº¿)
    ,total_asset      double             --æ€»èµ„äº§(äº¿)
    ,net_asset        double             --å‡€èµ„äº§(äº¿)
    ,total_capital    double             --æ€»è‚¡æœ¬(äº¿)
    ,float_capital    double             --æµé€šè‚¡(äº¿)
    ,shareholder_num  bigint             --è‚¡ä¸œäººæ•°
    ,pb               varchar(50)        --å¸‚å‡€ç‡
    ,pe               varchar(50)        --å¸‚ç›ˆ(åŠ¨)
    ,market           VARCHAR(50)        --å¸‚åœºç‰¹å¾ä¸»æ¿åˆ›ä¸šæ¿ç­‰
    ,plate_names      VARCHAR(500)       --æ¿å—åç§°
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
) ;


--1.2
------------------  dwd_stock_zt_list   æ¶¨åœè‚¡ç¥¨æ¸…å•
CREATE TABLE quant.dwd_stock_zt_list (
     ymd                DATE          NOT NULL   --äº¤æ˜“æ—¥æœŸ
    ,stock_code         VARCHAR(50)   NOT NULL   --è‚¡ç¥¨ä»£ç 
    ,stock_name         VARCHAR(50)   NOT NULL   --è‚¡ç¥¨åç§°
    ,last_close         FLOAT                    --æ˜¨æ—¥æ”¶ç›˜ä»·
    ,close              FLOAT                    --æ”¶ç›˜ä»·
    ,rate               FLOAT                    --æ¶¨å¹…
    ,market_value       double                   --æµé€šå¸‚å€¼(äº¿)
    ,total_value        double                   --æ€»å¸‚å€¼(äº¿)
    ,total_asset        double                   --æ€»èµ„äº§(äº¿)
    ,net_asset          double                   --å‡€èµ„äº§(äº¿)
    ,total_capital      double                   --æ€»è‚¡æœ¬(äº¿)
    ,float_capital      double                   --æµé€šè‚¡(äº¿)
    ,shareholder_num    bigint                   --è‚¡ä¸œäººæ•°
    ,pb                 varchar(50)              --å¸‚å‡€ç‡
    ,pe                 varchar(50)              --å¸‚ç›ˆ(åŠ¨)
	,market             VARCHAR(50)              --å¸‚åœºç‰¹å¾ä¸»æ¿åˆ›ä¸šæ¿ç­‰
    ,plate_names        VARCHAR(500)             --æ¿å—åç§°
    ,concept_plate      VARCHAR(500)             --æ¦‚å¿µæ¿å—
    ,index_plate        VARCHAR(500)             --æŒ‡æ•°æ¿å—
    ,industry_plate     VARCHAR(500)             --è¡Œä¸šæ¿å—
    ,style_plate        VARCHAR(500)             --é£æ ¼æ¿å—
    ,out_plate          VARCHAR(500)             --å¤–éƒ¨æ•°æ®æ¿å—
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
);



--1.3
------------------  dwd_stock_dt_list   è·Œåœè‚¡ç¥¨æ¸…å•
CREATE TABLE quant.dwd_stock_dt_list (
     ymd                DATE          NOT NULL   --äº¤æ˜“æ—¥æœŸ
    ,stock_code         VARCHAR(50)   NOT NULL   --è‚¡ç¥¨ä»£ç 
    ,stock_name         VARCHAR(50)   NOT NULL   --è‚¡ç¥¨åç§°
    ,last_close         FLOAT                    --æ˜¨æ—¥æ”¶ç›˜ä»·
    ,close              FLOAT                    --æ”¶ç›˜ä»·
    ,rate               FLOAT                    --æ¶¨å¹…
    ,market_value       double                   --æµé€šå¸‚å€¼(äº¿)
    ,total_value        double                   --æ€»å¸‚å€¼(äº¿)
    ,total_asset        double                   --æ€»èµ„äº§(äº¿)
    ,net_asset          double                   --å‡€èµ„äº§(äº¿)
    ,total_capital      double                   --æ€»è‚¡æœ¬(äº¿)
    ,float_capital      double                   --æµé€šè‚¡(äº¿)
    ,shareholder_num    bigint                   --è‚¡ä¸œäººæ•°
    ,pb                 varchar(50)              --å¸‚å‡€ç‡
    ,pe                 varchar(50)              --å¸‚ç›ˆ(åŠ¨)
	,market             VARCHAR(50)              --å¸‚åœºç‰¹å¾ä¸»æ¿åˆ›ä¸šæ¿ç­‰
    ,plate_names        VARCHAR(500)             --æ¿å—åç§°
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
);


--4.2        å¤šæ¸ é“æ¿å—æ•°æ® -- å¤šæ¸ é“æ±‡æ€»
------------------  dwd_stock_a_total_plate
CREATE TABLE quant.dwd_stock_a_total_plate (
     ymd          DATE        NOT NULL      --æ—¥æœŸ
    ,plate_name   VARCHAR(50) NOT NULL      --æ¿å—åç§°
    ,stock_code   VARCHAR(50)               --æ ‡çš„ä»£ç 
    ,stock_name   VARCHAR(50)               --æ ‡çš„åç§°
    ,source_table VARCHAR(50)               --æ¥æºè¡¨
    ,remark       VARCHAR(50)               --å¤‡æ³¨
    ,UNIQUE KEY unique_ymd_plate_code (ymd, plate_name, stock_code)
) ;


```

--------------------------------------------------------------------------------
## datas_prepare\C00_SQL\MART_mysql_tables_nopart.sql

```sql

--1.1
------------------  dmart_stock_zt_details   è‚¡ç¥¨æ¶¨åœæ˜ç»†
create table quant.dmart_stock_zt_details (
     ymd                DATE                     --æ—¥æœŸ
    ,stock_code         varchar(50)              --ä»£ç 
    ,stock_name         varchar(50)              --åç§°
    ,concept_plate      VARCHAR(500)             --æ¦‚å¿µæ¿å—
    ,index_plate        VARCHAR(500)             --æŒ‡æ•°æ¿å—
    ,industry_plate     VARCHAR(500)             --è¡Œä¸šæ¿å—
    ,style_plate        VARCHAR(500)             --é£æ ¼æ¿å—
    ,out_plate          VARCHAR(500)             --å¤–éƒ¨æ•°æ®æ¿å—
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
) ;


------------------  dmart_stock_zt_details   è‚¡ç¥¨æ¶¨åœæ˜ç»†æ‹†åˆ†
CREATE TABLE quant.dmart_stock_zt_details_expanded (
    ymd DATE,
    stock_code VARCHAR(20),
    stock_name VARCHAR(50),
    concept_plate VARCHAR(500),
    index_plate VARCHAR(500),
    industry_plate VARCHAR(500),
    style_plate VARCHAR(500),
    out_plate VARCHAR(500)
);


```

--------------------------------------------------------------------------------
## datas_prepare\C00_SQL\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C00_SQL\create_mysql_tables.sql

```sql

--1.1
------------------  ods_stock_code_daily_insight   å½“æ—¥å·²ä¸Šå¸‚è‚¡ç¥¨ç è¡¨
CREATE TABLE quant.ods_stock_code_daily_insight (
    ymd DATE NOT NULL,
    htsc_code VARCHAR(50) NOT NULL,
    name VARCHAR(50),
    exchange VARCHAR(50),
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);


--1.2
------------------  stock_kline_daily_insight   å½“æ—¥å·²ä¸Šå¸‚è‚¡ç¥¨çš„å†å²æ—¥K
CREATE TABLE quant.ods_stock_kline_daily_insight_now (
    htsc_code VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    num_trades BIGINT,
    volume BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_stock_kline_daily_insight (
    htsc_code VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    num_trades BIGINT,
    volume BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) PARTITION BY RANGE (YEAR(ymd) * 100 + MONTH(ymd)) (
    PARTITION p202112 VALUES LESS THAN (202201),
    PARTITION p202212 VALUES LESS THAN (202301),
    PARTITION p202312 VALUES LESS THAN (202401),
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    -- æ·»åŠ å…¶ä»–æœˆä»½çš„åˆ†åŒº
    PARTITION pmax VALUES LESS THAN MAXVALUE
);


--1.3
------------------  index_a_share_insight   å¤§Açš„ä¸»è¦æŒ‡æ•°æ—¥K
CREATE TABLE quant.ods_index_a_share_insight_now (
    htsc_code VARCHAR(50) NOT NULL,
    name VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    volume BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_index_a_share_insight (
    htsc_code VARCHAR(50) NOT NULL,
    name VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    volume BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) PARTITION BY RANGE (YEAR(ymd) * 100 + MONTH(ymd)) (
    PARTITION p202112 VALUES LESS THAN (202201),
    PARTITION p202212 VALUES LESS THAN (202301),
    PARTITION p202312 VALUES LESS THAN (202401),
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    -- æ·»åŠ å…¶ä»–æœˆä»½çš„åˆ†åŒº
    PARTITION pmax VALUES LESS THAN MAXVALUE
);


--1.4
------------------  stock_limit_summary_insight   å½“æ—¥å¤§Aè¡Œæƒ…æ¸©åº¦
CREATE TABLE quant.ods_stock_limit_summary_insight_now (
    ymd DATE NOT NULL,
    name VARCHAR(50) NOT NULL,
    today_ZT INT,
    today_DT INT,
    yesterday_ZT INT,
    yesterday_DT INT,
    yesterday_ZT_rate FLOAT,
    UNIQUE KEY unique_ymd_name (ymd, name)
) ;


CREATE TABLE quant.ods_stock_limit_summary_insight (
    ymd DATE NOT NULL,
    name VARCHAR(50) NOT NULL,
    today_ZT INT,
    today_DT INT,
    yesterday_ZT INT,
    yesterday_DT INT,
    yesterday_ZT_rate FLOAT,
    UNIQUE KEY unique_ymd_name (ymd, name)
) PARTITION BY RANGE (YEAR(ymd) * 100 + MONTH(ymd)) (
    PARTITION p202112 VALUES LESS THAN (202201),
    PARTITION p202212 VALUES LESS THAN (202301),
    PARTITION p202312 VALUES LESS THAN (202401),
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    -- æ·»åŠ å…¶ä»–æœˆä»½çš„åˆ†åŒº
    PARTITION pmax VALUES LESS THAN MAXVALUE
);



--1.5
------------------  future_inside_insight   å†…ç›˜ä¸»è¦æœŸè´§æ•°æ®æ—¥K
CREATE TABLE quant.ods_future_inside_insight_now (
    htsc_code VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    volume BIGINT,
    open_interest BIGINT,
    settle BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_future_inside_insight (
    htsc_code VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    close FLOAT,
    high FLOAT,
    low FLOAT,
    volume BIGINT,
    open_interest BIGINT,
    settle BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) PARTITION BY RANGE (YEAR(ymd) * 100 + MONTH(ymd)) (
    PARTITION p202112 VALUES LESS THAN (202201),
    PARTITION p202212 VALUES LESS THAN (202301),
    PARTITION p202312 VALUES LESS THAN (202401),
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    -- æ·»åŠ å…¶ä»–æœˆä»½çš„åˆ†åŒº
    PARTITION pmax VALUES LESS THAN MAXVALUE
);



--1.6
------------------  stock_chouma_insight   Aè‚¡çš„ç­¹ç åˆ†å¸ƒæ•°æ®

CREATE TABLE quant.ods_stock_chouma_insight (
    htsc_code                                VARCHAR(50) NOT NULL
   ,ymd                                      DATE NOT NULL
   ,exchange                                 VARCHAR(50)
   ,last                                     FLOAT
   ,prev_close                               FLOAT
   ,total_share                              BIGINT
   ,a_total_share                            BIGINT
   ,a_listed_share                           BIGINT
   ,listed_share                             BIGINT
   ,restricted_share                         BIGINT
   ,cost_5pct                                FLOAT
   ,cost_15pct                               FLOAT
   ,cost_50pct                               FLOAT
   ,cost_85pct                               FLOAT
   ,cost_95pct                               FLOAT
   ,avg_cost                                 FLOAT
   ,max_cost                                 FLOAT
   ,min_cost                                 FLOAT
   ,winner_rate                              FLOAT
   ,diversity                                FLOAT
   ,pre_winner_rate                          FLOAT
   ,restricted_avg_cost                      FLOAT
   ,restricted_max_cost                      FLOAT
   ,restricted_min_cost                      FLOAT
   ,large_shareholders_avg_cost              FLOAT
   ,large_shareholders_total_share           FLOAT
   ,large_shareholders_total_share_pct       FLOAT
   ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
 ) PARTITION BY RANGE (YEAR(ymd) * 100 + MONTH(ymd)) (
    PARTITION p202112 VALUES LESS THAN (202201),
    PARTITION p202212 VALUES LESS THAN (202301),
    PARTITION p202312 VALUES LESS THAN (202401),
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION p202406 VALUES LESS THAN (202407),
    PARTITION p202407 VALUES LESS THAN (202408),
    PARTITION p202408 VALUES LESS THAN (202409),
    -- æ·»åŠ å…¶ä»–æœˆä»½çš„åˆ†åŒº
    PARTITION pmax VALUES LESS THAN MAXVALUE
);



--1.7
------------------  astock_industry_overview   è¡Œä¸šåˆ†ç±»ï¼Œç”³ä¸‡ä¸‰çº§åˆ†ç±»
CREATE TABLE quant.ods_astock_industry_overview (
    ymd                  DATE
   ,classified           varchar(100)
   ,industry_name        varchar(100)
   ,industry_code        varchar(100)
   ,l1_code              varchar(100)
   ,l1_name              varchar(100)
   ,l2_code              varchar(100)
   ,l2_name              varchar(100)
   ,l3_code              varchar(100)
   ,l3_name              varchar(100)
   ,UNIQUE KEY unique_industry_code (ymd, industry_code)
 );


--1.8
------------------  astock_industry_detail   è‚¡ç¥¨&è¡Œä¸šçš„å…³è”
CREATE TABLE quant.ods_astock_industry_detail (
    ymd              DATE
   ,htsc_code        varchar(100)
   ,name             varchar(100)
   ,industry_name    varchar(100)
   ,industry_code    varchar(100)
   ,l1_code          varchar(100)
   ,l1_name          varchar(100)
   ,l2_code          varchar(100)
   ,l2_name          varchar(100)
   ,l3_code          varchar(100)
   ,l3_name          varchar(100)
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
);


--1.9
------------------  shareholder_num   ä¸ªè‚¡çš„è‚¡ä¸œæ•°
CREATE TABLE quant.ods_shareholder_num_now (
      htsc_code              varchar(100)
     ,name                   varchar(100)
     ,ymd                    DATE
     ,total_sh               DOUBLE
     ,avg_share              DOUBLE
     ,pct_of_total_sh        DOUBLE
     ,pct_of_avg_sh          DOUBLE
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
 );


CREATE TABLE quant.ods_shareholder_num (
      htsc_code              varchar(100)
     ,name                   varchar(100)
     ,ymd                    DATE
     ,total_sh               DOUBLE
     ,avg_share              DOUBLE
     ,pct_of_total_sh        DOUBLE
     ,pct_of_avg_sh          DOUBLE
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
 );


--1.10
------------------  north_bound   åŒ—å‘æŒä»“æ•°æ®
CREATE TABLE quant.ods_north_bound_daily_now (
      htsc_code            varchar(100)
     ,ymd                  DATE
     ,sh_hkshare_hold      BIGINT
     ,pct_total_share      FLOAT
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
 );


CREATE TABLE quant.ods_north_bound_daily (
      htsc_code            varchar(100)
     ,ymd                  DATE
     ,sh_hkshare_hold      BIGINT
     ,pct_total_share      FLOAT
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
 );


--2.1
------------------  us_stock_daily_vantage   ç¾è‚¡ æ—¥K
CREATE TABLE quant.ods_us_stock_daily_vantage (
    name VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    high FLOAT,
    low FLOAT,
    close FLOAT,
    volume BIGINT,
    UNIQUE KEY unique_ymd_stock_code (ymd, name)
) ;



--2.2
------------------  exchange_rate_vantage_detail   æ±‡ç‡&ç¾å…ƒæŒ‡æ•° æ—¥K
CREATE TABLE quant.ods_exchange_rate_vantage_detail (
    name VARCHAR(50) NOT NULL,
    ymd DATE NOT NULL,
    open FLOAT,
    high FLOAT,
    low FLOAT,
    close FLOAT,
    UNIQUE KEY unique_ymd_stock_code (ymd, name)
) ;


CREATE TABLE quant.ods_exchange_dxy_vantage (
    ymd DATE NOT NULL,
    name VARCHAR(50) NOT NULL,
    UNIQUE KEY unique_ymd_stock_code (ymd, name)
) ;



```

--------------------------------------------------------------------------------
## datas_prepare\C00_SQL\create_mysql_tables_nopart.sql

```sql

--1.1
------------------  ods_stock_code_daily_insight   å½“æ—¥å·²ä¸Šå¸‚è‚¡ç¥¨ç è¡¨
CREATE TABLE quant.ods_stock_code_daily_insight (
     ymd          DATE NOT NULL            --äº¤æ˜“æ—¥æœŸ
    ,htsc_code    VARCHAR(50) NOT NULL     --è‚¡ç¥¨ä»£ç 
    ,name         VARCHAR(50)              --è‚¡ç¥¨å
    ,exchange     VARCHAR(50)              --äº¤æ˜“æ‰€åç§°
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);


--1.2
------------------  ods_stock_kline_daily_insight   å½“æ—¥å·²ä¸Šå¸‚è‚¡ç¥¨çš„å†å²æ—¥K
CREATE TABLE quant.ods_stock_kline_daily_insight_now (
     htsc_code    VARCHAR(50) NOT NULL    --è‚¡ç¥¨ä»£ç 
    ,ymd          DATE NOT NULL           --äº¤æ˜“æ—¥æœŸ
    ,open         FLOAT                   --å¼€ç›˜ä»·
    ,close        FLOAT                   --æ”¶ç›˜ä»·
    ,high         FLOAT                   --æœ€é«˜ä»·
    ,low          FLOAT                   --æœ€ä½ä»·
    ,num_trades   BIGINT                  --äº¤æ˜“ç¬”æ•°
    ,volume       BIGINT                  --æˆäº¤é‡
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_stock_kline_daily_insight (
     htsc_code    VARCHAR(50) NOT NULL    --è‚¡ç¥¨ä»£ç 
    ,ymd          DATE NOT NULL           --äº¤æ˜“æ—¥æœŸ
    ,open         FLOAT                   --å¼€ç›˜ä»·
    ,close        FLOAT                   --æ”¶ç›˜ä»·
    ,high         FLOAT                   --æœ€é«˜ä»·
    ,low          FLOAT                   --æœ€ä½ä»·
    ,num_trades   BIGINT                  --äº¤æ˜“ç¬”æ•°
    ,volume       BIGINT                  --æˆäº¤é‡
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);


--1.3
------------------  ods_index_a_share_insight   å¤§Açš„ä¸»è¦æŒ‡æ•°æ—¥K
CREATE TABLE quant.ods_index_a_share_insight_now (
     htsc_code    VARCHAR(50) NOT NULL    --æŒ‡æ•°ä»£ç 
    ,name         VARCHAR(50) NOT NULL    --æŒ‡æ•°åç§°
    ,ymd          DATE NOT NULL           --äº¤æ˜“æ—¥æœŸ
    ,open         FLOAT                   --å¼€ç›˜ä»·
    ,close        FLOAT                   --æ”¶ç›˜ä»·
    ,high         FLOAT                   --æœ€é«˜ä»·
    ,low          FLOAT                   --æœ€ä½ä»·
    ,volume       BIGINT                  --æˆäº¤é‡
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_index_a_share_insight (
     htsc_code    VARCHAR(50) NOT NULL    --æŒ‡æ•°ä»£ç 
    ,name         VARCHAR(50) NOT NULL    --æŒ‡æ•°åç§°
    ,ymd          DATE NOT NULL           --äº¤æ˜“æ—¥æœŸ
    ,open         FLOAT                   --å¼€ç›˜ä»·
    ,close        FLOAT                   --æ”¶ç›˜ä»·
    ,high         FLOAT                   --æœ€é«˜ä»·
    ,low          FLOAT                   --æœ€ä½ä»·
    ,volume       BIGINT                  --æˆäº¤é‡
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);


--1.4
------------------  ods_stock_limit_summary_insight   å½“æ—¥å¤§Aè¡Œæƒ…æ¸©åº¦
CREATE TABLE quant.ods_stock_limit_summary_insight_now (
     ymd          DATE NOT NULL           --æ—¥æœŸ
    ,name         VARCHAR(50) NOT NULL    --å¸‚åœºåç§°
    ,today_ZT     INT                     --ä»Šæ—¥æ¶¨åœè‚¡ç¥¨æ•°
    ,today_DT     INT                     --ä»Šæ—¥è·Œåœè‚¡ç¥¨æ•°
    ,yesterday_ZT INT                     --æ˜¨æ—¥æ¶¨åœè‚¡ç¥¨æ•°
    ,yesterday_DT INT                     --æ˜¨æ—¥è·Œåœè‚¡ç¥¨æ•°
    ,yesterday_ZT_rate FLOAT              --æ˜¨æ—¥æ¶¨åœè‚¡ç¥¨çš„ä»Šæ—¥å¹³å‡æ¶¨å¹…
    ,UNIQUE KEY unique_ymd_name (ymd, name)
) ;


CREATE TABLE quant.ods_stock_limit_summary_insight (
     ymd          DATE NOT NULL           --æ—¥æœŸ
    ,name         VARCHAR(50) NOT NULL    --å¸‚åœºåç§°
    ,today_ZT     INT                     --ä»Šæ—¥æ¶¨åœè‚¡ç¥¨æ•°
    ,today_DT     INT                     --ä»Šæ—¥è·Œåœè‚¡ç¥¨æ•°
    ,yesterday_ZT INT                     --æ˜¨æ—¥æ¶¨åœè‚¡ç¥¨æ•°
    ,yesterday_DT INT                     --æ˜¨æ—¥è·Œåœè‚¡ç¥¨æ•°
    ,yesterday_ZT_rate FLOAT              --æ˜¨æ—¥æ¶¨åœè‚¡ç¥¨çš„ä»Šæ—¥å¹³å‡æ¶¨å¹…
    ,UNIQUE KEY unique_ymd_name (ymd, name)
) ;


--1.5
------------------  ods_future_inside_insight   å†…ç›˜ä¸»è¦æœŸè´§æ•°æ®æ—¥K
CREATE TABLE quant.ods_future_inside_insight_now (
     htsc_code      VARCHAR(50) NOT NULL  --æœŸè´§æ ‡çš„ä»£ç 
    ,ymd            DATE NOT NULL         --äº¤æ˜“æ—¥æœŸ
    ,open           FLOAT                 --å¼€ç›˜ä»·
    ,close          FLOAT                 --æ”¶ç›˜ä»·
    ,high           FLOAT                 --æœ€é«˜ä»·
    ,low            FLOAT                 --æœ€ä½ä»·
    ,volume         BIGINT                --æˆäº¤é‡
    ,open_interest  BIGINT
    ,settle         BIGINT
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


CREATE TABLE quant.ods_future_inside_insight (
     htsc_code      VARCHAR(50) NOT NULL  --æœŸè´§æ ‡çš„ä»£ç 
    ,ymd            DATE NOT NULL         --äº¤æ˜“æ—¥æœŸ
    ,open           FLOAT                 --å¼€ç›˜ä»·
    ,close          FLOAT                 --æ”¶ç›˜ä»·
    ,high           FLOAT                 --æœ€é«˜ä»·
    ,low            FLOAT                 --æœ€ä½ä»·
    ,volume         BIGINT                --æˆäº¤é‡
    ,open_interest  BIGINT
    ,settle         BIGINT
    ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
) ;


--1.6
------------------  ods_stock_chouma_insight   Aè‚¡çš„ç­¹ç åˆ†å¸ƒæ•°æ®
CREATE TABLE quant.ods_stock_chouma_insight (
    htsc_code                                VARCHAR(50) NOT NULL     --è¯åˆ¸ä»£ç 
   ,ymd                                      DATE NOT NULL            --äº¤æ˜“æ—¥
   ,exchange                                 VARCHAR(50)              --äº¤æ˜“æ‰€
   ,last                                     FLOAT                    --æœ€æ–°ä»·æ ¼
   ,prev_close                               FLOAT                    --æ˜¨æ”¶ä»·æ ¼
   ,total_share                              BIGINT                   --æ€»è‚¡æœ¬ï¼ˆè‚¡ï¼‰
   ,a_total_share                            BIGINT                   --Aè‚¡æ€»æ•°(è‚¡)
   ,a_listed_share                           BIGINT                   --æµé€šaè‚¡ï¼ˆä¸‡è‚¡ï¼‰
   ,listed_share                             BIGINT                   --æµé€šè‚¡æ€»æ•°
   ,restricted_share                         BIGINT                   --é™å”®è‚¡æ€»æ•°
   ,cost_5pct                                FLOAT                    --5åˆ†ä½æŒä»“æˆæœ¬ï¼ˆæŒä»“æˆæœ¬æœ€ä½çš„ 5%çš„æŒä»“æˆæœ¬ï¼‰
   ,cost_15pct                               FLOAT                    --15åˆ†ä½æŒä»“æˆæœ¬
   ,cost_50pct                               FLOAT                    --50åˆ†ä½æŒä»“æˆæœ¬
   ,cost_85pct                               FLOAT                    --85åˆ†ä½æŒä»“æˆæœ¬
   ,cost_95pct                               FLOAT                    --95åˆ†ä½æŒä»“æˆæœ¬
   ,avg_cost                                 FLOAT                    --æµé€šè‚¡åŠ æƒå¹³å‡æŒä»“æˆæœ¬
   ,max_cost                                 FLOAT                    --æµé€šè‚¡æœ€å¤§æŒä»“æˆæœ¬
   ,min_cost                                 FLOAT                    --æµé€šè‚¡æœ€å°æŒä»“æˆæœ¬
   ,winner_rate                              FLOAT                    --æµé€šè‚¡è·åˆ©èƒœç‡
   ,diversity                                FLOAT                    --æµé€šè‚¡ç­¹ç åˆ†æ•£ç¨‹åº¦ç™¾åˆ†æ¯”
   ,pre_winner_rate                          FLOAT                    --æµé€šè‚¡æ˜¨æ—¥è·åˆ©èƒœç‡
   ,restricted_avg_cost                      FLOAT                    --é™å”®è‚¡å¹³å‡æŒä»“æˆæœ¬
   ,restricted_max_cost                      FLOAT                    --é™å”®è‚¡æœ€å¤§æŒä»“æˆæœ¬
   ,restricted_min_cost                      FLOAT                    --é™å”®è‚¡æœ€å°æŒä»“æˆæœ¬
   ,large_shareholders_avg_cost              FLOAT                    --å¤§æµé€šè‚¡è‚¡ä¸œæŒè‚¡å¹³å‡æŒä»“æˆæœ¬
   ,large_shareholders_total_share           FLOAT                    --å¤§æµé€šè‚¡è‚¡ä¸œæŒè‚¡æ€»æ•°
   ,large_shareholders_total_share_pct       FLOAT                    --å¤§æµé€šè‚¡è‚¡ä¸œæŒè‚¡å æ€»è‚¡æœ¬çš„æ¯”ä¾‹
   ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
 );



--1.7
------------------  ods_astock_industry_overview   è¡Œä¸šåˆ†ç±»ï¼Œç”³ä¸‡ä¸‰çº§åˆ†ç±»
CREATE TABLE quant.ods_astock_industry_overview (
    ymd                  DATE                  --äº¤æ˜“æ—¥æœŸ
   ,classified           varchar(100)          --è¡Œä¸šåˆ†ç±»
   ,industry_name        varchar(100)          --è¡Œä¸šåç§°
   ,industry_code        varchar(100)          --è¡Œä¸šä»£ç 
   ,l1_code              varchar(100)          --ä¸€çº§è¡Œä¸šä»£ç 
   ,l1_name              varchar(100)          --ä¸€çº§è¡Œä¸šåç§°
   ,l2_code              varchar(100)          --äºŒçº§è¡Œä¸šä»£ç 
   ,l2_name              varchar(100)          --äºŒçº§è¡Œä¸šåç§°
   ,l3_code              varchar(100)          --ä¸‰çº§è¡Œä¸šä»£ç 
   ,l3_name              varchar(100)          --ä¸‰çº§è¡Œä¸šåç§°
   ,UNIQUE KEY unique_industry_code (ymd, industry_code)
 );


--1.8
------------------  ods_astock_industry_detail   è‚¡ç¥¨&è¡Œä¸šçš„å…³è”
CREATE TABLE quant.ods_astock_industry_detail (
    ymd              DATE                      --äº¤æ˜“æ—¥æœŸ
   ,htsc_code        varchar(100)              --è‚¡ç¥¨ä»£ç 
   ,name             varchar(100)              --è‚¡ç¥¨åç§°
   ,industry_name    varchar(100)              --è¡Œä¸šåç§°
   ,industry_code    varchar(100)              --è¡Œä¸šä»£ç 
   ,l1_code          varchar(100)              --ä¸€çº§è¡Œä¸šä»£ç 
   ,l1_name          varchar(100)              --ä¸€çº§è¡Œä¸šåç§°
   ,l2_code          varchar(100)              --äºŒçº§è¡Œä¸šä»£ç 
   ,l2_name          varchar(100)              --äºŒçº§è¡Œä¸šåç§°
   ,l3_code          varchar(100)              --ä¸‰çº§è¡Œä¸šä»£ç 
   ,l3_name          varchar(100)              --ä¸‰çº§è¡Œä¸šåç§°
   ,UNIQUE KEY unique_industry_code (ymd, htsc_code)
);


--1.9
------------------  ods_shareholder_num   ä¸ªè‚¡çš„è‚¡ä¸œæ•°
CREATE TABLE quant.ods_shareholder_num_now (
       htsc_code              varchar(100)            --è‚¡ç¥¨ä»£ç 
      ,name                   varchar(100)            --è‚¡ç¥¨åç§°
      ,ymd                    DATE                    --äº¤æ˜“æ—¥æœŸ
      ,total_sh               DOUBLE                  --æ€»è‚¡ä¸œæ•°
      ,avg_share              DOUBLE(10, 4)           --æ¯ä¸ªè‚¡ä¸œå¹³å‡æŒè‚¡æ•°
      ,pct_of_total_sh        DOUBLE(10, 4)           --è‚¡ä¸œæ•°è¾ƒä¸ŠæœŸç¯æ¯”æ³¢åŠ¨ç™¾åˆ†æ¯”
      ,pct_of_avg_sh          DOUBLE(10, 4)           --æ¯ä¸ªè‚¡ä¸œå¹³å‡æŒè‚¡æ•°è¾ƒä¸ŠæœŸç¯æ¯”æ³¢åŠ¨ç™¾åˆ†æ¯”
      ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);

CREATE TABLE quant.ods_shareholder_num (
       htsc_code              varchar(100)            --è‚¡ç¥¨ä»£ç 
      ,name                   varchar(100)            --è‚¡ç¥¨åç§°
      ,ymd                    DATE                    --äº¤æ˜“æ—¥æœŸ
      ,total_sh               DOUBLE                  --æ€»è‚¡ä¸œæ•°
      ,avg_share              DOUBLE(10, 4)           --æ¯ä¸ªè‚¡ä¸œå¹³å‡æŒè‚¡æ•°
      ,pct_of_total_sh        DOUBLE(10, 4)           --è‚¡ä¸œæ•°è¾ƒä¸ŠæœŸç¯æ¯”æ³¢åŠ¨ç™¾åˆ†æ¯”
      ,pct_of_avg_sh          DOUBLE(10, 4)           --æ¯ä¸ªè‚¡ä¸œå¹³å‡æŒè‚¡æ•°è¾ƒä¸ŠæœŸç¯æ¯”æ³¢åŠ¨ç™¾åˆ†æ¯”
      ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
);


--1.10
------------------  ods_north_bound_daily   åŒ—å‘æŒä»“æ•°æ®
CREATE TABLE quant.ods_north_bound_daily_now (
      htsc_code            varchar(100)
     ,ymd                  DATE
     ,sh_hkshare_hold      BIGINT
     ,pct_total_share      FLOAT
     ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
 );


CREATE TABLE quant.ods_north_bound_daily (
      htsc_code            varchar(100)
     ,ymd                  DATE
     ,sh_hkshare_hold      BIGINT
     ,pct_total_share      FLOAT
     ,UNIQUE KEY unique_ymd_stock_code (ymd, htsc_code)
 );


--2.1
------------------  ods_us_stock_daily_vantage   ç¾è‚¡ æ—¥K
CREATE TABLE quant.ods_us_stock_daily_vantage (
     name     VARCHAR(50) NOT NULL          --è‚¡ç¥¨åç§°
    ,ymd      DATE        NOT NULL          --äº¤æ˜“æ—¥æœŸ
    ,open     FLOAT                         --å¼€ç›˜ä»·
    ,high     FLOAT                         --æœ€é«˜ä»·
    ,low      FLOAT                         --æœ€ä½ä»·
    ,close    FLOAT                         --æ”¶ç›˜ä»·
    ,volume   BIGINT                        --æˆäº¤é‡
    ,UNIQUE KEY unique_ymd_name (ymd, name)
) ;


--2.2
------------------  ods_exchange_rate_vantage_detail   æ±‡ç‡&ç¾å…ƒæŒ‡æ•° æ—¥K
CREATE TABLE quant.ods_exchange_rate_vantage_detail (
     name      VARCHAR(50) NOT NULL         --è´§å¸å¯¹
    ,ymd       DATE        NOT NULL         --äº¤æ˜“æ—¥æœŸ
    ,open      FLOAT                        --å¼€ç›˜ä»·
    ,high      FLOAT                        --æœ€é«˜ä»·
    ,low       FLOAT                        --æœ€ä½ä»·
    ,close     FLOAT                        --æ”¶ç›˜ä»·
    ,UNIQUE KEY unique_ymd_name (ymd, name)
) ;


--2.3
------------------  ods_exchange_dxy_vantage   ç¾å…ƒæŒ‡æ•° æ—¥K
CREATE TABLE quant.ods_exchange_dxy_vantage (
    ymd DATE NOT NULL,
    name VARCHAR(50) NOT NULL,
    UNIQUE KEY unique_ymd_name (ymd, name)
) ;


--3.1        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_concept_plate   é€šè¾¾ä¿¡æ¦‚å¿µæ¿å—æ•°æ®
CREATE TABLE quant.ods_tdx_stock_concept_plate (
     ymd DATE NOT NULL                    --æ—¥æœŸ
    ,concept_code VARCHAR(50) NOT NULL    --æ¦‚å¿µæ¿å—ä»£ç 
    ,concept_name VARCHAR(50)             --æ¦‚å¿µæ¿å—åç§°
    ,stock_code VARCHAR(50)               --è‚¡ç¥¨ä»£ç 
    ,stock_name VARCHAR(50)               --è‚¡ç¥¨åç§°
) ;


--3.2        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_style_plate   é€šè¾¾ä¿¡é£æ ¼æ¿å—æ•°æ®
CREATE TABLE quant.ods_tdx_stock_style_plate (
     ymd DATE NOT NULL                    --æ—¥æœŸ
    ,style_code VARCHAR(50) NOT NULL    --æ¦‚å¿µæ¿å—ä»£ç 
    ,style_name VARCHAR(50)             --æ¦‚å¿µæ¿å—åç§°
    ,stock_code VARCHAR(50)               --è‚¡ç¥¨ä»£ç 
    ,stock_name VARCHAR(50)               --è‚¡ç¥¨åç§°
) ;


--3.3        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_industry_plate   é€šè¾¾ä¿¡è¡Œä¸šæ¿å—æ•°æ®
CREATE TABLE quant.ods_tdx_stock_industry_plate (
     ymd DATE NOT NULL                    --æ—¥æœŸ
    ,industry_code VARCHAR(50) NOT NULL   --è¡Œä¸šæ¿å—ä»£ç 
    ,industry_name VARCHAR(50)            --è¡Œä¸šæ¿å—åç§°
    ,stock_code VARCHAR(50)               --è‚¡ç¥¨ä»£ç 
    ,stock_name VARCHAR(50)               --è‚¡ç¥¨åç§°
) ;


--3.4        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_region_plate   é€šè¾¾ä¿¡åœ°åŒºæ¿å—æ•°æ®
CREATE TABLE quant.ods_tdx_stock_region_plate (
     ymd         DATE NOT NULL            --æ—¥æœŸ
    ,region_code VARCHAR(50) NOT NULL     --åœ°åŒºæ¿å—ä»£ç 
    ,region_name VARCHAR(50)              --åœ°åŒºæ¿å—åç§°
    ,stock_code  VARCHAR(50)              --è‚¡ç¥¨ä»£ç 
    ,stock_name  VARCHAR(50)              --è‚¡ç¥¨åç§°
) ;


--3.5        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_index_plate   é€šè¾¾ä¿¡æŒ‡æ•°æ¿å—æ•°æ®
CREATE TABLE quant.ods_tdx_stock_index_plate (
     ymd         DATE NOT NULL            --æ—¥æœŸ
    ,index_code  VARCHAR(50) NOT NULL     --æŒ‡æ•°æ¿å—ä»£ç 
    ,index_name  VARCHAR(50)              --æŒ‡æ•°æ¿å—åç§°
    ,stock_code  VARCHAR(50)              --è‚¡ç¥¨ä»£ç 
    ,stock_name  VARCHAR(50)              --è‚¡ç¥¨åç§°
) ;


--3.6        é€šè¾¾ä¿¡æ•°æ®
------------------  ods_tdx_stock_pepb_info   è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®_èµ„äº§æ•°æ®
CREATE TABLE quant.ods_tdx_stock_pepb_info (
     ymd              DATE               --æ—¥æœŸ
    ,stock_code       varchar(50)        --ä»£ç 
    ,stock_name       varchar(50)        --åç§°
    ,market_value     double             --æµé€šå¸‚å€¼(äº¿)
    ,total_asset      double             --æ€»èµ„äº§(äº¿)
    ,net_asset        double             --å‡€èµ„äº§(äº¿)
    ,total_capital    double             --æ€»è‚¡æœ¬(äº¿)
    ,float_capital    double             --æµé€šè‚¡(äº¿)
    ,shareholder_num  bigint             --è‚¡ä¸œäººæ•°
    ,pb               double             --å¸‚å‡€ç‡
    ,pe               double             --å¸‚ç›ˆ(åŠ¨)
    ,industry         varchar(50)        --ç»†åˆ†è¡Œä¸š
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
) ;


--4.1        å¤šæ¸ é“æ¿å—æ•°æ® -- å°çº¢ä¹¦
------------------  ods_stock_plate_redbook
CREATE TABLE quant.ods_stock_plate_redbook (
     ymd          DATE        NOT NULL      --æ—¥æœŸ
    ,plate_name   VARCHAR(50) NOT NULL      --æ¿å—åç§°
    ,stock_code   VARCHAR(50)               --æ ‡çš„ä»£ç 
    ,stock_name   VARCHAR(50)               --æ ‡çš„åç§°
    ,remark       VARCHAR(50)               --å¤‡æ³¨
) ;


--4.2        å¤šæ¸ é“æ¿å—æ•°æ® -- å¤šæ¸ é“æ±‡æ€»
------------------  dwd_stock_a_total_plate
CREATE TABLE quant.dwd_stock_a_total_plate (
     ymd          DATE        NOT NULL      --æ—¥æœŸ
    ,plate_name   VARCHAR(50) NOT NULL      --æ¿å—åç§°
    ,stock_code   VARCHAR(50)               --æ ‡çš„ä»£ç 
    ,stock_name   VARCHAR(50)               --æ ‡çš„åç§°
    ,source_table VARCHAR(50)               --æ¥æºè¡¨
    ,remark       VARCHAR(50)               --å¤‡æ³¨
) ;


--5.1        è‚¡ç¥¨åŸºæœ¬é¢æ•°æ®_æ‰€å±äº¤æ˜“æ‰€ï¼Œä¸»æ¿/åˆ›ä¸šæ¿/ç§‘åˆ›æ¿/åŒ—è¯
------------------  ods_stock_exchange_market
CREATE TABLE quant.ods_stock_exchange_market (
     ymd          DATE        NOT NULL      --æ—¥æœŸ
    ,stock_code   VARCHAR(50)               --æ ‡çš„ä»£ç 
    ,stock_name   VARCHAR(50)               --æ ‡çš„åç§°
    ,market       VARCHAR(50)               --å¸‚åœºç‰¹å¾ä¸»æ¿åˆ›ä¸šæ¿ç­‰
    ,UNIQUE KEY unique_ymd_stock_code (ymd, stock_code)
) ;



```

--------------------------------------------------------------------------------
## datas_prepare\C01_data_download_daily\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C01_data_download_daily\download_insight_data_afternoon.py

```python
# -*- coding: utf-8 -*-

import os
import sys
import io
import numpy as np
from insight_python.com.insight import common
from insight_python.com.insight.query import *
from insight_python.com.insight.market_service import market_service
from datetime import datetime
import contextlib

import time
import logging
import platform


import CommonProperties.Base_Properties as base_properties
import CommonProperties.Base_utils as base_utils
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties.DateUtility import DateUtility
from CommonProperties.Base_utils import timing_decorator
from CommonProperties import set_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯ä¸‹åˆæ”¶ç›˜åä¸‹è½½ insight è¡Œæƒ…æºæ•°æ®, æœ¬åœ°ä¿å­˜,ç”¨äºåç»­åˆ†æ
# éœ€è¦ä¸‹è½½çš„æ•°æ®:
# 1.ä¸Šå¸‚è‚¡ç¥¨ä»£ç    get_all_stocks()
# 2.ç­¹ç åˆ†å¸ƒæ•°æ®   get_chouma_datas()


# ************************************************************************


# ************************************************************************
#  è°ƒç”¨æ—¥å¿—é…ç½®
set_config.setup_logging_config()
#  è°ƒç”¨mysqlæ—¥å¿—é…ç½®
local_user = base_properties.local_mysql_user
local_password = base_properties.local_mysql_password
local_database = base_properties.local_mysql_database
local_host = base_properties.local_mysql_host

origin_user = base_properties.origin_mysql_user
origin_password = base_properties.origin_mysql_password
origin_database = base_properties.origin_mysql_database
origin_host = base_properties.origin_mysql_host
# ************************************************************************


class SaveInsightData:

    def __init__(self):

        self.init_dirs()

        self.init_variant()

    def init_dirs(self):
        """
        å…³é”®è·¯å¾„åˆå§‹åŒ–
        """
        #  æ–‡ä»¶è·¯å¾„_____insightæ–‡ä»¶åŸºç¡€è·¯å¾„
        self.dir_insight_base = base_properties.dir_insight_base

        #  æ–‡ä»¶è·¯å¾„_____ä¸Šå¸‚äº¤æ˜“è‚¡ç¥¨codes
        self.dir_stock_codes_base = os.path.join(self.dir_insight_base, 'stock_codes')

        #  æ–‡ä»¶è·¯å¾„_____ä¸Šå¸‚äº¤æ˜“è‚¡ç¥¨çš„æ—¥kçº¿æ•°æ®
        self.dir_stock_kline_base = os.path.join(self.dir_insight_base, 'stock_kline')

        #  æ–‡ä»¶è·¯å¾„_____å…³é”®å¤§ç›˜æŒ‡æ•°
        self.dir_index_a_share_base = os.path.join(self.dir_insight_base, 'index_a_share')

        #  æ–‡ä»¶è·¯å¾„_____æ¶¨è·Œåœæ•°é‡
        self.dir_limit_summary_base = os.path.join(self.dir_insight_base, 'limit_summary')

        #  æ–‡ä»¶è·¯å¾„_____å†…ç›˜æœŸè´§
        self.dir_future_inside_base = os.path.join(self.dir_insight_base, 'future_inside')

        #  æ–‡ä»¶è·¯å¾„_____ç­¹ç æ•°æ®
        self.dir_chouma_base = os.path.join(self.dir_insight_base, 'chouma')

        #  æ–‡ä»¶è·¯å¾„_____è¡Œä¸šåˆ†ç±»æ•°æ®_æ¦‚è§ˆ
        self.dir_industry_overview_base = os.path.join(self.dir_insight_base, 'industry_overview')

        #  æ–‡ä»¶è·¯å¾„_____è¡Œä¸šåˆ†ç±»æ•°æ®_æ˜ç»†
        self.dir_industry_detail_base = os.path.join(self.dir_insight_base, 'industry_detail')

        #  æ–‡ä»¶è·¯å¾„_____ä¸ªè‚¡çš„è‚¡ä¸œæ•°_æ˜ç»†
        self.dir_shareholder_num_base = os.path.join(self.dir_insight_base, 'shareholder_num')

        #  æ–‡ä»¶è·¯å¾„_____åŒ—å‘æŒä»“æ•°æ®_æ˜ç»†
        self.dir_north_bound_base = os.path.join(self.dir_insight_base, 'north_bound')


    def init_variant(self):
        """
        ç»“æœå˜é‡åˆå§‹åŒ–
        """
        #  é™¤å» ST|é€€|B çš„äº”è¦ç´    [ymd	htsc_code	name	exchange]
        self.stock_code_df = pd.DataFrame()

        #  ä¸Šè¿°stock_code å¯¹åº”çš„æ—¥K
        self.stock_kline_df = pd.DataFrame()

        #  è·å¾—Aè‚¡å¸‚åœºçš„è‚¡æŒ‡ [htsc_code 	time	frequency	open	close	high	low	volume	value]
        self.index_a_share = pd.DataFrame()

        #  å¤§ç›˜æ¶¨è·Œåœæ•°é‡          [time	name	ä»Šæ—¥æ¶¨åœ	ä»Šæ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœ	æ˜¨æ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœè¡¨ç°]
        self.limit_summary_df = pd.DataFrame()

        #  æœŸè´§å¸‚åœºæ•°æ®    åŸæ²¹  è´µé‡‘å±  æœ‰è‰²
        self.future_index = pd.DataFrame()

        #  å¯ä»¥è·å–ç­¹ç çš„è‚¡ç¥¨æ•°æ®
        self.stock_chouma_available = pd.DataFrame()

        #  å¯ä»¥è·å–insightä¸­çš„ è¡Œä¸šåˆ†ç±» æ•°æ®æ¦‚è§ˆ
        self.industry_overview = pd.DataFrame()

        #  å¯ä»¥è·å–insightä¸­çš„ è¡Œä¸šåˆ†ç±» æ•°æ®æ˜ç»†
        self.industry_detail = pd.DataFrame()

        #  è·å–insight ä¸­ä¸ªè‚¡çš„ è‚¡ä¸œæ•°
        self.shareholder_num_df = pd.DataFrame()

        #  è·å–insight ä¸­åŒ—å‘çš„ æŒä»“æ•°æ®
        self.north_bound_df = pd.DataFrame()


    # @timing_decorator
    def login(self):
        # ç™»é™†å‰ åˆå§‹åŒ–ï¼Œæ²¡æœ‰å¯†ç å¯ä»¥è®¿é—®è¿›è¡Œè‡ªåŠ¨åŒ–æ³¨å†Œ
        # https://findata-insight.htsc.com:9151/terminalWeb/#/signup
        user = base_properties.user
        password = base_properties.password
        common.login(market_service, user, password)


    # @timing_decorator
    def get_stock_codes(self):
        """
        è·å–å½“æ—¥çš„stockä»£ç åˆé›†
        :return:
         stock_code_df  [ymd	htsc_code	name	exchange]
        """

        #  1.è·å–æ—¥æœŸ
        formatted_date = DateUtility.today()
        # formatted_date = '20240930'

        #  2.è¯·æ±‚insightæ•°æ®   get_all_stocks_info
        stock_all_df = get_all_stocks_info(listing_state="ä¸Šå¸‚äº¤æ˜“")
        print(stock_all_df.shape)
        #  3.æ—¥æœŸæ ¼å¼è½¬æ¢
        stock_all_df.insert(0, 'ymd', formatted_date)

        #  4.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        stock_all_df = stock_all_df[['ymd', 'htsc_code', 'name', 'exchange']]
        filtered_df = stock_all_df[~stock_all_df['name'].str.contains('ST|é€€|B')]

        #  5.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        filtered_df = filtered_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        #  6.æ›´æ–°dataframe ymd  htsc_code  name  exchange
        self.stock_code_df = filtered_df

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            #  7.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            filehead = 'stocks_codes_all'
            stock_codes_listed_filename = base_utils.save_out_filename(filehead=filehead, file_type='csv')
            stock_codes_listed_dir = os.path.join(self.dir_stock_codes_base, stock_codes_listed_filename)
            filtered_df.to_csv(stock_codes_listed_dir, index=False)

            #  8.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=filtered_df,
                                                     table_name="ods_stock_code_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])

            #  9.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=filtered_df,
                                                     table_name="ods_stock_code_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])
        else:
            #  9.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=filtered_df,
                                                     table_name="ods_stock_code_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])


    @timing_decorator
    def get_stock_kline(self):
        """
        æ ¹æ®å½“æ—¥ä¸Šå¸‚çš„stock_codesï¼Œæ¥è·å¾—å…¨éƒ¨(å»é™¤ST|é€€|B)è‚¡ç¥¨çš„å†å²æ•°æ®
        :return:
         stock_kline_df  [ymd	htsc_code	name	exchange]
        """

        # 1. è·å–ä»Šå¤©æ—¥æœŸ
        today = DateUtility.today()

        # 2. è®¡ç®—å½“æœˆ15å¤©å‰çš„æ—¥æœŸ
        if int(today[6:8]) > 15:
            time_start_date = DateUtility.next_day(-15)  # 15å¤©å‰
        else:
            time_start_date = DateUtility.first_day_of_month()  # å½“æœˆ1å·

        # 3. è®¾ç½®ç»“æŸæ—¥æœŸä¸ºä»Šå¤©
        time_end_date = today

        # time_start_date = '20241026'
        # time_end_date = '20241026'

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æ¯ä¸ªæ‰¹æ¬¡å– 100 ä¸ªå…ƒç´ 
        batch_size = 100

        #  3.è¿™æ˜¯ä¸€ä¸ªåˆ‡åˆ†æ‰¹æ¬¡çš„å†…éƒ¨å‡½æ•°
        def get_batches(lst, batch_size):
            for start in range(0, len(lst), batch_size):
                yield lst[start:start + batch_size]

        #  4.è·å–æœ€æ–°çš„stock_code çš„list
        stock_code_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  5.è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (len(stock_code_list) + batch_size - 1) // batch_size

        #  6.klineçš„æ€»å’Œdataframe
        kline_total_df = pd.DataFrame()

        #  7.è¯·æ±‚insightæ•°æ®  get_kline
        for i, batch_list in enumerate(get_batches(stock_code_list, batch_size), start=1):
            #  ä¸€ç§éå¸¸å·§å¦™çš„å¾ªç¯æ‰“å°æ—¥å¿—çš„æ–¹å¼
            sys.stdout.write(f"\rå½“å‰æ‰§è¡Œget_stock_klineçš„ ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_batches} ä¸ªæ‰¹æ¬¡")
            sys.stdout.flush()
            time.sleep(0.01)

            res = get_kline(htsc_code=batch_list, time=[time_start_date, time_end_date], frequency="daily", fq="pre")
            kline_total_df = pd.concat([kline_total_df, res], ignore_index=True)

        sys.stdout.write("\n")

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not kline_total_df.empty:

            #  8.æ—¥æœŸæ ¼å¼è½¬æ¢
            kline_total_df['time'] = pd.to_datetime(kline_total_df['time']).dt.strftime('%Y%m%d')
            kline_total_df.rename(columns={'time': 'ymd'}, inplace=True)

            #  9.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            kline_total_df = kline_total_df[
                ['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'num_trades', 'volume']]

            #  10.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            kline_total_df = kline_total_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

            #  11.æ›´æ–°dataframe
            self.stock_kline_df = kline_total_df

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            if platform.system() == "Windows":
                #  12.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                stock_kline_filename = base_utils.save_out_filename(filehead='stock_kline', file_type='csv')
                stcok_kline_filedir = os.path.join(self.dir_stock_kline_base, stock_kline_filename)
                kline_total_df.to_csv(stcok_kline_filedir, index=False)

                #  13.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=kline_total_df,
                                                         table_name="ods_stock_kline_daily_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])

                #  14.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=kline_total_df,
                                                         table_name="ods_stock_kline_daily_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
            else:
                #  14.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=kline_total_df,
                                                         table_name="ods_stock_kline_daily_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_stock_kline çš„è¿”å›å€¼ä¸ºç©ºå€¼')


    @timing_decorator
    def get_index_a_share(self):
        """
        000001.SH    ä¸Šè¯æŒ‡æ•°
        399002.SZ    æ·±æˆæŒ‡
        399006.SZ	 åˆ›ä¸šæ¿æŒ‡
        000016.SH    ä¸Šè¯50
        000300.SH    æ²ªæ·±300
        000849.SH    æ²ªæ·±300éé“¶è¡Œé‡‘èæŒ‡æ•°
        000905.SH	 ä¸­è¯500
        399852.SZ    ä¸­è¯1000
        000688.SH    ç§‘åˆ›50
        å½“æœˆè‡³ä»Šçš„æŒ‡æ•°
        Returns:
             index_a_share   [htsc_code 	time	frequency	open	close	high	low	volume	value]
        """

        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        start_date = DateUtility.first_day_of_month()
        end_date = DateUtility.today()

        # start_date = '20240901'
        # end_date = '20240930'

        start_date = datetime.strptime(start_date, '%Y%m%d')
        end_date = datetime.strptime(end_date, '%Y%m%d')

        #  2.æŸ¥è¯¢æ ‡çš„
        index_dict = {"000001.SH": "ä¸Šè¯æŒ‡æ•°"
            , "399002.SZ": "æ·±æˆæŒ‡"
            , "399006.SZ": "åˆ›ä¸šæ¿æŒ‡"
            , "000016.SH": "ä¸Šè¯50"
            , "000300.SH": "æ²ªæ·±300"
            , "000849.SH": "300éé“¶"
            , "000905.SH": "ä¸­è¯500"
            , "399852.SZ": "ä¸­è¯1000"
            , "000688.SH": "ç§‘åˆ›50"}
        index_list = list(index_dict.keys())

        #  3.index_a_share çš„æ€»å’Œdataframe
        index_df = pd.DataFrame()

        #  4.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_kline(htsc_code=index_list, time=[start_date, end_date],
                        frequency="daily", fq="pre")
        index_df = pd.concat([index_df, res], ignore_index=True)

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not index_df.empty:

            #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
            index_df['time'] = pd.to_datetime(index_df['time']).dt.strftime('%Y%m%d')
            index_df.rename(columns={'time': 'ymd'}, inplace=True)

            #  6.æ ¹æ®æ˜ å°„å…³ç³»ï¼Œæ·»åŠ stock_name
            index_df['name'] = index_df['htsc_code'].map(index_dict)

            #  7.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            index_df = index_df[['htsc_code', 'name', 'ymd', 'open', 'close', 'high', 'low', 'volume']]

            #  8.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            index_df = index_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  9.æ›´æ–°dataframe
            self.index_a_share = index_df

            if platform.system() == "Windows":
                #  10.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                index_filename = base_utils.save_out_filename(filehead='index_a_share', file_type='csv')
                index_filedir = os.path.join(self.dir_index_a_share_base, index_filename)
                index_df.to_csv(index_filedir, index=False)

                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=index_df,
                                                         table_name="ods_index_a_share_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])

                #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=index_df,
                                                         table_name="ods_index_a_share_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
            else:
                #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=index_df,
                                                         table_name="ods_index_a_share_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_index_a_share çš„è¿”å›å€¼ä¸ºç©ºå€¼')


    @timing_decorator
    def get_limit_summary(self):
        """
        å¤§ç›˜æ¶¨è·Œåœåˆ†ææ•°æ®
        Args:
            market:
                1	sh_a_share	ä¸Šæµ·Aè‚¡
                2	sz_a_share	æ·±åœ³Aè‚¡
                3	a_share	Aè‚¡
                4	a_share	Bè‚¡
                5	gem	åˆ›ä¸š
                6	sme	ä¸­å°æ¿
                7	star	ç§‘åˆ›æ¿
            trading_day: List<datetime>	äº¤æ˜“æ—¥æœŸèŒƒå›´ï¼Œ[start_date, end_date]

        Returns: ups_downs_limit_count_up_limits
                 ups_downs_limit_count_down_limits
                 ups_downs_limit_count_pre_up_limits
                 ups_downs_limit_count_pre_down_limits
                 ups_downs_limit_count_pre_up_limits_average_change_percent

                 [time	name	ä»Šæ—¥æ¶¨åœ	ä»Šæ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœ	æ˜¨æ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœè¡¨ç°]

        """

        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        start_date = DateUtility.first_day_of_month()
        end_date = DateUtility.today()

        # start_date = '20240901'
        # end_date = '20240930'

        start_date = datetime.strptime(start_date, '%Y%m%d')
        end_date = datetime.strptime(end_date, '%Y%m%d')

        #  2.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_change_summary(market=["a_share"], trading_day=[start_date, end_date])

        #  3.limit_summary çš„æ€»å’Œdataframe
        limit_summary_df = pd.DataFrame()
        limit_summary_df = pd.concat([limit_summary_df, res], ignore_index=True)

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not limit_summary_df.empty:
            #  4.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            limit_summary_df = limit_summary_df[['time',
                                                 'name',
                                                 'ups_downs_limit_count_up_limits',
                                                 'ups_downs_limit_count_down_limits',
                                                 'ups_downs_limit_count_pre_up_limits',
                                                 'ups_downs_limit_count_pre_down_limits',
                                                 'ups_downs_limit_count_pre_up_limits_average_change_percent']]
            limit_summary_df.columns = ['ymd', 'name', 'today_ZT', 'today_DT', 'yesterday_ZT', 'yesterday_DT',
                                        'yesterday_ZT_rate']

            #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
            limit_summary_df['ymd'] = pd.to_datetime(limit_summary_df['ymd']).dt.strftime('%Y%m%d')

            #  6.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            limit_summary_df = limit_summary_df.drop_duplicates(subset=['ymd', 'name'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  7.æ›´æ–°dataframe
            self.limit_summary_df = limit_summary_df

            if platform.system() == "Windows":
                #  8.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                test_summary_filename = base_utils.save_out_filename(filehead='stock_limit_summary', file_type='csv')
                test_summary_dir = os.path.join(self.dir_limit_summary_base, test_summary_filename)
                limit_summary_df.to_csv(test_summary_dir, index=False)

                #  9.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=limit_summary_df,
                                                         table_name="ods_stock_limit_summary_insight_now",
                                                         merge_on=['ymd', 'name'])

                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=limit_summary_df,
                                                         table_name="ods_stock_limit_summary_insight_now",
                                                         merge_on=['ymd', 'name'])
            else:
                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=limit_summary_df,
                                                         table_name="ods_stock_limit_summary_insight_now",
                                                         merge_on=['ymd', 'name'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_limit_summary çš„è¿”å›å€¼ä¸ºç©ºå€¼')


    @timing_decorator
    def get_future_inside(self):
        """
        æœŸè´§å¸‚åœºæ•°æ®
        è´µé‡‘å±,  æœ‰è‰²æ•°æ®
        å›½é™…å¸‚åœº  å›½å†…å¸‚åœº
        AU9999.SHF    æ²ªé‡‘ä¸»è¿
        AU2409.SHF	  æ²ªé‡‘
        AG9999.SHF    æ²ªé“¶ä¸»è¿
        AG2409.SHF    æ²ªé“¶
        CU9999.SHF    æ²ªé“œä¸»è¿
        CU2409.SHF    æ²ªé“œ

        EC9999.INE    æ¬§çº¿é›†è¿ä¸»è¿
        EC2410.INE    æ¬§çº¿é›†è¿
        SC9999.INE    åŸæ²¹ä¸»è¿
        SC2410.INE    åŸæ²¹

        V9999.DCE     PVCä¸»è¿
        V2409.DCE     PVC
        MA9999.ZCE    ç”²é†‡ä¸»è¿      (æ‰¾ä¸åˆ°)
        MA2409.ZCE    ç”²é†‡         (æ‰¾ä¸åˆ°)
        ç›®å‰ä¸»è¿æ‰¾ä¸åˆ°æ•°æ®ï¼Œåªæœ‰æœˆä»½çš„ï¼Œæš‚æ—¶ç”¨ t+2 æœˆå»ä»£æ›¿ä¸»è¿å§

        Returns:
        """
        #  1.èµ·æ­¢æ—¶é—´ æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™2æœˆå‰çš„æœˆåˆç¬¬1å¤©
        time_start_date = DateUtility.first_day_of_month(-2)
        time_end_date = DateUtility.today()

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æŸ¥è¯¢æ ‡çš„
        index_list = ["AU{}.SHF", "AG{}.SHF", "CU{}.SHF", "EC{}.INE", "SC{}.INE", "V{}.DCE"]
        replacement = DateUtility.first_day_of_month(2)[2:6]

        future_index_list = [index.format(replacement) for index in index_list]

        #  3.future_inside çš„æ€»å’Œdataframe
        future_inside_df = pd.DataFrame()

        #  4.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_kline(htsc_code=future_index_list, time=[time_start_date, time_end_date],
                        frequency="daily", fq="pre")
        future_inside_df = pd.concat([future_inside_df, res], ignore_index=True)

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not future_inside_df.empty:

            #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
            future_inside_df['time'] = pd.to_datetime(future_inside_df['time']).dt.strftime('%Y%m%d')
            future_inside_df.rename(columns={'time': 'ymd'}, inplace=True)

            #  6.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            future_inside_df = future_inside_df[
                ['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'volume', 'open_interest', 'settle']]

            #  7.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            future_inside_df = future_inside_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  8.æ›´æ–°dataframe
            self.future_index = future_inside_df

            if platform.system() == "Windows":
                #  9.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                # future_inside_df = future_inside_df.fillna(value=None)
                future_inside_df_filename = base_utils.save_out_filename(filehead='future_inside', file_type='csv')
                future_inside_df_filedir = os.path.join(self.dir_future_inside_base, future_inside_df_filename)
                future_inside_df.to_csv(future_inside_df_filedir, index=False)

                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=future_inside_df,
                                                         table_name="ods_future_inside_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])

                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=future_inside_df,
                                                         table_name="ods_future_inside_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
            else:
                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=future_inside_df,
                                                         table_name="ods_future_inside_insight_now",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_future_inside çš„è¿”å›å€¼ä¸ºç©ºå€¼')


    @timing_decorator
    def get_chouma_datas(self):
        """
        1.è·å–æ¯æ—¥çš„ç­¹ç åˆ†å¸ƒæ•°æ®
        2.æ‰¾åˆ°é‚£äº›å½“æ—¥èƒ½å¤Ÿæ‹¿åˆ°ç­¹ç æ•°æ®çš„codes
        :return:
        """
        #  1.èµ·æ­¢æ—¶é—´ æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™æœ¬æœˆæœˆåˆ
        time_start_date = DateUtility.first_day_of_month()
        #  ç»“æŸæ—¶é—´å¿…é¡»å¤§äºç­‰äºå½“æ—¥ï¼Œè¿™é‡Œå–æ˜å¤©çš„æ—¥æœŸ
        time_end_date = DateUtility.next_day(1)

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æ¯ä¸ªæ‰¹æ¬¡å– 100 ä¸ªå…ƒç´ ï¼ˆåŸä»£ç æ˜¯1ï¼Œä¿ç•™ä½ çš„é…ç½®ï¼‰
        batch_size = 1

        #  3.è¿™æ˜¯ä¸€ä¸ªåˆ‡åˆ†æ‰¹æ¬¡çš„å†…éƒ¨å‡½æ•°
        def get_batches(lst, batch_size):
            for start in range(0, len(lst), batch_size):
                yield lst[start:start + batch_size]

        #  4.è·å–æœ€æ–°çš„stock_code_list
        stock_code_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  5.è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (len(stock_code_list) + batch_size - 1) // batch_size

        #  6.chouma çš„æ€»å’Œdataframe
        chouma_total_df = pd.DataFrame()

        #  7.è°ƒç”¨insightæ•°æ®  get_chip_distribution
        for i, code_list in enumerate(get_batches(stock_code_list, batch_size), start=1):
            #  ä¸€ç§éå¸¸å·§å¦™çš„å¾ªç¯æ‰“å°æ—¥å¿—çš„æ–¹å¼
            valid_num = chouma_total_df.shape[0]
            sys.stdout.write(
                f"\rå½“å‰æ‰§è¡Œ get_chouma_datas  ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_batches} ä¸ªæ‰¹æ¬¡, {valid_num}ä¸ªæœ‰æ•ˆç­¹ç æ•°æ®")
            sys.stdout.flush()
            time.sleep(0.01)

            try:
                res = get_chip_distribution(htsc_code=code_list, trading_day=[time_start_date, time_end_date])
                chouma_total_df = pd.concat([chouma_total_df, res], ignore_index=True)
            except Exception as e:
                continue
            time.sleep(0.1)

        sys.stdout.write("\n")

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not chouma_total_df.empty:
            #  8.æ—¥æœŸæ ¼å¼è½¬æ¢
            chouma_total_df['time'] = pd.to_datetime(chouma_total_df['time']).dt.strftime('%Y%m%d')
            chouma_total_df.rename(columns={'time': 'ymd'}, inplace=True)

            #  9.æ•°æ®æ ¼å¼è°ƒæ•´
            cols_to_clean = ['last', 'prev_close', 'avg_cost', 'max_cost', 'min_cost', 'winner_rate', 'diversity',
                             'pre_winner_rate', 'restricted_avg_cost', 'restricted_max_cost', 'restricted_min_cost',
                             'large_shareholders_avg_cost', 'large_shareholders_total_share_pct']

            for col in cols_to_clean:
                # ========== æ ¸å¿ƒä¿®æ”¹1ï¼šä¿®å¤inplace=Trueè­¦å‘Šï¼Œåˆå¹¶å†—ä½™æ­¥éª¤ ==========
                # åŸä»£ç ï¼šå…ˆè½¬å­—ç¬¦ä¸²â†’replace(inplace)â†’to_numericâ†’fillna(inplace)â†’apply
                # ä¼˜åŒ–åï¼šé“¾å¼è°ƒç”¨ï¼Œä¸€æ¬¡éå†å®Œæˆæ‰€æœ‰æ“ä½œï¼Œå»æ‰inplace=True
                chouma_total_df[col] = (
                    chouma_total_df[col]
                    # è½¬ä¸ºå­—ç¬¦ä¸²ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
                    .astype(str)
                    # æ›¿æ¢ç©ºå­—ç¬¦ä¸²å’Œ'nan'ä¸ºNaNï¼ˆå»æ‰inplaceï¼Œç›´æ¥èµ‹å€¼ï¼‰
                    .replace({'': np.nan, 'nan': np.nan})
                    # è½¬æ¢ä¸ºfloatï¼Œé”™è¯¯è¿”å›NaN
                    .pipe(lambda s: pd.to_numeric(s, errors='coerce'))
                    # å¡«å……NaNä¸º0ï¼ˆå»æ‰inplaceï¼Œç›´æ¥èµ‹å€¼ï¼‰
                    .fillna(0)
                    # ä»·æ ¼è½¬æ¢é€»è¾‘ï¼ˆä¿ç•™åŸé€»è¾‘ï¼‰
                    .apply(lambda x: round(x * 10000, 2) if x < 1 else x)
                )

            # ========== æ ¸å¿ƒä¿®æ”¹2ï¼šä¿®å¤applymapå·²å¼ƒç”¨è­¦å‘Š ==========
            # åŸä»£ç ï¼šchouma_total_df[cols_to_clean] = chouma_total_df[cols_to_clean].applymap(lambda x: f"{x:.2f}")
            # ä¿®å¤æ–¹æ¡ˆ1ï¼šå¦‚æœæ˜¯å¤šåˆ—ï¼Œç”¨apply + mapï¼ˆå…¼å®¹æ‰€æœ‰pandasç‰ˆæœ¬ï¼‰
            chouma_total_df[cols_to_clean] = chouma_total_df[cols_to_clean].apply(
                lambda s: s.map(lambda x: f"{x:.2f}")
            )
            # ä¿®å¤æ–¹æ¡ˆ2ï¼šå¦‚æœæƒ³ä¿ç•™æ•°å€¼ç±»å‹ï¼ˆæ¨èï¼Œé¿å…åç»­æ•°æ®åº“æ’å…¥çš„ç±»å‹é—®é¢˜ï¼‰ï¼Œå¯æ›¿æ¢ä¸ºï¼š
            # chouma_total_df[cols_to_clean] = chouma_total_df[cols_to_clean].round(2)

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  9.æ›´æ–°dataframe
            self.stock_chouma_available = chouma_total_df

            if platform.system() == "Windows":
                #  10.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                chouma_filename = base_utils.save_out_filename(filehead=f"stock_chouma", file_type='csv')
                chouma_data_filedir = os.path.join(self.dir_chouma_base, 'chouma_data', chouma_filename)
                chouma_total_df.to_csv(chouma_data_filedir, index=False)

                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=chouma_total_df,
                                                         table_name="ods_stock_chouma_insight",
                                                         merge_on=['ymd', 'htsc_code'])

                #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=chouma_total_df,
                                                         table_name="ods_stock_chouma_insight",
                                                         merge_on=['ymd', 'htsc_code'])
            else:
                #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=chouma_total_df,
                                                         table_name="ods_stock_chouma_insight",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_chouma_datas çš„è¿”å›å€¼ä¸ºç©ºå€¼')



    @timing_decorator
    def get_Ashare_industry_overview(self):
        """
        è·å–è¡Œä¸šä¿¡æ¯ ç”³ä¸‡ä¸‰çº§ çš„è¡Œä¸šä¿¡æ¯
        :return:
         industry_overview  ['ymd', 'classified', 'industry_name', 'industry_code', 'l1_code', 'l1_name', 'l2_code', 'l2_name', 'l3_code', 'l3_name']
        """

        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        time_today = DateUtility.today()
        # time_today = '20240930'

        time_today = datetime.strptime(time_today, '%Y%m%d')

        #  2.è¡Œä¸šä¿¡æ¯çš„æ€»å’Œdataframe
        industry_df = pd.DataFrame()

        #  3.è¯·æ±‚insight ä¸Šçš„ç”³ä¸‡ä¸‰çº§è¡Œä¸š æ•°æ®
        res = get_industries(classified='sw_l3')
        industry_df = pd.concat([industry_df, res], ignore_index=True)

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not industry_df.empty:
            #  4.æ—¥æœŸæ ¼å¼è½¬æ¢
            industry_df.insert(0, 'ymd', time_today)
            industry_df['ymd'] = pd.to_datetime(industry_df['ymd']).dt.strftime('%Y%m%d')

            #  5.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            industry_df = industry_df[
                ['ymd', 'classified', 'industry_name', 'industry_code', 'l1_code', 'l1_name', 'l2_code', 'l2_name',
                 'l3_code', 'l3_name']]

            #  6.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            industry_df = industry_df.drop_duplicates(subset=['ymd', 'industry_code'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  7.æ›´æ–°dataframe
            self.industry_overview = industry_df

            if platform.system() == "Windows":
                #  8.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                sw_industry_filename = base_utils.save_out_filename(filehead='sw_industry', file_type='csv')
                sw_industry_filedir = os.path.join(self.dir_industry_overview_base, sw_industry_filename)
                industry_df.to_csv(sw_industry_filedir, index=False)

                #  9.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=industry_df,
                                                         table_name="ods_astock_industry_overview",
                                                         merge_on=['ymd', 'industry_code'])

                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=industry_df,
                                                         table_name="ods_astock_industry_overview",
                                                         merge_on=['ymd', 'industry_code'])
            else:
                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=industry_df,
                                                         table_name="ods_astock_industry_overview",
                                                         merge_on=['ymd', 'industry_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_Ashare_industry_overview çš„è¿”å›å€¼ä¸ºç©ºå€¼')


    @timing_decorator
    def get_Ashare_industry_detail(self):
        """
        è·å–è‚¡ç¥¨çš„è¡Œä¸šä¿¡æ¯ ç”³ä¸‡ä¸‰çº§ çš„è¡Œä¸šä¿¡æ¯
        :return:
         industry_detail  ['ymd', 'htsc_code', 'name', 'industry_name', 'industry_code', 'l1_code', 'l1_name', 'l2_code', 'l2_name', 'l3_code', 'l3_name']
        """

        # å¦‚æœä»Šå¤©ä¸æ˜¯å‘¨äº”ï¼Œè·³è¿‡é€»è¾‘
        if not DateUtility.is_friday():
            logging.info("ä»Šå¤©ä¸æ˜¯å‘¨äº”ï¼Œè·³è¿‡è¡Œä¸šä¿¡æ¯è·å–é€»è¾‘ã€‚")
            return


        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        time_today = DateUtility.today()
        # time_today = '20240930'

        time_today = datetime.strptime(time_today, '%Y%m%d')

        #  2.è¡Œä¸šä¿¡æ¯çš„æ€»å’Œdataframe
        stock_in_industry_df = pd.DataFrame()

        #  3.è·å–æœ€æ–°çš„ stock_code æ•°æ®
        index_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  4.è¯·æ±‚insight ä¸Šçš„ç”³ä¸‡ä¸‰çº§è¡Œä¸š æ•°æ®
        i = 1                                     # æ€»ç¬¬ iä¸ª å¾ªç¯æ ‡è®°
        total_batches = len(index_list)           # æ€»æ‰¹æ¬¡æ•°é‡

        for stock_code in index_list:

            valid_num = stock_in_industry_df.shape[0]
            sys.stdout.write(f"\rå½“å‰æ‰§è¡Œ get_Ashare_industry_detail  ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_batches} ä¸ªæ‰¹æ¬¡, {valid_num}ä¸ªæœ‰æ•ˆè‚¡ç¥¨è¡Œä¸šæ•°æ®")
            sys.stdout.flush()
            time.sleep(0.03)

            res = get_industry(htsc_code=stock_code, classified='sw')
            stock_in_industry_df = pd.concat([stock_in_industry_df, res], ignore_index=True)

            i += 1

        sys.stdout.write("\n")

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not stock_in_industry_df.empty:
            #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
            stock_in_industry_df.insert(0, 'ymd', time_today)
            stock_in_industry_df['ymd'] = pd.to_datetime(stock_in_industry_df['ymd']).dt.strftime('%Y%m%d')

            #  6.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            stock_in_industry_df = stock_in_industry_df[
                ['ymd', 'htsc_code', 'name', 'industry_name', 'industry_code', 'l1_code', 'l1_name', 'l2_code',
                 'l2_name', 'l3_code', 'l3_name']]

            #  7.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            stock_in_industry_df = stock_in_industry_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  8.æ›´æ–°dataframe
            self.industry_detail = stock_in_industry_df

            if platform.system() == "Windows":
                #  9.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                sw_industry_filename = base_utils.save_out_filename(filehead='sw_industry', file_type='csv')
                sw_industry_filedir = os.path.join(self.dir_industry_detail_base, sw_industry_filename)
                stock_in_industry_df.to_csv(sw_industry_filedir, index=False)

                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=stock_in_industry_df,
                                                         table_name="ods_astock_industry_detail",
                                                         merge_on=['ymd', 'htsc_code'])

                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=stock_in_industry_df,
                                                         table_name="ods_astock_industry_detail",
                                                         merge_on=['ymd', 'htsc_code'])
            else:
                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=stock_in_industry_df,
                                                         table_name="ods_astock_industry_detail",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_Ashare_industry_detail çš„è¿”å›å€¼ä¸ºç©ºå€¼')



    @timing_decorator
    def get_shareholder_north_bound_num(self):
        """
        è·å– è‚¡ä¸œæ•° & åŒ—å‘èµ„é‡‘æƒ…å†µ
        Returns:
        """

        #  1.èµ·æ­¢æ—¶é—´ æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™ 2æœˆå‰çš„æœˆåˆ
        time_start_date = DateUtility.first_day_of_month(-2)
        #  ç»“æŸæ—¶é—´å¿…é¡»å¤§äºç­‰äºå½“æ—¥ï¼Œè¿™é‡Œå–æ˜å¤©çš„æ—¥æœŸ
        time_end_date = DateUtility.next_day(1)

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.è¡Œä¸šä¿¡æ¯çš„æ€»å’Œdataframe
        shareholder_num_df = pd.DataFrame()
        #  åŒ—å‘èµ„é‡‘çš„æ€»å’Œdataframe
        # north_bound_df = pd.DataFrame()

        #  3.è·å–æœ€æ–°çš„stock_codes æ•°æ®
        code_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  4.è¯·æ±‚insight  ä¸ªè‚¡è‚¡ä¸œæ•°   æ•°æ®
        #    è¯·æ±‚insight  åŒ—å‘èµ„é‡‘æŒä»“  æ•°æ®
        total_xunhuan = len(code_list)
        i = 1                       # æ€»å¾ªç¯æ ‡è®°

        for stock_code in code_list:
            # å±è”½ stdout å’Œ stderr
            with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
                res_shareholder = get_shareholder_num(htsc_code=stock_code, end_date=[time_start_date, time_end_date])
                # res_north_bound =get_north_bound(htsc_code=stock_code, trading_day=[time_start_date, time_end_date])

                valid_shareholder = shareholder_num_df.shape[0]
                # valid_north_bound = north_bound_df.shape[0]

            if res_shareholder is not None:
                shareholder_num_df = pd.concat([shareholder_num_df, res_shareholder], ignore_index=True)
                sys.stdout.write(f"\rå½“å‰æ‰§è¡Œ get_shareholder_num  ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_xunhuan} ä¸ªæ‰¹æ¬¡, {valid_shareholder}ä¸ªæœ‰æ•ˆè‚¡ä¸œæ•°æ®")
                sys.stdout.flush()

            time.sleep(0.03)

            i += 1

        sys.stdout.write("\n")

        ##  insight è¿”å›å€¼çš„éç©ºåˆ¤æ–­
        if not shareholder_num_df.empty:

            #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
            shareholder_num_df.rename(columns={'end_date': 'ymd'}, inplace=True)
            shareholder_num_df['ymd'] = pd.to_datetime(shareholder_num_df['ymd']).dt.strftime('%Y%m%d')

            # north_bound_df.rename(columns={'trading_day': 'ymd'}, inplace=True)
            # north_bound_df['ymd'] = pd.to_datetime(shareholder_num_df['ymd']).dt.strftime('%Y%m%d')

            #  6.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
            shareholder_num_df = shareholder_num_df[
                ['htsc_code', 'name', 'ymd', 'total_sh', 'avg_share', 'pct_of_total_sh', 'pct_of_avg_sh']]
            # north_bound_df = north_bound_df[['htsc_code', 'ymd', 'sh_hkshare_hold', 'pct_total_share']]

            #  7.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
            shareholder_num_df = shareholder_num_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')
            # north_bound_df = north_bound_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

            ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
            #  8.æ›´æ–°dataframe
            self.shareholder_num_df = shareholder_num_df
            # self.north_bound_df = north_bound_df

            if platform.system() == "Windows":
                #  9.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
                shareholder_num_filename = base_utils.save_out_filename(filehead='shareholder_num', file_type='csv')
                shareholder_num_filedir = os.path.join(self.dir_shareholder_num_base, shareholder_num_filename)
                shareholder_num_df.to_csv(shareholder_num_filedir, index=False)

                # north_bound_filename = base_utils.save_out_filename(filehead='north_bound', file_type='csv')
                # north_bound_filedir = os.path.join(self.dir_north_bound_base, north_bound_filename)
                # north_bound_df.to_csv(north_bound_filedir, index=False)

                #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                         password=local_password,
                                                         host=local_host,
                                                         database=local_database,
                                                         df=shareholder_num_df,
                                                         table_name="ods_shareholder_num_now",
                                                         merge_on=['ymd', 'htsc_code'])

                # mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                #                                          password=local_password,
                #                                          host=local_host,
                #                                          database=local_database,
                #                                          df=north_bound_df,
                #                                          table_name="north_bound_daily_now",
                #                                          merge_on=['ymd', 'htsc_code'])

                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=shareholder_num_df,
                                                         table_name="ods_shareholder_num_now",
                                                         merge_on=['ymd', 'htsc_code'])

                # mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                #                                          password=origin_password,
                #                                          host=origin_host,
                #                                          database=origin_database,
                #                                          df=north_bound_df,
                #                                          table_name="north_bound_daily_now",
                #                                          merge_on=['ymd', 'htsc_code'])
            else:
                #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
                mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                         password=origin_password,
                                                         host=origin_host,
                                                         database=origin_database,
                                                         df=shareholder_num_df,
                                                         table_name="ods_shareholder_num_now",
                                                         merge_on=['ymd', 'htsc_code'])
        else:
            ## insight è¿”å›ä¸ºç©ºå€¼
            logging.info('    get_shareholder_north_bound_num çš„è¿”å›å€¼ä¸ºç©ºå€¼')



    # @timing_decorator
    def setup(self):
        #  ç™»é™†insightæ•°æ®æº
        self.login()

        #  é™¤å» ST |  é€€  | B çš„è‚¡ç¥¨é›†åˆ
        self.get_stock_codes()

        #  è·å–ä¸Šè¿°è‚¡ç¥¨çš„å½“æœˆæ—¥K
        self.get_stock_kline()

        #  è·å–ä¸»è¦è‚¡æŒ‡
        self.get_index_a_share()

        #  å¤§ç›˜æ¶¨è·Œæ¦‚è§ˆ
        self.get_limit_summary()

        #  æœŸè´§__å†…ç›˜
        self.get_future_inside()

        # ç­¹ç æ¦‚è§ˆ
        self.get_chouma_datas()

        # è·å–Aè‚¡çš„è¡Œä¸šåˆ†ç±»æ•°æ®, æ˜¯è¡Œä¸šæ•°æ®
        self.get_Ashare_industry_overview()

        # è·å–Aè‚¡çš„è¡Œä¸šåˆ†ç±»æ•°æ®, æ˜¯stock_code & industry å…³è”åçš„å¤§è¡¨æ•°æ®
        self.get_Ashare_industry_detail()

        #  ä¸ªè‚¡è‚¡ä¸œæ•°
        self.get_shareholder_north_bound_num()



if __name__ == '__main__':
    save_insight_data = SaveInsightData()
    save_insight_data.setup()

```

--------------------------------------------------------------------------------
## datas_prepare\C01_data_download_daily\download_insight_data_afternoon_of_history.py

```python
# -*- coding: utf-8 -*-

import os
import sys
import contextlib
import io
from insight_python.com.insight import common
from insight_python.com.insight.query import *
from insight_python.com.insight.market_service import market_service
from datetime import datetime
import time
import platform


import CommonProperties.Base_Properties as base_properties
import CommonProperties.Base_utils as base_utils
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties.DateUtility import DateUtility
from CommonProperties.Base_utils import timing_decorator
from CommonProperties.set_config import setup_logging_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯ä¸‹åˆæ”¶ç›˜åä¸‹è½½ insight è¡Œæƒ…æºæ•°æ®, æœ¬åœ°ä¿å­˜,ç”¨äºåç»­åˆ†æ
# éœ€è¦ä¸‹è½½çš„æ•°æ®:
# 1.ä¸Šå¸‚è‚¡ç¥¨ä»£ç    get_all_stocks()
# 2.ç­¹ç åˆ†å¸ƒæ•°æ®   get_chouma_datas()

# è°ƒç”¨æ—¥å¿—é…ç½®
setup_logging_config()

# ************************************************************************

######################  mysql é…ç½®ä¿¡æ¯  æœ¬åœ°å’Œè¿œç«¯æœåŠ¡å™¨  ####################
local_user = base_properties.local_mysql_user
local_password = base_properties.local_mysql_password
local_database = base_properties.local_mysql_database
local_host = base_properties.local_mysql_host

origin_user = base_properties.origin_mysql_user
origin_password = base_properties.origin_mysql_password
origin_database = base_properties.origin_mysql_database
origin_host = base_properties.origin_mysql_host



class SaveInsightHistoryData:

    def __init__(self):

        self.init_dirs()

        self.init_variant()

    def init_dirs(self):
        """
        å…³é”®è·¯å¾„åˆå§‹åŒ–
        """
        #  æ–‡ä»¶è·¯å¾„_____insightæ–‡ä»¶åŸºç¡€è·¯å¾„
        self.dir_history_insight_base = base_properties.dir_history_insight_base

        #  æ–‡ä»¶è·¯å¾„_____ä¸Šå¸‚äº¤æ˜“è‚¡ç¥¨codes
        self.dir_history_stock_codes_base = os.path.join(self.dir_history_insight_base, 'stock_codes')

        #  æ–‡ä»¶è·¯å¾„_____ä¸Šå¸‚äº¤æ˜“è‚¡ç¥¨çš„æ—¥kçº¿æ•°æ®
        self.dir_history_stock_kline_base = os.path.join(self.dir_history_insight_base, 'stock_kline')

        #  æ–‡ä»¶è·¯å¾„_____å…³é”®å¤§ç›˜æŒ‡æ•°
        self.dir_history_index_a_share_base = os.path.join(self.dir_history_insight_base, 'index_a_share')

        #  æ–‡ä»¶è·¯å¾„_____æ¶¨è·Œåœæ•°é‡
        self.dir_history_limit_summary_base = os.path.join(self.dir_history_insight_base, 'limit_summary')

        #  æ–‡ä»¶è·¯å¾„_____å†…ç›˜æœŸè´§
        self.dir_history_future_inside_base = os.path.join(self.dir_history_insight_base, 'future_inside')

        #  æ–‡ä»¶è·¯å¾„_____ç­¹ç æ•°æ®
        self.dir_history_chouma_base = os.path.join(self.dir_history_insight_base, 'chouma')

        #  æ–‡ä»¶è·¯å¾„_____ä¸ªè‚¡çš„è‚¡ä¸œæ•°_æ˜ç»†
        self.dir_history_shareholder_num_base = os.path.join(self.dir_history_insight_base, 'shareholder_num')

        #  æ–‡ä»¶è·¯å¾„_____åŒ—å‘æŒä»“æ•°æ®_æ˜ç»†
        self.dir_history_north_bound_base = os.path.join(self.dir_history_insight_base, 'north_bound')


    def init_variant(self):
        """
        ç»“æœå˜é‡åˆå§‹åŒ–
        """
        #  é™¤å» ST|é€€|B çš„äº”è¦ç´    [ymd	htsc_code	name	exchange]
        self.stock_code_df = pd.DataFrame()

        #  è·å–ä¸Šè¿°è‚¡ç¥¨çš„å†å²æ•°æ®   æ—¥Kçº§åˆ«
        self.kline_total_history = pd.DataFrame()

        #  è·å¾—Aè‚¡å¸‚åœºçš„è‚¡æŒ‡ [htsc_code 	time	frequency	open	close	high	low	volume	value]
        self.index_a_share = pd.DataFrame()

        #  å¤§ç›˜æ¶¨è·Œåœæ•°é‡          [time	name	ä»Šæ—¥æ¶¨åœ	ä»Šæ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœ	æ˜¨æ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœè¡¨ç°]
        self.limit_summary_df = pd.DataFrame()

        #  æœŸè´§å¸‚åœºæ•°æ®    åŸæ²¹  è´µé‡‘å±  æœ‰è‰²
        self.future_index = pd.DataFrame()

        #  å¯ä»¥è·å–ç­¹ç çš„è‚¡ç¥¨æ•°æ®
        self.stock_chouma_available = ""


    @timing_decorator
    def login(self):
        # ç™»é™†å‰ åˆå§‹åŒ–ï¼Œæ²¡æœ‰å¯†ç å¯ä»¥è®¿é—®è¿›è¡Œè‡ªåŠ¨åŒ–æ³¨å†Œ
        # https://findata-insight.htsc.com:9151/terminalWeb/#/signup
        user = base_properties.user
        password = base_properties.password
        common.login(market_service, user, password)


    @timing_decorator
    def get_stock_codes(self):
        """
        è·å–å½“æ—¥çš„stockä»£ç åˆé›†   å‰”é™¤æ‰ST  é€€  B
        :return:
         stock_code_df  [ymd	htsc_code	name	exchange]
        """

        #  1.è·å–æ—¥æœŸ
        formatted_date = DateUtility.today()

        #  2.è¯·æ±‚insightæ•°æ®   get_all_stocks_info
        stock_all_df = get_all_stocks_info(listing_state="ä¸Šå¸‚äº¤æ˜“")

        #  3.æ—¥æœŸæ ¼å¼è½¬æ¢
        stock_all_df.insert(0, 'ymd', formatted_date)

        #  4.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        stock_all_df = stock_all_df[['ymd', 'htsc_code', 'name', 'exchange']]
        filtered_df = stock_all_df[~stock_all_df['name'].str.contains('ST|é€€|B')]

        #  5.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        filtered_df = filtered_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        #  6.å·²ä¸Šå¸‚çŠ¶æ€stock_codes
        self.stock_code_df = filtered_df


    @timing_decorator
    def get_stock_kline(self):
        """
        æ ¹æ®å½“æ—¥ä¸Šå¸‚çš„stock_codesï¼Œæ¥è·å¾—å…¨éƒ¨(å»é™¤ST|é€€|B)è‚¡ç¥¨çš„å†å²æ•°æ®
        :return:
         stock_kline_df  [ymd	htsc_code	name	exchange]
        """

        #  1.å†å²æ•°æ®çš„èµ·æ­¢æ—¶é—´
        time_start_date = DateUtility.first_day_of_year(-3)
        time_end_date = DateUtility.today()

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æ¯ä¸ªæ‰¹æ¬¡å– 40 ä¸ªå…ƒç´ 
        batch_size = 40

        #  3.è¿™æ˜¯ä¸€ä¸ªåˆ‡åˆ†æ‰¹æ¬¡çš„å†…éƒ¨å‡½æ•°
        def get_batches(lst, batch_size):
            for start in range(0, len(lst), batch_size):
                yield lst[start:start + batch_size]

        #  4.è·å–æœ€æ–° stock_code çš„list
        stock_code_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  5.è®¡ç®—æ€»æ‰¹æ¬¡æ•°
        total_batches = (len(stock_code_list) + batch_size - 1) // batch_size

        #  6.klineçš„æ€»å’Œdataframe
        kline_total_df = pd.DataFrame()

        #  7.è¯·æ±‚insightæ•°æ®
        for i, batch_list in enumerate(get_batches(stock_code_list, batch_size), start=1):
            #  ä¸€ç§éå¸¸å·§å¦™çš„å¾ªç¯æ‰“å°æ—¥å¿—çš„æ–¹å¼
            sys.stdout.write(f"\rå½“å‰æ‰§è¡Œget_stock_klineçš„ ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_batches} ä¸ªæ‰¹æ¬¡")
            sys.stdout.flush()
            time.sleep(0.01)

            res = get_kline(htsc_code=batch_list, time=[time_start_date, time_end_date], frequency="daily", fq="pre")
            kline_total_df = pd.concat([kline_total_df, res], ignore_index=True)

        #  8.å¾ªç¯ç»“æŸåæ‰“å°æ¢è¡Œç¬¦ï¼Œä»¥ç¡®ä¿åç»­è¾“å‡ºåœ¨æ–°è¡Œå¼€å§‹
        sys.stdout.write("\n")

        #  9.æ—¥æœŸæ ¼å¼è½¬æ¢
        kline_total_df['time'] = pd.to_datetime(kline_total_df['time']).dt.strftime('%Y%m%d')
        kline_total_df.rename(columns={'time': 'ymd'}, inplace=True)

        #  10.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤valueåˆ—
        kline_total_df = kline_total_df[['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'num_trades', 'volume']]

        #  11.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        # kline_total_df = kline_total_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        #  12.æ–‡ä»¶è¾“å‡ºæ¨¡å—
        self.kline_total_history = kline_total_df

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################

        if platform.system() == "Windows":
            #  13.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            kline_total_filename = base_utils.save_out_filename(filehead='stock_kline_history', file_type='csv')
            kline_total_filedir = os.path.join(self.dir_history_stock_kline_base, kline_total_filename)
            kline_total_df.to_csv(kline_total_filedir, index=False)

            #  14.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=kline_total_df,
                                                     table_name="ods_stock_kline_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])

            #  15.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=kline_total_df,
                                                     table_name="ods_stock_kline_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])
        else:
            #  15.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=kline_total_df,
                                                     table_name="ods_stock_kline_daily_insight",
                                                     merge_on=['ymd', 'htsc_code'])



    @timing_decorator
    def get_index_a_share(self):
        """
        000001.SH    ä¸Šè¯æŒ‡æ•°
        399006.SZ	 åˆ›ä¸šæ¿æŒ‡
        000016.SH    ä¸Šè¯50
        000300.SH    æ²ªæ·±300
        000849.SH    æ²ªæ·±300éé“¶è¡Œé‡‘èæŒ‡æ•°
        000905.SH	 ä¸­è¯500
        399852.SZ    ä¸­è¯1000
        000688.SH    ç§‘åˆ›50
        899050.BJ    åŒ—è¯50

        Returns:
             index_a_share   [htsc_code 	time	frequency	open	close	high	low	volume	value]
        """

        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        time_start_date = DateUtility.first_day_of_year(-3)
        time_end_date = DateUtility.today()

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æŸ¥è¯¢æ ‡çš„
        index_dict = {"000001.SH": "ä¸Šè¯æŒ‡æ•°"
            , "399002.SZ": "æ·±æˆæŒ‡"
            , "399006.SZ": "åˆ›ä¸šæ¿æŒ‡"
            , "000016.SH": "ä¸Šè¯50"
            , "000300.SH": "æ²ªæ·±300"
            , "000849.SH": "300éé“¶"
            , "000905.SH": "ä¸­è¯500"
            , "399852.SZ": "ä¸­è¯1000"
            , "000688.SH": "ç§‘åˆ›50"
            , "899050.BJ": "åŒ—è¯50"}
        index_list = list(index_dict.keys())

        #  3.index_a_share çš„æ€»å’Œdataframe
        index_df = pd.DataFrame()

        #  4.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_kline(htsc_code=index_list, time=[time_start_date, time_end_date],
                        frequency="daily", fq="pre")
        index_df = pd.concat([index_df, res], ignore_index=True)

        #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
        index_df['time'] = pd.to_datetime(index_df['time']).dt.strftime('%Y%m%d')
        index_df.rename(columns={'time': 'ymd'}, inplace=True)

        #  6.æ ¹æ®æ˜ å°„å…³ç³»ï¼Œæ·»åŠ stock_name
        index_df['name'] = index_df['htsc_code'].map(index_dict)

        #  7.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        index_df = index_df[['htsc_code', 'name', 'ymd', 'open', 'close', 'high', 'low', 'volume']]

        #  8.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        index_df = index_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        #  9.æ›´æ–°dataframe
        self.index_a_share = index_df

        if platform.system() == "Windows":
            #  10.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            index_filename = base_utils.save_out_filename(filehead='index_a_share_history', file_type='csv')
            index_filedir = os.path.join(self.dir_history_index_a_share_base, index_filename)
            index_df.to_csv(index_filedir, index=False)

            #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=index_df,
                                                     table_name="ods_index_a_share_insight",
                                                     merge_on=['ymd', 'htsc_code'])

            #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=index_df,
                                                     table_name="ods_index_a_share_insight",
                                                     merge_on=['ymd', 'htsc_code'])
        else:
            #  12.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=index_df,
                                                     table_name="ods_index_a_share_insight",
                                                     merge_on=['ymd', 'htsc_code'])


    @timing_decorator
    def get_limit_summary(self):
        """
        å¤§ç›˜æ¶¨è·Œåœåˆ†ææ•°æ®
        Args:
            market:
                1	sh_a_share	ä¸Šæµ·Aè‚¡
                2	sz_a_share	æ·±åœ³Aè‚¡
                3	a_share	Aè‚¡
                4	a_share	Bè‚¡
                5	gem	åˆ›ä¸š
                6	sme	ä¸­å°æ¿
                7	star	ç§‘åˆ›æ¿
            trading_day: List<datetime>	äº¤æ˜“æ—¥æœŸèŒƒå›´ï¼Œ[start_date, end_date]

        Returns: ups_downs_limit_count_up_limits
                 ups_downs_limit_count_down_limits
                 ups_downs_limit_count_pre_up_limits
                 ups_downs_limit_count_pre_down_limits
                 ups_downs_limit_count_pre_up_limits_average_change_percent

                 [time	name	ä»Šæ—¥æ¶¨åœ	ä»Šæ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœ	æ˜¨æ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœè¡¨ç°]

        """

        #  1.å½“æœˆæ•°æ®çš„èµ·æ­¢æ—¶é—´
        start_date = DateUtility.first_day_of_year(-3)
        end_date = DateUtility.today()

        start_date = datetime.strptime(start_date, '%Y%m%d')
        end_date = datetime.strptime(end_date, '%Y%m%d')

        #  2.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_change_summary(market=["a_share"], trading_day=[start_date, end_date])

        #  3.limit_summary çš„æ€»å’Œdataframe
        filter_limit_df = pd.DataFrame()
        filter_limit_df = pd.concat([filter_limit_df, res], ignore_index=True)

        #  4.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        filter_limit_df = filter_limit_df[['time',
                                     'name',
                                     'ups_downs_limit_count_up_limits',
                                     'ups_downs_limit_count_down_limits',
                                     'ups_downs_limit_count_pre_up_limits',
                                     'ups_downs_limit_count_pre_down_limits',
                                     'ups_downs_limit_count_pre_up_limits_average_change_percent']]
        filter_limit_df.columns = ['ymd', 'name', 'today_ZT', 'today_DT', 'yesterday_ZT', 'yesterday_DT',
                                   'yesterday_ZT_rate']

        #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
        filter_limit_df['ymd'] = pd.to_datetime(filter_limit_df['ymd']).dt.strftime('%Y%m%d')

        #  6.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        filter_limit_df = filter_limit_df.drop_duplicates(subset=['ymd', 'name'], keep='first')

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        #  7.æ›´æ–°dataframe
        self.limit_summary_df = filter_limit_df

        if platform.system() == "Windows":
            #  8.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            summary_filename = base_utils.save_out_filename(filehead='stock_limit_summary', file_type='csv')
            summary_dir = os.path.join(self.dir_history_limit_summary_base, summary_filename)
            filter_limit_df.to_csv(summary_dir, index=False)

            #  9.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=filter_limit_df,
                                                     table_name="ods_stock_limit_summary_insight",
                                                     merge_on=['ymd', 'name'])

            #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=filter_limit_df,
                                                     table_name="ods_stock_limit_summary_insight",
                                                     merge_on=['ymd', 'name'])
        else:
            #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=filter_limit_df,
                                                     table_name="ods_stock_limit_summary_insight",
                                                     merge_on=['ymd', 'name'])

    @timing_decorator
    def get_future_inside(self):
        """
        æœŸè´§å¸‚åœºæ•°æ®
        è´µé‡‘å±,  æœ‰è‰²æ•°æ®
        å›½é™…å¸‚åœº  å›½å†…å¸‚åœº
        AU9999.SHF    æ²ªé‡‘ä¸»è¿
        AU2409.SHF	  æ²ªé‡‘
        AG9999.SHF    æ²ªé“¶ä¸»è¿
        AG2409.SHF    æ²ªé“¶
        CU9999.SHF    æ²ªé“œä¸»è¿
        CU2409.SHF    æ²ªé“œ

        EC9999.INE    æ¬§çº¿é›†è¿ä¸»è¿
        EC2410.INE    æ¬§çº¿é›†è¿
        SC9999.INE    åŸæ²¹ä¸»è¿
        SC2410.INE    åŸæ²¹

        V9999.DCE     PVCä¸»è¿
        V2409.DCE     PVC
        MA9999.ZCE    ç”²é†‡ä¸»è¿      (æ‰¾ä¸åˆ°)
        MA2409.ZCE    ç”²é†‡         (æ‰¾ä¸åˆ°)
        ç›®å‰ä¸»è¿æ‰¾ä¸åˆ°æ•°æ®ï¼Œåªæœ‰æœˆä»½çš„ï¼Œæš‚æ—¶ç”¨ t+2 æœˆå»ä»£æ›¿ä¸»è¿å§

        Returns:
        """

        #  1.èµ·æ­¢æ—¶é—´ æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™2æœˆå‰çš„æœˆåˆç¬¬1å¤©
        #  æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™36æœˆå‰çš„æœˆåˆç¬¬1å¤©
        time_start_date = DateUtility.first_day_of_month(-36)
        time_end_date = DateUtility.today()

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.æŸ¥è¯¢æ ‡çš„
        index_list = ["AU{}.SHF", "AG{}.SHF", "CU{}.SHF", "EC{}.INE", "SC{}.INE", "V{}.DCE"]
        replacement = DateUtility.first_day_of_month(2)[2:6]

        future_index_list = [index.format(replacement) for index in index_list]

        #  3.future_inside çš„æ€»å’Œdataframe
        future_inside_df = pd.DataFrame()

        #  4.è¯·æ±‚insightæ•°æ®   get_kline
        res = get_kline(htsc_code=future_index_list, time=[time_start_date, time_end_date],
                        frequency="daily", fq="pre")
        future_inside_df = pd.concat([future_inside_df, res], ignore_index=True)

        #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
        future_inside_df['time'] = pd.to_datetime(future_inside_df['time']).dt.strftime('%Y%m%d')
        future_inside_df.rename(columns={'time': 'ymd'}, inplace=True)

        #  6.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        future_inside_df = future_inside_df[
            ['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'volume', 'open_interest', 'settle']]

        #  7.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        future_inside_df = future_inside_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        #  8.æ›´æ–°dataframe
        self.future_index = future_inside_df

        if platform.system() == "Windows":
            #  9.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            future_inside_filename = base_utils.save_out_filename(filehead='future_inside', file_type='csv')
            future_inside_filedir = os.path.join(self.dir_history_future_inside_base, future_inside_filename)
            future_inside_df.to_csv(future_inside_filedir, index=False)

            #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=future_inside_df,
                                                     table_name="ods_future_inside_insight",
                                                     merge_on=['ymd', 'htsc_code'])

            #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=future_inside_df,
                                                     table_name="ods_future_inside_insight",
                                                     merge_on=['ymd', 'htsc_code'])
        else:
            #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=future_inside_df,
                                                     table_name="ods_future_inside_insight",
                                                     merge_on=['ymd', 'htsc_code'])


    @timing_decorator
    def get_shareholder_north_bound_num(self):
        """
        è·å– è‚¡ä¸œæ•° & åŒ—å‘èµ„é‡‘æƒ…å†µ
        Returns:
        """

        #  1.èµ·æ­¢æ—¶é—´ æŸ¥è¯¢èµ·å§‹æ—¶é—´å†™ 36æœˆå‰çš„æœˆåˆ
        time_start_date = DateUtility.first_day_of_month(-36)
        #  ç»“æŸæ—¶é—´å¿…é¡»å¤§äºç­‰äºå½“æ—¥ï¼Œè¿™é‡Œå–æ˜å¤©çš„æ—¥æœŸ
        time_end_date = DateUtility.next_day(1)

        time_start_date = datetime.strptime(time_start_date, '%Y%m%d')
        time_end_date = datetime.strptime(time_end_date, '%Y%m%d')

        #  2.è¡Œä¸šä¿¡æ¯çš„æ€»å’Œdataframe
        shareholder_num_df = pd.DataFrame()
        #  åŒ—å‘èµ„é‡‘çš„æ€»å’Œdataframe
        north_bound_df = pd.DataFrame()

        #  3.è·å–æœ€æ–°çš„stock_codes æ•°æ®
        code_list = mysql_utils.get_stock_codes_latest(self.stock_code_df)

        #  4.è¯·æ±‚insight  ä¸ªè‚¡è‚¡ä¸œæ•°   æ•°æ®
        #    è¯·æ±‚insight  åŒ—å‘èµ„é‡‘æŒä»“  æ•°æ®
        total_xunhuan = len(code_list)
        i = 1                       # æ€»å¾ªç¯æ ‡è®°
        valid_shareholder = 1       # ä¸ªè‚¡è‚¡ä¸œæ•°æœ‰æ•ˆæ ‡è®°
        valid_north_bound = 1       # åŒ—å‘èµ„é‡‘æŒä»“æœ‰æ•ˆæ ‡è®°

        for stock_code in code_list:
            # å±è”½ stdout å’Œ stderr
            with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
                res_shareholder = get_shareholder_num(htsc_code=stock_code, end_date=[time_start_date, time_end_date])
                res_north_bound =get_north_bound(htsc_code=stock_code, trading_day=[time_start_date, time_end_date])

            if res_shareholder is not None:
                shareholder_num_df = pd.concat([shareholder_num_df, res_shareholder], ignore_index=True)
                sys.stdout.write(f"\rå½“å‰æ‰§è¡Œ get_shareholder_num  ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_xunhuan} ä¸ªæ‰¹æ¬¡, {valid_shareholder}ä¸ªæœ‰æ•ˆè‚¡ä¸œæ•°æ®")
                sys.stdout.flush()
                valid_shareholder += 1

            if res_north_bound is not None:
                north_bound_df = pd.concat([north_bound_df, res_north_bound], ignore_index=True)
                sys.stdout.write(f"\rå½“å‰æ‰§è¡Œ get_north_bound  ç¬¬ {i} æ¬¡å¾ªç¯ï¼Œæ€»å…± {total_xunhuan} ä¸ªæ‰¹æ¬¡, {valid_north_bound}ä¸ªæœ‰æ•ˆåŒ—å‘æŒä»“æ•°æ®")
                sys.stdout.flush()
                valid_north_bound += 1

            i += 1

        sys.stdout.write("\n")

        #  5.æ—¥æœŸæ ¼å¼è½¬æ¢
        shareholder_num_df.rename(columns={'end_date': 'ymd'}, inplace=True)
        shareholder_num_df['ymd'] = pd.to_datetime(shareholder_num_df['ymd']).dt.strftime('%Y%m%d')

        north_bound_df.rename(columns={'trading_day': 'ymd'}, inplace=True)
        north_bound_df['ymd'] = pd.to_datetime(shareholder_num_df['ymd']).dt.strftime('%Y%m%d')

        #  6.å£°æ˜æ‰€æœ‰çš„åˆ—åï¼Œå»é™¤å¤šä½™åˆ—
        shareholder_num_df = shareholder_num_df[['htsc_code', 'name', 'ymd', 'total_sh', 'avg_share', 'pct_of_total_sh', 'pct_of_avg_sh']]
        north_bound_df = north_bound_df[['htsc_code', 'ymd', 'sh_hkshare_hold', 'pct_total_share']]

        #  7.åˆ é™¤é‡å¤è®°å½•ï¼Œåªä¿ç•™æ¯ç»„ (ymd, stock_code) ä¸­çš„ç¬¬ä¸€ä¸ªè®°å½•
        shareholder_num_df = shareholder_num_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')
        north_bound_df = north_bound_df.drop_duplicates(subset=['ymd', 'htsc_code'], keep='first')

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        #  8.æ›´æ–°dataframe
        self.shareholder_num_df = shareholder_num_df
        self.north_bound_df = north_bound_df

        if platform.system() == "Windows":
            #  9.æœ¬åœ°csvæ–‡ä»¶çš„è½ç›˜ä¿å­˜
            shareholder_num_filename = base_utils.save_out_filename(filehead='shareholder_num', file_type='csv')
            shareholder_num_filedir = os.path.join(self.dir_history_north_bound_base, shareholder_num_filename)
            shareholder_num_df.to_csv(shareholder_num_filedir, index=False)

            north_bound_filename = base_utils.save_out_filename(filehead='north_bound', file_type='csv')
            north_bound_filedir = os.path.join(self.dir_history_north_bound_base, north_bound_filename)
            north_bound_df.to_csv(north_bound_filedir, index=False)

            #  10.ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=shareholder_num_df,
                                                     table_name="ods_shareholder_num",
                                                     merge_on=['ymd', 'htsc_code'])

            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=north_bound_df,
                                                     table_name="ods_north_bound_daily",
                                                     merge_on=['ymd', 'htsc_code'])

            #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=shareholder_num_df,
                                                     table_name="ods_shareholder_num",
                                                     merge_on=['ymd', 'htsc_code'])

            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=north_bound_df,
                                                     table_name="ods_north_bound_daily",
                                                     merge_on=['ymd', 'htsc_code'])
        else:
            #  11.ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=shareholder_num_df,
                                                     table_name="ods_shareholder_num",
                                                     merge_on=['ymd', 'htsc_code'])

            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=north_bound_df,
                                                     table_name="ods_north_bound_daily",
                                                     merge_on=['ymd', 'htsc_code'])



    @timing_decorator
    def setup(self):
        #  ç™»é™†insightæ•°æ®æº
        self.login()

        #  é™¤å» ST |  é€€  | B çš„è‚¡ç¥¨é›†åˆ
        self.get_stock_codes()

        #  è·å–å½“å‰å·²ä¸Šå¸‚è‚¡ç¥¨è¿‡å»3å¹´åˆ°ä»Šå¤©çš„å†å²kline
        self.get_stock_kline()

        #  è·å–ä¸»è¦è‚¡æŒ‡
        self.get_index_a_share()

        #  å¤§ç›˜æ¶¨è·Œæ¦‚è§ˆ
        self.get_limit_summary()

        #  æœŸè´§__å†…ç›˜
        self.get_future_inside()

        #  ä¸ªè‚¡è‚¡ä¸œæ•°
        self.get_shareholder_north_bound_num()


if __name__ == '__main__':
    save_insight_data = SaveInsightHistoryData()
    save_insight_data.setup()

```

--------------------------------------------------------------------------------
## datas_prepare\C01_data_download_daily\download_vantage_data_afternoon.py

```python
# -*- coding: utf-8 -*-

import pandas as pd
import requests
import platform
# from yahoo_fin.stock_info import *
from io import StringIO
import os
import logging


from CommonProperties.DateUtility import DateUtility
import CommonProperties.Base_Properties as base_properties
import CommonProperties.Base_utils as base_utils
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties.Base_utils import timing_decorator
from CommonProperties.set_config import setup_logging_config


# é…ç½®æ—¥å¿—å¤„ç†å™¨
# è°ƒç”¨æ—¥å¿—é…ç½®
setup_logging_config()

#  vantage  æµ‹è¯•ç¯å¢ƒæ–‡ä»¶ä¿å­˜ç›®å½•
vantage_test_dir = os.path.join(base_properties.dir_vantage_base, 'test')


api_key = 'ICTN 9 P9 ES 00 EADUF'
# api_key = 'BI8JFEOOP3C563PO'
key_US_stock = ['TSLA', 'AAPL', 'NVDA', 'MSFT', 'META']

# æ„å»º API è¯·æ±‚ URL
base_url = 'https://www.alphavantage.co/query'
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'
}



######################  mysql é…ç½®ä¿¡æ¯  æœ¬åœ°å’Œè¿œç«¯æœåŠ¡å™¨  ####################
local_user = base_properties.local_mysql_user
local_password = base_properties.local_mysql_password
local_database = base_properties.local_mysql_database
local_host = base_properties.local_mysql_host

origin_user = base_properties.origin_mysql_user
origin_password = base_properties.origin_mysql_password
origin_database = base_properties.origin_mysql_database
origin_host = base_properties.origin_mysql_host



class SaveVantageData:
    def __init__(self):
        self.init_dirs()
        self.init_variant()

    def init_dirs(self):
        """
        å…³é”®è·¯å¾„åˆå§‹åŒ–
        """
        #  æ–‡ä»¶è·¯å¾„_____vantage æ–‡ä»¶åŸºç¡€è·¯å¾„
        self.dir_vantage_base = base_properties.dir_vantage_base

        #  æ–‡ä»¶è·¯å¾„_____US çš„ stock
        self.dir_US_stock_base = os.path.join(self.dir_vantage_base, 'US_stock')

        #  æ–‡ä»¶è·¯å¾„_____USD çš„ æ±‡ç‡æ˜ç»†
        self.dir_USD_FX_detail_base = os.path.join(self.dir_vantage_base, 'USD_FX_detail')

        #  æ–‡ä»¶è·¯å¾„_____USD çš„ ç¾å…ƒæŒ‡æ•°
        self.dir_USD_FX_base = os.path.join(self.dir_vantage_base, 'USD_FX')


    def init_variant(self):
        """
        ç»“æœå˜é‡åˆå§‹åŒ–
        """
        #  å…³é”®çš„stock_code
        self.key_US_stock = ['TSLA', 'AAPL', 'NVDA', 'MSFT', 'META']

        #  è·å¾—USæ ¸å¿ƒstock  [name, timestamp  open  high  low   close   volume]
        self.vantage_US_stock = pd.DataFrame()


    @timing_decorator
    def get_US_stock_from_vantage(self):
        """
        å…³é”® US stcok
        Returns:
            [name, timestamp  open  high  low   close   volume]
        """

        function = 'TIME_SERIES_DAILY'
        res_df = pd.DataFrame()

        for symbol in self.key_US_stock:
            url = f'{base_url}?function={function}&symbol={symbol}&apikey={api_key}&outputsize=full&datatype=csv'
            # å‘é€ GET è¯·æ±‚
            response = requests.get(url, headers=headers, timeout=10)

            # å¤„ç†å“åº”æ•°æ®
            if response.status_code == 200:
                # è¿”å›csvå­—ç¬¦ä¸²
                csv_string = response.text
                csv_file = StringIO(csv_string)
                vantage_df = pd.read_csv(csv_file)
                vantage_df.insert(0, 'name', symbol)

                res_df = pd.concat([res_df, vantage_df], ignore_index=True)
            else:
                print(f'Error fetching {symbol} data: {response.status_code} - {response.text}')

        #  8.æ—¥æœŸæ ¼å¼è½¬æ¢
        res_df['timestamp'] = pd.to_datetime(res_df['timestamp']).dt.strftime('%Y%m%d')
        res_df.rename(columns={'timestamp': 'ymd'}, inplace=True)

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            US_stock_filename = base_utils.save_out_filename(filehead='US_stock', file_type='csv')
            US_stock_filedir = os.path.join(self.dir_US_stock_base, US_stock_filename)
            res_df.to_csv(US_stock_filedir, index=False)

            #  ç»“æœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=res_df,
                                                     table_name="ods_us_stock_daily_vantage",
                                                     merge_on=['ymd', 'name'])

            #  ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=res_df,
                                                     table_name="ods_us_stock_daily_vantage",
                                                     merge_on=['ymd', 'name'])
        else:
            #  ç»“æœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=res_df,
                                                     table_name="ods_us_stock_daily_vantage",
                                                     merge_on=['ymd', 'name'])




    def get_USD_FX_core(self, url, flag):
        """
        Args:
            url: è¯·æ±‚çš„URLåœ°å€
            flag: æ•°æ®æ ‡è¯†ç¬¦
        Returns:
            è¿”å›åŒ…å«æ±‡ç‡æ•°æ®çš„DataFrame
        """
        # å­˜æ”¾æ±‡ç‡ç»“æœæ•°æ®
        res_df = pd.DataFrame()

        # ä½¿ç”¨å¸¦é‡è¯•åŠŸèƒ½çš„è¯·æ±‚
        response = base_utils.get_with_retries(url, headers=headers, timeout=10)

        # å¤„ç†å“åº”æ•°æ®
        if response is not None and response.status_code == 200:
            # è¿”å›csvå­—ç¬¦ä¸²
            csv_string = response.text
            csv_file = StringIO(csv_string)
            vantage_df = pd.read_csv(csv_file)
            vantage_df.insert(0, 'name', flag)

            res_df = pd.concat([res_df, vantage_df], ignore_index=True)
        else:
            logging.error(f'Error fetching {flag} data: è¯·æ±‚å¤±è´¥æˆ–æ— æ•ˆçš„å“åº”')

        logging.info(f"get_USD_FX_core å®Œæˆ {flag} æ±‡ç‡æŸ¥è¯¢")

        return res_df


    @timing_decorator
    def get_USD_FX_from_vantage(self):
        """
        è®¡ç®—ç¾å…ƒæŒ‡æ•°, ä»ä¸»æµè´§å¸å»è®¡ç®—ç¾å…ƒæŒ‡æ•°
        Returns:
            [name, timestamp  open  high  low   close   volume]
        """
        function = 'FX_DAILY'

        #  å­˜æ”¾æ±‡ç‡æ•°æ®
        res_df = pd.DataFrame()

        # å®šä¹‰æƒé‡
        weights = {
            'EUR_USD': -0.576,
            'USD_JPY': 0.136,
            'GBP_USD': -0.119,
            'USD_CAD': 0.091,
            'USD_SEK': 0.042,
            'USD_CHF': 0.036
        }

        # å®šä¹‰åˆå§‹å¸¸æ•°
        constant = 50.14348112

        #  --------------------------  å¼€å§‹è®¡ç®—ç¾å…ƒæŒ‡æ•°  ------------------------------
        #  æ¬§å…ƒå…‘ç¾å…ƒ
        url_EUR_USD = f'{base_url}?function={function}&from_symbol=EUR&to_symbol=USD&apikey={api_key}&datatype=csv'
        df_EUR_USD = self.get_USD_FX_core(url=url_EUR_USD, flag='EUR_USD')

        #  ç¾å…ƒå…‘æ—¥å…ƒ
        url_USD_JPY = f'{base_url}?function={function}&from_symbol=USD&to_symbol=JPY&apikey={api_key}&datatype=csv'
        df_USD_JPY = self.get_USD_FX_core(url=url_USD_JPY, flag='USD_JPY')

        #  è‹±é•‘å…‘ç¾å…ƒ
        url_GBP_USD = f'{base_url}?function={function}&from_symbol=GBP&to_symbol=USD&apikey={api_key}&datatype=csv'
        df_GBP_USD = self.get_USD_FX_core(url=url_GBP_USD, flag='GBP_USD')

        #  ç¾å…ƒå…‘åŠ æ‹¿å¤§å…ƒ
        url_USD_CAD = f'{base_url}?function={function}&from_symbol=USD&to_symbol=CAD&apikey={api_key}&datatype=csv'
        df_USD_CAD = self.get_USD_FX_core(url=url_USD_CAD, flag='USD_CAD')

        #  ç¾å…ƒå…‘ç‘å…¸å…‹æœ—
        url_USD_SEK = f'{base_url}?function={function}&from_symbol=USD&to_symbol=SEK&apikey={api_key}&datatype=csv'
        df_USD_SEK = self.get_USD_FX_core(url=url_USD_SEK, flag='USD_SEK')

        #  ç¾å…ƒå…‘ç‘å£«æ³•éƒ
        url_USD_CHF = f'{base_url}?function={function}&from_symbol=USD&to_symbol=CHF&apikey={api_key}&datatype=csv'
        df_USD_CHF = self.get_USD_FX_core(url=url_USD_CHF, flag='USD_CHF')

        #  æ±‡æ€»å¾—åˆ°ç¾å…ƒæŒ‡æ•°çš„ä¸»è¦æˆåˆ†
        res_df = pd.concat([res_df, df_EUR_USD, df_USD_JPY, df_GBP_USD, df_USD_CAD, df_USD_SEK, df_USD_CHF], ignore_index=True)

        #  æ—¥æœŸæ ¼å¼è½¬æ¢
        res_df['timestamp'] = pd.to_datetime(res_df['timestamp']).dt.strftime('%Y%m%d')
        res_df.rename(columns={'timestamp': 'ymd'}, inplace=True)

        if platform.system() == "Windows":
            ##  æ–‡ä»¶è¾“å‡ºæ¨¡å—     è¾“å‡ºæ±‡ç‡æ˜ç»†
            USD_FX_detail_filename = base_utils.save_out_filename(filehead='USD_FX_detail', file_type='csv')
            USD_FX_detail_filedir = os.path.join(self.dir_USD_FX_detail_base, USD_FX_detail_filename)
            res_df.to_csv(USD_FX_detail_filedir, index=False)

            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ æœ¬åœ° mysql
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=res_df,
                                                     table_name="ods_exchange_rate_vantage_detail",
                                                     merge_on=["ymd", "name"])

            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ è¿œç«¯ mysql
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=res_df,
                                                     table_name="ods_exchange_rate_vantage_detail",
                                                     merge_on=["ymd", "name"])
        else:
            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ è¿œç«¯ mysql
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=res_df,
                                                     table_name="ods_exchange_rate_vantage_detail",
                                                     merge_on=["ymd", "name"])


        #  --------------------------  å¼€å§‹è®¡ç®—ç¾å…ƒæŒ‡æ•°  ------------------------------
        # è·å–å”¯ä¸€çš„æ—¶é—´æˆ³
        timestamps = res_df['ymd'].unique()

        # åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨æ¥å­˜å‚¨ç»“æœ
        results = []

        for timestamp in timestamps:
            # è·å–å½“å‰æ—¶é—´æˆ³çš„æ‰€æœ‰æ±‡ç‡æ•°æ®
            current_data = res_df[res_df['ymd'] == timestamp]
            if current_data.shape[0] != 6:
                break

            # åˆå§‹åŒ–DXYå€¼
            dxy = constant
            # è®¡ç®—DXY
            for name, weight in weights.items():
                rate = current_data[current_data['name'] == name]['close'].values[0]
                dxy *= rate ** weight
            # å°†ç»“æœæ·»åŠ åˆ°åˆ—è¡¨ä¸­
            results.append([timestamp, dxy])

        # å°†ç»“æœè½¬æ¢ä¸ºDataFrame
        dxy_df = pd.DataFrame(results, columns=['ymd', 'DXY'])

        if platform.system() == "Windows":
            ##  æ–‡ä»¶è¾“å‡ºæ¨¡å—     è¾“å‡ºç¾å…ƒæŒ‡æ•°
            USD_FX_filename = base_utils.save_out_filename(filehead='USD_FX', file_type='csv')
            USD_FX_filedir = os.path.join(self.dir_USD_FX_base, USD_FX_filename)
            dxy_df.to_csv(USD_FX_filedir, index=False)

            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ æœ¬åœ° mysql
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=dxy_df,
                                                     table_name="ods_exchange_dxy_vantage",
                                                     merge_on=["ymd", "name"])

            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ è¿œç«¯ mysql
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=dxy_df,
                                                     table_name="ods_exchange_dxy_vantage",
                                                     merge_on=["ymd", "name"])
        else:
            #  å°†æ±‡ç‡æ˜ç»†å†™å…¥ è¿œç«¯ mysql
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=dxy_df,
                                                     table_name="ods_exchange_dxy_vantage",
                                                     merge_on=["ymd", "name"])


    @timing_decorator
    def setup(self):

        #  è·å– US ä¸»è¦stock çš„å…¨éƒ¨æ•°æ®
        # self.get_US_stock_from_vantage()
        self.get_USD_FX_from_vantage()


if __name__ == '__main__':
    save_vantage_data = SaveVantageData()
    save_vantage_data.setup()



```

--------------------------------------------------------------------------------
## datas_prepare\C02_data_merge\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C02_data_merge\merge_insight_data_afternoon.py

```python
# -*- coding: utf-8 -*-

import os
from datetime import datetime

import pandas as pd
from sqlalchemy import create_engine, text
import time
import platform

# import dataprepare_properties
# import dataprepare_utils
from CommonProperties import Base_Properties
import CommonProperties.Base_utils as base_utils
from CommonProperties.DateUtility import DateUtility
from CommonProperties.Base_utils import timing_decorator
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties import set_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯ä¸‹åˆæ”¶ç›˜åé’ˆå¯¹ insight è¡Œæƒ…æºæ•°æ®çš„æœ¬åœ°ä¿å­˜éƒ¨åˆ†å¼€å±•merge
# éœ€è¦ä¸‹è½½çš„æ•°æ®:
# 1.ä¸Šå¸‚è‚¡ç¥¨ä»£ç    get_all_stocks()
# 2.ç­¹ç åˆ†å¸ƒæ•°æ®   get_chouma_datas()


# ************************************************************************
#  è°ƒç”¨æ—¥å¿—é…ç½®
set_config.setup_logging_config()

######################  mysql é…ç½®ä¿¡æ¯  æœ¬åœ°å’Œè¿œç«¯æœåŠ¡å™¨  ####################
local_user = Base_Properties.local_mysql_user
local_password = Base_Properties.local_mysql_password
local_database = Base_Properties.local_mysql_database
local_host = Base_Properties.local_mysql_host

origin_user = Base_Properties.origin_mysql_user
origin_password = Base_Properties.origin_mysql_password
origin_database = Base_Properties.origin_mysql_database
origin_host = Base_Properties.origin_mysql_host





class MergeInsightData:

    def __init__(self):
        pass


    @timing_decorator
    def merge_stock_kline(self):
        """
        å°† stock_kline çš„å†å²æ•°æ®å’Œå½“æœˆæ•°æ®åšmerge
        :return:
         stock_kline_df  [ymd	htsc_code	name	exchange]
        """
        source_table = 'ods_stock_kline_daily_insight_now'
        target_table = 'ods_stock_kline_daily_insight'
        columns = ['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'num_trades', 'volume']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    @timing_decorator
    def merge_index_a_share(self):
        """
        000001.SH    ä¸Šè¯æŒ‡æ•°
        399006.SZ	 åˆ›ä¸šæ¿æŒ‡
        000016.SH    ä¸Šè¯50
        000300.SH    æ²ªæ·±300
        000849.SH    æ²ªæ·±300éé“¶è¡Œé‡‘èæŒ‡æ•°
        000905.SH	 ä¸­è¯500
        399852.SZ    ä¸­è¯1000
        000688.SH    ç§‘åˆ›50

        Returns:
             index_a_share   [htsc_code 	time	frequency	open	close	high	low	volume	value]
        """
        source_table = 'ods_index_a_share_insight_now'
        target_table = 'ods_index_a_share_insight'
        columns = ['htsc_code', 'name', 'ymd', 'open', 'close', 'high', 'low', 'volume']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    @timing_decorator
    def merge_limit_summary(self):
        """
        å¤§ç›˜æ¶¨è·Œåœåˆ†ææ•°æ®
        Args:
            market:
                1	sh_a_share	ä¸Šæµ·Aè‚¡
                2	sz_a_share	æ·±åœ³Aè‚¡
                3	a_share	Aè‚¡
                4	a_share	Bè‚¡
                5	gem	åˆ›ä¸š
                6	sme	ä¸­å°æ¿
                7	star	ç§‘åˆ›æ¿
            trading_day: List<datetime>	äº¤æ˜“æ—¥æœŸèŒƒå›´ï¼Œ[start_date, end_date]

        Returns: ups_downs_limit_count_up_limits
                 ups_downs_limit_count_down_limits
                 ups_downs_limit_count_pre_up_limits
                 ups_downs_limit_count_pre_down_limits
                 ups_downs_limit_count_pre_up_limits_average_change_percent

                 [time	name	ä»Šæ—¥æ¶¨åœ	ä»Šæ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœ	æ˜¨æ—¥è·Œåœ	æ˜¨æ—¥æ¶¨åœè¡¨ç°]
        """
        source_table = 'ods_stock_limit_summary_insight_now'
        target_table = 'ods_stock_limit_summary_insight'
        columns = ['ymd', 'name', 'today_ZT', 'today_DT', 'yesterday_ZT', 'yesterday_DT', 'yesterday_ZT_rate']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    @timing_decorator
    def merge_future_inside(self):
        """
        æœŸè´§å¸‚åœºæ•°æ®
        è´µé‡‘å±,  æœ‰è‰²æ•°æ®
        å›½é™…å¸‚åœº  å›½å†…å¸‚åœº
        AU9999.SHF    æ²ªé‡‘ä¸»è¿
        AU2409.SHF	  æ²ªé‡‘
        AG9999.SHF    æ²ªé“¶ä¸»è¿
        AG2409.SHF    æ²ªé“¶
        CU9999.SHF    æ²ªé“œä¸»è¿
        CU2409.SHF    æ²ªé“œ

        EC9999.INE    æ¬§çº¿é›†è¿ä¸»è¿
        EC2410.INE    æ¬§çº¿é›†è¿
        SC9999.INE    åŸæ²¹ä¸»è¿
        SC2410.INE    åŸæ²¹

        V9999.DCE     PVCä¸»è¿
        V2409.DCE     PVC
        MA9999.ZCE    ç”²é†‡ä¸»è¿      (æ‰¾ä¸åˆ°)
        MA2409.ZCE    ç”²é†‡         (æ‰¾ä¸åˆ°)
        ç›®å‰ä¸»è¿æ‰¾ä¸åˆ°æ•°æ®ï¼Œåªæœ‰æœˆä»½çš„ï¼Œæš‚æ—¶ç”¨ t+2 æœˆå»ä»£æ›¿ä¸»è¿å§

        Returns:
        """
        source_table = 'ods_future_inside_insight_now'
        target_table = 'ods_future_inside_insight'
        columns = ['htsc_code', 'ymd', 'open', 'close', 'high', 'low', 'volume', 'open_interest', 'settle']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    @timing_decorator
    def merge_shareholder_num(self):
        """
        Aè‚¡å¸‚åœºçš„è‚¡ä¸œæ•°
        Returns:
        """
        source_table = 'ods_shareholder_num_now'
        target_table = 'ods_shareholder_num'
        columns = ['htsc_code', 'name', 'ymd', 'total_sh', 'avg_share', 'pct_of_total_sh', 'pct_of_avg_sh']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    @timing_decorator
    def merge_north_bound(self):
        """
        Aè‚¡å¸‚åœºçš„åŒ—å‘èµ„é‡‘æ•°æ®
        Returns:
        """
        source_table = 'ods_north_bound_daily_now'
        target_table = 'ods_north_bound_daily'
        columns = ['htsc_code', 'ymd', 'sh_hkshare_hold', 'pct_total_share']
        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            # å¯¹æœ¬åœ° Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=local_user,
                                     password=local_password,
                                     host=local_host,
                                     database=local_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)

            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)
        else:
            # å¯¹è¿œç«¯ Mysql åšæ•°æ®èšåˆ
            mysql_utils.upsert_table(user=origin_user,
                                     password=origin_password,
                                     host=origin_host,
                                     database=origin_database,
                                     source_table=source_table,
                                     target_table=target_table,
                                     columns=columns)


    def setup(self):

        #  è·å–å½“å‰å·²ä¸Šå¸‚è‚¡ç¥¨è¿‡å»3å¹´åˆ°ä»Šå¤©çš„å†å²kline
        self.merge_stock_kline()

        #  è·å–ä¸»è¦è‚¡æŒ‡
        self.merge_index_a_share()

        #  å¤§ç›˜æ¶¨è·Œæ¦‚è§ˆ
        self.merge_limit_summary()

        #  æœŸè´§__å†…ç›˜
        self.merge_future_inside()

        #  è‚¡ä¸œæ•°
        self.merge_shareholder_num()

        #  åŒ—å‘
        #self.merge_north_bound()



if __name__ == '__main__':
    save_insight_data = MergeInsightData()
    save_insight_data.setup()

```

--------------------------------------------------------------------------------
## datas_prepare\C03_data_DWD\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C03_data_DWD\calculate_DWD_datas.py

```python
# -*- coding: utf-8 -*-

import os
from datetime import datetime

import pandas as pd
from sqlalchemy import create_engine, text
import time
import platform
import logging


# import dataprepare_properties
# import dataprepare_utils
from CommonProperties import Base_Properties
import CommonProperties.Base_utils as base_utils
from CommonProperties.DateUtility import DateUtility
from CommonProperties.Base_utils import timing_decorator

import CommonProperties.Mysql_Utils as mysql_utils

from CommonProperties import set_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯ä¸‹åˆæ”¶ç›˜åé’ˆå¯¹ insight è¡Œæƒ…æºæ•°æ®çš„æœ¬åœ°ä¿å­˜éƒ¨åˆ†å¼€å±•merge
# éœ€è¦ä¸‹è½½çš„æ•°æ®:
# 1.ä¸Šå¸‚è‚¡ç¥¨ä»£ç 
# 2.ç­¹ç åˆ†å¸ƒæ•°æ®   get_chouma_datas()


# ************************************************************************
#  è°ƒç”¨æ—¥å¿—é…ç½®
set_config.setup_logging_config()

######################  mysql é…ç½®ä¿¡æ¯  æœ¬åœ°å’Œè¿œç«¯æœåŠ¡å™¨  ####################
local_user = Base_Properties.local_mysql_user
local_password = Base_Properties.local_mysql_password
local_database = Base_Properties.local_mysql_database
local_host = Base_Properties.local_mysql_host

origin_user = Base_Properties.origin_mysql_user
origin_password = Base_Properties.origin_mysql_password
origin_database = Base_Properties.origin_mysql_database
origin_host = Base_Properties.origin_mysql_host


class CalDWD:

    def __init__(self):



        pass

    @timing_decorator
    def cal_ashare_plate(self):
        """
        èšåˆè‚¡ç¥¨çš„æ¿å—ï¼ŒæŠŠå„ä¸ªæ¿å—æ•°æ®èšåˆåœ¨ä¸€èµ·
        Returns:
        """

        #  1.è·å–æ—¥æœŸ
        ymd = DateUtility.today()
        # ymd = "20241004"

        # 2.å®šä¹‰ SQL æ¨¡æ¿
        sql_statements_template = [
            """
            DELETE FROM quant.dwd_stock_a_total_plate WHERE ymd='{ymd}';
            """,
            """
            INSERT INTO quant.dwd_stock_a_total_plate
            SELECT 
                ymd, 
                concept_name AS plate_name,
                stock_code,
                stock_name,
                'ods_tdx_stock_concept_plate' AS source_table,
                '' AS remark
            FROM quant.ods_tdx_stock_concept_plate
            WHERE ymd='{ymd}'
            UNION ALL
            SELECT 
                ymd,
                style_name AS plate_name,
                stock_code,
                stock_name,
                'ods_tdx_stock_style_plate' AS source_table,
                '' AS remark
            FROM quant.ods_tdx_stock_style_plate
            WHERE ymd='{ymd}'
            UNION ALL
            SELECT 
                ymd,
                industry_name AS plate_name,
                stock_code,
                stock_name,
                'ods_tdx_stock_industry_plate' AS source_table,
                '' AS remark
            FROM quant.ods_tdx_stock_industry_plate
            WHERE ymd='{ymd}'
            UNION ALL
            SELECT 
                ymd,
                region_name AS plate_name,
                stock_code,
                stock_name,
                'ods_tdx_stock_region_plate' AS source_table,
                '' AS remark
            FROM quant.ods_tdx_stock_region_plate
            WHERE ymd='{ymd}'
            UNION ALL
            SELECT 
                ymd,
                index_name AS plate_name,
                stock_code,
                stock_name,
                'ods_tdx_stock_index_plate' AS source_table,
                '' AS remark
            FROM quant.ods_tdx_stock_index_plate
            WHERE ymd='{ymd}'
            UNION ALL
            SELECT 
                ymd,
                plate_name,
                stock_code,
                stock_name,
                'ods_stock_plate_redbook' AS source_table,
                remark
            FROM quant.ods_stock_plate_redbook
            WHERE ymd='{ymd}';
            """
        ]

        # 3.ä¸»ç¨‹åºæ›¿æ¢ {ymd} å ä½ç¬¦
        sql_statements = [stmt.format(ymd=ymd) for stmt in sql_statements_template]

        # 4.æ‰§è¡Œ SQL
        if platform.system() == "Windows":

            mysql_utils.execute_sql_statements(
                user=local_user,
                password=local_password,
                host=local_host,
                database=local_database,
                sql_statements=sql_statements)

            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)
        else:
            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)


    @timing_decorator
    def cal_stock_exchange(self):
        """
        è®¡ç®—è‚¡ç¥¨æ‰€å½’å±çš„äº¤æ˜“æ‰€ï¼Œåˆ¤æ–­å…¶æ˜¯ä¸»åŠã€åˆ›ä¸šæ¿ã€ç§‘åˆ›æ¿ã€åŒ—äº¤æ‰€ç­‰ç­‰
        Returns:
        """

        #  1.è·å–æ—¥æœŸ
        ymd = DateUtility.today()
        # ymd = "20241122"

        # 2.å®šä¹‰ SQL æ¨¡æ¿
        sql_statements_template = [
            """
            DELETE  FROM quant.ods_stock_exchange_market WHERE  ymd = '{ymd}';
            """,
            """
            INSERT INTO quant.ods_stock_exchange_market (ymd, stock_code, stock_name, market)
            SELECT 
                t1.ymd
               ,t1.htsc_code AS stock_code
               ,t1.name      AS stock_name
               ,CASE
               WHEN t1.htsc_code LIKE '300%' OR t1.htsc_code LIKE '301%' THEN 'åˆ›ä¸šæ¿' 
               WHEN t1.htsc_code LIKE '8%'   OR t1.htsc_code LIKE '4%'   THEN 'åŒ—äº¤æ‰€'  
               WHEN t1.htsc_code LIKE '000%' OR t1.htsc_code LIKE '001%' OR t1.htsc_code LIKE '002%' OR t1.htsc_code LIKE '003%' THEN 'æ·±åœ³ä¸»æ¿' 
               WHEN t1.htsc_code LIKE '688%' OR t1.htsc_code LIKE '689%' THEN 'ç§‘åˆ›æ¿'  
               WHEN t1.htsc_code LIKE '600%' OR t1.htsc_code LIKE '601%' OR t1.htsc_code LIKE '603%' OR t1.htsc_code LIKE '605%' THEN 'ä¸Šæµ·ä¸»æ¿' 
               ELSE 'æœªçŸ¥ç±»å‹' 
               END AS market
            FROM quant.ods_stock_code_daily_insight     t1
            WHERE  t1.ymd = '{ymd}';
            """
        ]

        # 3.ä¸»ç¨‹åºæ›¿æ¢ {ymd} å ä½ç¬¦
        sql_statements = [stmt.format(ymd=ymd) for stmt in sql_statements_template]

        # 4.æ‰§è¡Œ SQL
        if platform.system() == "Windows":

            mysql_utils.execute_sql_statements(
                user=local_user,
                password=local_password,
                host=local_host,
                database=local_database,
                sql_statements=sql_statements)

            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)
        else:
            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)


    @timing_decorator
    def cal_stock_base_info(self):
        """
        è®¡ç®—è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œæ±‡æ€»è¡¨ï¼Œåç§°ã€ç¼–ç ã€æ¿å—ã€è‚¡æœ¬ã€å¸‚å€¼ã€å‡€èµ„äº§
        Returns:
        """

        #  1.è·å–æ—¥æœŸ
        ymd = DateUtility.today()
        # ymd = "20241122"

        # 2.å®šä¹‰ SQL æ¨¡æ¿
        sql_statements_template = [
            """
            DELETE  FROM quant.dwd_ashare_stock_base_info WHERE  ymd = '{ymd}';
            """,
            """
            insert IGNORE  into quant.dwd_ashare_stock_base_info 
            select 
                  tkline.ymd                                         
                 ,tpbe.stock_code                                    
                 ,tpbe.stock_name                                    
                 ,tkline.close                                       
                 ,tpbe.market_value                                  
                 ,tpbe.total_capital*tkline.close   as  total_value  
                 ,tpbe.total_asset                                   
                 ,tpbe.net_asset                                     
                 ,tpbe.total_capital                                 
                 ,tpbe.float_capital                                 
                 ,tpbe.shareholder_num                               
                 ,tpbe.pb                                            
                 ,tpbe.pe                                            
                 ,texchange.market                                   
                 ,tplate.plate_names         
                 ,tconcept.plate_names             as concept_plate
                 ,tindex.plate_names               as index_plate
                 ,tindustry.plate_names            as industry_plate
                 ,tstyle.plate_names               as style_plate
                 ,tout.plate_names                 as out_plate
            from  
             ( select
                  htsc_code                                         
                 ,ymd                                               
                 ,open                                              
                 ,close                                             
                 ,high                                              
                 ,low                                               
                 ,num_trades                                        
                 ,volume                                            
              from  quant.ods_stock_kline_daily_insight   
              where ymd = (SELECT MAX(ymd) FROM quant.ods_stock_kline_daily_insight)
            ) tkline
            left join 
            ( select 
                  ymd                                                
                 ,stock_code                                         
                 ,stock_name                                         
                 ,market_value                                       
                 ,total_asset                                        
                 ,net_asset                                          
                 ,total_capital                                      
                 ,float_capital                                      
                 ,shareholder_num                                    
                 ,pb                                                 
                 ,pe                                                 
                 ,industry                                           
              from  quant.ods_tdx_stock_pepb_info 
              WHERE ymd = (SELECT MAX(ymd) FROM quant.ods_tdx_stock_pepb_info)
            ) tpbe
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tpbe.stock_code
            left join 
            ( select 
                  ymd                                               
                 ,stock_code                                        
                 ,stock_name                                        
                 ,market                                            
              from  quant.ods_stock_exchange_market 
              where ymd = (SELECT MAX(ymd) FROM quant.ods_stock_exchange_market)
            ) texchange 
            on tkline.htsc_code = texchange.stock_code
            left join 
            (
              select 
                  ymd                                              
                 ,stock_code                                       
                 ,stock_name                                       
                 ,GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
              from  quant.dwd_stock_a_total_plate  
              where ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
              group by ymd, stock_code, stock_name 
            ) tplate
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tplate.stock_code
            LEFT JOIN 
                (
                    SELECT 
                        ymd,                                              
                        stock_code,                                       
                        GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
                    FROM quant.dwd_stock_a_total_plate  
                    WHERE ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
                      AND source_table = 'ods_tdx_stock_concept_plate'
                    GROUP BY ymd, stock_code
                ) tconcept
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tconcept.stock_code
            LEFT JOIN 
                (
                    SELECT 
                        ymd,                                              
                        stock_code,                                       
                        GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
                    FROM quant.dwd_stock_a_total_plate  
                    WHERE ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
                      AND source_table = 'ods_tdx_stock_index_plate'
                    GROUP BY ymd, stock_code
                ) tindex
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tindex.stock_code
            LEFT JOIN 
                (
                    SELECT 
                        ymd,                                              
                        stock_code,                                       
                        GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
                    FROM quant.dwd_stock_a_total_plate  
                    WHERE ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
                      AND source_table = 'ods_tdx_stock_industry_plate'
                    GROUP BY ymd, stock_code
                ) tindustry
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tindustry.stock_code
            LEFT JOIN 
                (
                    SELECT 
                        ymd,                                              
                        stock_code,                                       
                        GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
                    FROM quant.dwd_stock_a_total_plate  
                    WHERE ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
                      AND source_table = 'ods_tdx_stock_style_plate'
                    GROUP BY ymd, stock_code
                ) tstyle
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tstyle.stock_code
            LEFT JOIN 
                (
                    SELECT 
                        ymd,                                              
                        stock_code,                                       
                        GROUP_CONCAT(plate_name ORDER BY plate_name SEPARATOR ',') AS plate_names   
                    FROM quant.dwd_stock_a_total_plate  
                    WHERE ymd = (SELECT MAX(ymd) FROM quant.dwd_stock_a_total_plate)
                      AND source_table = 'ods_stock_plate_redbook'
                    GROUP BY ymd, stock_code
                ) tout
            ON SUBSTRING_INDEX(tkline.htsc_code, '.', 1) = tout.stock_code;
            """]

        # 3.ä¸»ç¨‹åºæ›¿æ¢ {ymd} å ä½ç¬¦
        sql_statements = [stmt.format(ymd=ymd) for stmt in sql_statements_template]

        # 4.æ‰§è¡Œ SQL
        if platform.system() == "Windows":

            mysql_utils.execute_sql_statements(
                user=local_user,
                password=local_password,
                host=local_host,
                database=local_database,
                sql_statements=sql_statements)

            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)
        else:
            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)


    @timing_decorator
    def cal_ZT_DT(self):
        """
        è®¡ç®—ä¸€åªè‚¡ç¥¨æ˜¯å¦ æ¶¨åœ / è·Œåœ
        Returns:
        """

        # 1.ç¡®å®šèµ·æ­¢æ—¥æœŸ
        time_start_date = DateUtility.next_day(-7)
        time_end_date = DateUtility.next_day(0)

        # 2.è·å–èµ·æ­¢æ—¥æœŸèŒƒå›´å†…çš„æ—¥Kçº¿æ•°æ®
        df = mysql_utils.data_from_mysql_to_dataframe(user=origin_user, password=origin_password, host=origin_host,
                                                      database=origin_database,
                                                      table_name='ods_stock_kline_daily_insight',
                                                      start_date=time_start_date, end_date=time_end_date)

        if df.empty:
            # print(f"{time_start_date} - {time_end_date}æ—¥æœŸçš„Kçº¿æ•°æ®ä¸ºç©ºï¼Œç»ˆæ­¢ cal_ZT_DT è¿è¡Œï¼")
            logging.info(f"{time_start_date} - {time_end_date}æ—¥æœŸçš„Kçº¿æ•°æ®ä¸ºç©ºï¼Œç»ˆæ­¢ cal_ZT_DT è¿è¡Œï¼")
            return

        df = df.rename(columns={'htsc_code': 'stock_code'})

        # æŒ‰ç…§ ymd æ’åºï¼Œç¡®ä¿æ•°æ®æ˜¯æŒ‰æ—¥æœŸæ’åˆ—çš„
        latest_15_days = df.sort_values(by=['stock_code', 'ymd'])

        # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼Œç„¶åå¯¹æ¯ä¸ªåˆ†ç»„è¿›è¡Œ shift(1) æ“ä½œ, è®¡ç®—æ˜¨æ—¥close
        latest_15_days['last_close'] = latest_15_days.groupby('stock_code')['close'].shift(1)

        # è¿‡æ»¤æ‰æ²¡æœ‰æ˜¨æ—¥æ•°æ®çš„è¡Œ
        latest_15_days = latest_15_days.dropna(subset=['last_close'])

        if latest_15_days.empty:
            # print(f"{time_start_date} - {time_end_date}æ—¥æœŸçš„æ—¥æœŸå·®å€¼æ—¶é—´ä¸ºç©ºï¼Œç»ˆæ­¢ cal_ZT_DT è¿è¡Œï¼")
            logging.info(f"{time_start_date} - {time_end_date}æ—¥æœŸçš„æ—¥æœŸå·®å€¼æ—¶é—´ä¸ºç©ºï¼Œç»ˆæ­¢ cal_ZT_DT è¿è¡Œï¼")
            return

        # è·å–å¸‚åœºç‰¹å¾
        stock_market_init = mysql_utils.data_from_mysql_to_dataframe_latest(
            user=origin_user, password=origin_password, host=origin_host,
            database=origin_database, table_name='dwd_ashare_stock_base_info')

        stock_base_info = stock_market_init[['stock_code', 'stock_name', 'market_value', 'total_value',
                                           'total_asset', 'net_asset', 'total_capital', 'float_capital',
                                           'shareholder_num', 'pb', 'pe', 'market', 'plate_names']]


        # åˆå¹¶å¸‚åœºä¿¡æ¯åˆ°æœ€æ–°çš„15å¤©æ•°æ®
        latest_15_days['stock_code'] = latest_15_days['stock_code'].str.split('.').str[0]

        latest_15_days = latest_15_days[['ymd', 'stock_code', 'close', 'last_close']]

        latest_15_days = pd.merge(latest_15_days, stock_base_info, on='stock_code', how='left', suffixes=('_latest', '_base'))

        def calculate_ZT_DT(row):
            if row['market'] in ['åˆ›ä¸šæ¿', 'ç§‘åˆ›æ¿']:
                up_limit = row['last_close'] * 1.20
                down_limit = row['last_close'] * 0.80
            else:  # ä¸Šæµ·ä¸»æ¿ã€æ·±åœ³ä¸»æ¿
                up_limit = row['last_close'] * 1.10
                down_limit = row['last_close'] * 0.90
            return pd.Series([up_limit, down_limit])  # ç¡®ä¿è¿”å›ä¸¤ä¸ªå€¼

        # åº”ç”¨è®¡ç®—
        latest_15_days[['æ˜¨æ—¥ZTä»·', 'æ˜¨æ—¥DTä»·']] = latest_15_days.apply(calculate_ZT_DT, axis=1, result_type='expand')

        def ZT_DT_orz(price, target_price):
            # å¦‚æœ price å’Œ target_price ä¹‹é—´çš„å·®è·å°äºç­‰äº0.01ï¼Œæ‰è¿›ä¸€æ­¥è®¡ç®—
            if abs(target_price - price) <= 0.01:
                # è®¡ç®— price å‘¨å›´ 0.01 èŒƒå›´å†…çš„æœ€æ¥è¿‘çš„2ä¸ªä»·æ ¼
                left_price = price - 0.01
                right_price = price + 0.01

                # ç®—ä»·å·®
                left_delta = abs(left_price - target_price)
                mid_delta = abs(price - target_price)
                right_delta = abs(right_price - target_price)
                min_delta = min(left_delta, mid_delta, right_delta)

                # åˆ¤æ–­ä¸ºZT or DT
                if mid_delta == min_delta:
                    return True

            # ä¸å¯èƒ½ ZT or DT
            return False

        # 3. åˆ¤æ–­æ¯æ—¥çš„æ¶¨åœæˆ–è·Œåœ
        latest_15_days['æ˜¯å¦æ¶¨åœ'] = latest_15_days.apply(
            lambda row: ZT_DT_orz(row['close'], row['æ˜¨æ—¥ZTä»·']), axis=1)
        latest_15_days['æ˜¯å¦è·Œåœ'] = latest_15_days.apply(
            lambda row: ZT_DT_orz(row['close'], row['æ˜¨æ—¥DTä»·']), axis=1)

        # 4. ç­›é€‰å‡ºæ¶¨åœå’Œè·Œåœçš„è®°å½•ï¼Œåˆ†åˆ«å­˜å…¥ä¸¤ä¸ª DataFrame
        zt_records = latest_15_days[latest_15_days['æ˜¯å¦æ¶¨åœ'] == True].copy()
        zt_records['rate'] = ((zt_records['close'] - zt_records['last_close']) / zt_records['last_close'] * 100).round(2)
        zt_df = zt_records[
            ['ymd', 'stock_code', 'stock_name', 'last_close', 'close', 'rate', 'market_value', 'total_value',
             'total_asset', 'net_asset', 'total_capital', 'float_capital', 'shareholder_num', 'pb', 'pe',
             'market', 'plate_names']]
        zt_df = zt_df.sort_values(by=['ymd', 'stock_code'])

        dt_records = latest_15_days[latest_15_days['æ˜¯å¦è·Œåœ'] == True].copy()
        dt_records['rate'] = ((dt_records['close'] - dt_records['last_close']) / dt_records['last_close'] * 100).round(2)
        dt_df = dt_records[
            ['ymd', 'stock_code', 'stock_name', 'last_close', 'close', 'rate', 'market_value', 'total_value',
             'total_asset', 'net_asset', 'total_capital', 'float_capital', 'shareholder_num', 'pb', 'pe',
             'market', 'plate_names']]
        dt_df = dt_df.sort_values(by=['ymd', 'stock_code'])

        ############################   æ–‡ä»¶è¾“å‡ºæ¨¡å—     ############################
        if platform.system() == "Windows":
            #  æ¶¨åœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=zt_df,
                                                     table_name="dwd_stock_zt_list",
                                                     merge_on=['ymd', 'stock_code'])

            #  æ¶¨åœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=zt_df,
                                                     table_name="dwd_stock_zt_list",
                                                     merge_on=['ymd', 'stock_code'])

            #  è·Œåœæ•°æ®ä¿å­˜åˆ° æœ¬åœ° mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=local_user,
                                                     password=local_password,
                                                     host=local_host,
                                                     database=local_database,
                                                     df=dt_df,
                                                     table_name="dwd_stock_dt_list",
                                                     merge_on=['ymd', 'stock_code'])

            #  è·Œåœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=dt_df,
                                                     table_name="dwd_stock_dt_list",
                                                     merge_on=['ymd', 'stock_code'])
        else:
            #  æ¶¨åœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=zt_df,
                                                     table_name="dwd_stock_zt_list",
                                                     merge_on=['ymd', 'stock_code'])

            #  è·Œåœæ•°æ®ä¿å­˜åˆ° è¿œç«¯ mysqlä¸­
            mysql_utils.data_from_dataframe_to_mysql(user=origin_user,
                                                     password=origin_password,
                                                     host=origin_host,
                                                     database=origin_database,
                                                     df=dt_df,
                                                     table_name="dwd_stock_dt_list",
                                                     merge_on=['ymd', 'stock_code'])


    def setup(self):

        # èšåˆè‚¡ç¥¨çš„æ¿å—ï¼ŒæŠŠå„ä¸ªæ¿å—æ•°æ®èšåˆåœ¨ä¸€èµ·
        self.cal_ashare_plate()

        # è®¡ç®—è‚¡ç¥¨æ‰€å½’å±çš„äº¤æ˜“æ‰€ï¼Œåˆ¤æ–­å…¶æ˜¯ä¸»åŠã€åˆ›ä¸šæ¿ã€ç§‘åˆ›æ¿ã€åŒ—äº¤æ‰€ç­‰ç­‰
        self.cal_stock_exchange()

        # è®¡ç®—è‚¡ç¥¨åŸºç¡€ä¿¡æ¯ï¼Œæ±‡æ€»è¡¨ï¼Œåç§°ã€ç¼–ç ã€æ¿å—ã€è‚¡æœ¬ã€å¸‚å€¼ã€å‡€èµ„äº§
        self.cal_stock_base_info()

        # è®¡ç®—ä¸€åªè‚¡ç¥¨æ˜¯å¦ æ¶¨åœ / è·Œåœ
        self.cal_ZT_DT()


if __name__ == '__main__':
    save_insight_data = CalDWD()
    save_insight_data.setup()
```

--------------------------------------------------------------------------------
## datas_prepare\C04_data_MART\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C04_data_MART\calculate_MART_datas.py

```python
# -*- coding: utf-8 -*-

import os
from datetime import datetime

import pandas as pd
from sqlalchemy import create_engine, text
import time
import platform
import logging


# import dataprepare_properties
# import dataprepare_utils
from CommonProperties import Base_Properties
import CommonProperties.Base_utils as base_utils
from CommonProperties.DateUtility import DateUtility
from CommonProperties.Base_utils import timing_decorator

import CommonProperties.Mysql_Utils as mysql_utils

from CommonProperties import set_config

# ************************************************************************
# æœ¬ä»£ç çš„ä½œç”¨æ˜¯ä¸‹åˆæ”¶ç›˜åé’ˆå¯¹ insight è¡Œæƒ…æºæ•°æ®çš„æœ¬åœ°ä¿å­˜éƒ¨åˆ†å¼€å±•merge
# éœ€è¦ä¸‹è½½çš„æ•°æ®:
# 1.ä¸Šå¸‚è‚¡ç¥¨ä»£ç 
# 2.ç­¹ç åˆ†å¸ƒæ•°æ®   get_chouma_datas()


# ************************************************************************
#  è°ƒç”¨æ—¥å¿—é…ç½®
set_config.setup_logging_config()

######################  mysql é…ç½®ä¿¡æ¯  æœ¬åœ°å’Œè¿œç«¯æœåŠ¡å™¨  ####################
local_user = Base_Properties.local_mysql_user
local_password = Base_Properties.local_mysql_password
local_database = Base_Properties.local_mysql_database
local_host = Base_Properties.local_mysql_host

origin_user = Base_Properties.origin_mysql_user
origin_password = Base_Properties.origin_mysql_password
origin_database = Base_Properties.origin_mysql_database
origin_host = Base_Properties.origin_mysql_host


class CalDMART:

    def __init__(self):
        pass


    # @timing_decorator
    def cal_zt_details(self):
        """
        æ¶¨åœè‚¡ç¥¨çš„æ˜ç»†
        Returns:
        """
        #  1.è·å–æ—¥æœŸ
        # ymd = DateUtility.today()
        time_start_date = DateUtility.next_day(-2)
        time_start_date = '20250215'

        time_end_date = DateUtility.next_day(0)
        time_end_date = '20250319'

        # 2.å®šä¹‰ SQL æ¨¡æ¿
        sql_statements_template = [
            """
            DELETE FROM quant.dmart_stock_zt_details WHERE ymd>= '{time_start_date}' and ymd <= '{time_end_date}';
            """,
            """
            INSERT IGNORE INTO quant.dmart_stock_zt_details
            SELECT 
                tzt.ymd
               ,tzt.stock_code
               ,tzt.stock_name
               ,tbase.concept_plate
               ,tbase.index_plate
               ,tbase.industry_plate
               ,tbase.style_plate
               ,tbase.out_plate
            FROM 
                quant.dwd_stock_zt_list                     tzt
            LEFT JOIN 
                (
                    SELECT 
                        t2.ymd
                       ,t2.stock_code
                       ,t2.concept_plate
                       ,t2.index_plate
                       ,t2.industry_plate
                       ,t2.style_plate
                       ,t2.out_plate
                    FROM 
                        quant.dwd_ashare_stock_base_info    t2
                    INNER JOIN 
                        (
                            SELECT 
                                MAX(ymd) AS latest_ymd
                            FROM 
                                quant.dwd_ashare_stock_base_info
                        ) latest 
                    ON t2.ymd = latest.latest_ymd
                ) tbase
            ON     tzt.stock_code = tbase.stock_code
            WHERE  tzt.ymd >= '{time_start_date}'  AND  tzt.ymd <= '{time_end_date}'  ;
            """
        ]

        # 3.ä¸»ç¨‹åºæ›¿æ¢ {ymd} å ä½ç¬¦
        sql_statements = [stmt.format(time_start_date=time_start_date, time_end_date=time_end_date) for stmt in sql_statements_template]

        # 4.æ‰§è¡Œ SQL
        if platform.system() == "Windows":

            # mysql_utils.execute_sql_statements(
            #     user=local_user,
            #     password=local_password,
            #     host=local_host,
            #     database=local_database,
            #     sql_statements=sql_statements)

            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)
        else:
            mysql_utils.execute_sql_statements(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                sql_statements=sql_statements)

    def cal_zt_details_explode(self):
        """
        æ¶¨åœè‚¡ç¥¨çš„æ˜ç»†çš„æ‹†åˆ†
        Returns:
        """
        # 1. è·å–æ—¥æœŸèŒƒå›´
        time_start_date = DateUtility.next_day(-2)  # 2å¤©å‰çš„æ—¥æœŸ
        time_start_date = '20241126'

        time_end_date = DateUtility.next_day(0)  # å½“å‰æ—¥æœŸ
        time_end_date = '20250318'

        logging.info(f"å¼€å§‹å¤„ç†æ¶¨åœè‚¡ç¥¨æ˜ç»†æ•°æ®ï¼Œæ—¥æœŸèŒƒå›´ï¼š{time_start_date} è‡³ {time_end_date}")

        # 2. ä» MySQL è·å–èµ·æ­¢æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®
        df = mysql_utils.data_from_mysql_to_dataframe(
            user=origin_user,
            password=origin_password,
            host=origin_host,
            database=origin_database,
            table_name='dmart_stock_zt_details',
            start_date=time_start_date,
            end_date=time_end_date,
            cols=['ymd', 'stock_code', 'stock_name', 'concept_plate', 'index_plate', 'industry_plate', 'style_plate',
                  'out_plate']
        )

        if df.empty:
            logging.warning("æœªè·å–åˆ°æ•°æ®ï¼Œå¯èƒ½æ—¥æœŸèŒƒå›´å†…æ²¡æœ‰æ•°æ®æˆ–è¡¨ä¸ºç©ºã€‚")
            return

        logging.info(f"æˆåŠŸè·å–åˆ° {len(df)} æ¡æ•°æ®ï¼Œå¼€å§‹æ‹†è§£å¤„ç†...")

        # 3. å®šä¹‰ unpack_plates å‡½æ•°
        def unpack_plates(df):
            result = []
            for _, row in df.iterrows():
                ymd = row['ymd']
                stock_code = row['stock_code']
                stock_name = row['stock_name']

                # è·å–æ¯ä¸ªå­—æ®µçš„åˆ†éš”å€¼
                fields = {
                    'concept_plate': row['concept_plate'].split(',') if pd.notna(row['concept_plate']) else [],
                    'index_plate': row['index_plate'].split(',') if pd.notna(row['index_plate']) else [],
                    'industry_plate': row['industry_plate'].split(',') if pd.notna(row['industry_plate']) else [],
                    'style_plate': row['style_plate'].split(',') if pd.notna(row['style_plate']) else [],
                    'out_plate': row['out_plate'].split(',') if pd.notna(row['out_plate']) else []
                }

                # æ‰¾åˆ°åˆ†éš”å€¼æœ€å¤šçš„å­—æ®µ
                max_length = max(len(fields[field]) for field in fields)

                # æŒ‰æœ€å¤§é•¿åº¦å¡«å……æ•°æ®
                for i in range(max_length):
                    result_row = {
                        'ymd': ymd.strftime('%Y%m%d'),
                        'stock_code': stock_code,
                        'stock_name': stock_name,
                        'concept_plate': fields['concept_plate'][i].strip() if i < len(
                            fields['concept_plate']) else None,
                        'index_plate': fields['index_plate'][i].strip() if i < len(fields['index_plate']) else None,
                        'industry_plate': fields['industry_plate'][i].strip() if i < len(
                            fields['industry_plate']) else None,
                        'style_plate': fields['style_plate'][i].strip() if i < len(fields['style_plate']) else None,
                        'out_plate': fields['out_plate'][i].strip() if i < len(fields['out_plate']) else None
                    }
                    result.append(result_row)

            return pd.DataFrame(result)

        # 4. è°ƒç”¨ unpack_plates å‡½æ•°å¤„ç†æ•°æ®
        output_df = unpack_plates(df)

        # 5. å°†å¤„ç†åçš„æ•°æ®ä¿å­˜åˆ° MySQL
        if platform.system() == "Windows":
            mysql_utils.data_from_dataframe_to_mysql(
                user=local_user,
                password=local_password,
                host=local_host,
                database=local_database,
                df=output_df,
                table_name="dmart_stock_zt_details_expanded",
                merge_on=['ymd', 'stock_code', 'concept_plate', 'index_plate', 'industry_plate', 'style_plate',
                          'out_plate']
            )
            logging.info(
                f"æ•°æ®å¤„ç†å®Œæˆï¼Œå·²å°†ç»“æœä¿å­˜åˆ° {local_host} çš„ {local_database}.dmart_stock_zt_details_expanded è¡¨ä¸­ã€‚")

            mysql_utils.data_from_dataframe_to_mysql(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                df=output_df,
                table_name="dmart_stock_zt_details_expanded",
                merge_on=['ymd', 'stock_code', 'concept_plate', 'index_plate', 'industry_plate', 'style_plate',
                          'out_plate']
            )
            logging.info(
                f"æ•°æ®å¤„ç†å®Œæˆï¼Œå·²å°†ç»“æœä¿å­˜åˆ° {origin_host} çš„ {origin_database}.dmart_stock_zt_details_expanded è¡¨ä¸­ã€‚")


        else:
            mysql_utils.data_from_dataframe_to_mysql(
                user=origin_user,
                password=origin_password,
                host=origin_host,
                database=origin_database,
                df=output_df,
                table_name="dmart_stock_zt_details_expanded",
                merge_on=['ymd', 'stock_code', 'concept_plate', 'index_plate', 'industry_plate', 'style_plate',
                          'out_plate']
            )
            logging.info(
                f"æ•°æ®å¤„ç†å®Œæˆï¼Œå·²å°†ç»“æœä¿å­˜åˆ° {origin_host} çš„ {origin_database}.dmart_stock_zt_details_expanded è¡¨ä¸­ã€‚")



    def setup(self):

        # æ¶¨åœè‚¡ç¥¨çš„æ˜ç»†
        self.cal_zt_details()

        self.cal_zt_details_explode()


if __name__ == '__main__':
    cal_dmart_data = CalDMART()
    cal_dmart_data.setup()




```

--------------------------------------------------------------------------------
## datas_prepare\C06_data_transfer\__init__.py

```python

```

--------------------------------------------------------------------------------
## datas_prepare\C06_data_transfer\get_example_tables.py

```python
import os
import pandas as pd
import logging
from datetime import datetime
from sqlalchemy import create_engine, text
from pathlib import Path  # æ–°å¢ï¼šå¯¼å…¥Pathç”¨äºè·¯å¾„å¤„ç†
import CommonProperties.Base_Properties as Base_Properties
from CommonProperties.set_config import setup_logging_config

# é…ç½®æ—¥å¿—
setup_logging_config()
logger = logging.getLogger(__name__)


class TableDataExporterFull:
    """å¯¼å‡ºæ•°æ®åº“è¡¨æ•°æ®æ ·ä¾‹åˆ°å•ä¸ªæ–‡ä»¶ - æ˜¾ç¤ºå®Œæ•´æ•°æ®"""

    def __init__(self):
        # ä½¿ç”¨æ‚¨çš„MySQLé…ç½®
        self.user = Base_Properties.origin_mysql_user
        self.password = Base_Properties.origin_mysql_password
        self.host = Base_Properties.origin_mysql_host
        self.database = Base_Properties.origin_mysql_database

        # ====================== æ ¸å¿ƒä¼˜åŒ–ï¼šç²¾å‡†æ¨å¯¼ Quant/Others/output è·¯å¾„ ======================
        # 1. è·å–å½“å‰è„šæœ¬ï¼ˆexport_table_samples_full.pyï¼‰çš„ç»å¯¹è·¯å¾„
        current_script_path = Path(__file__).resolve()

        # 2. å‘ä¸Šè¿½æº¯æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½• Quant/ï¼ˆå…³é”®ï¼šåŸºäº CommonProperties ç›®å½•åå‘å®šä½ï¼Œæ›´ç¨³å®šï¼‰
        # æ–¹æ¡ˆ1ï¼šé€šè¿‡ CommonProperties ç›®å½•ï¼ˆé¡¹ç›®ä¸­å›ºå®šå­˜åœ¨ï¼‰å®šä½ Quant/ï¼ˆæ¨èï¼Œå…¼å®¹æ€§æ›´å¼ºï¼‰
        current_dir = current_script_path.parent
        project_root = None
        # å‘ä¸Šéå†ç›®å½•ï¼Œç›´åˆ°æ‰¾åˆ°åŒ…å« CommonProperties çš„ç›®å½•ï¼ˆå³ Quant/ï¼‰
        while current_dir != current_dir.parent:
            if (current_dir / "CommonProperties").exists():
                project_root = current_dir
                break
            current_dir = current_dir.parent

        # æ–¹æ¡ˆ2ï¼šå¦‚æœè„šæœ¬ç›®å½•ç»“æ„å›ºå®šï¼Œå¯ç›´æ¥å‘ä¸Šè¿½æº¯ï¼ˆå¤‡ç”¨ï¼Œç®€æ´ä½†ä¾èµ–ç›®å½•ç»“æ„ï¼‰
        # project_root = current_script_path.parent.parent  # è‹¥è„šæœ¬åœ¨ Quant/xxx/ ä¸‹ï¼Œç›´æ¥å‘ä¸Šä¸¤çº§åˆ° Quant/

        # æ ¡éªŒé¡¹ç›®æ ¹ç›®å½•æ˜¯å¦æ‰¾åˆ°
        if not project_root or not (project_root / "CommonProperties").exists():
            raise FileNotFoundError("âŒ æœªæ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½• Quant/ï¼ˆç¼ºå°‘ CommonProperties ç›®å½•ï¼‰")

        # 3. æ„é€  Quant/Others ç›®å½•è·¯å¾„
        others_dir = project_root / "Others"

        # 4. æ„é€  Quant/Others/output ç›®å½•è·¯å¾„
        self.output_dir = others_dir / "output"

        # 5. è‡ªåŠ¨åˆ›å»º Others å’Œ output ç›®å½•ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ è‡ªåŠ¨åˆ›å»º/ç¡®è®¤è¾“å‡ºç›®å½•: {self.output_dir}")

        # 6. æ„é€ å®Œæ•´çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆæ”¾å…¥ Quant/Others/output ç›®å½•ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_filename = f"quant_tables_full_{timestamp}.txt"
        self.output_file = self.output_dir / output_filename  # Pathå¯¹è±¡ï¼Œæ”¯æŒåç»­ç›´æ¥æ“ä½œ
        # ======================================================================================

        print(f"æ•°æ®åº“é…ç½®:")
        print(f"  ä¸»æœº: {self.host}")
        print(f"  æ•°æ®åº“: {self.database}")
        print(f"  ç”¨æˆ·: {self.user}")
        print(f"  è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åˆ°: {self.output_file}")  # æ–°å¢ï¼šæç¤ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
        print("-" * 50)

    def test_connection(self):
        """æµ‹è¯•æ•°æ®åº“è¿æ¥"""
        try:
            db_url = f'mysql+pymysql://{self.user}:{self.password}@{self.host}:3306/{self.database}'
            engine = create_engine(db_url)
            with engine.connect() as connection:
                result = connection.execute(text("SELECT 1"))
                print("âœ“ æ•°æ®åº“è¿æ¥æˆåŠŸ")
                return True
        except Exception as e:
            print(f"âœ— æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
            return False

    def get_all_tables(self):
        """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰è¡¨å"""
        try:
            db_url = f'mysql+pymysql://{self.user}:{self.password}@{self.host}:3306/{self.database}'
            engine = create_engine(db_url)

            print("æ­£åœ¨è·å–è¡¨åˆ—è¡¨...")

            # ä½¿ç”¨SHOW TABLES
            with engine.connect() as connection:
                result = connection.execute(text("SHOW TABLES"))
                tables = [row[0] for row in result]

            print(f"âœ“ æ‰¾åˆ° {len(tables)} å¼ è¡¨")
            return tables

        except Exception as e:
            print(f"âœ— è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
            return []

    def get_table_info(self, table_name):
        """è·å–è¡¨çš„å®Œæ•´ä¿¡æ¯"""
        try:
            db_url = f'mysql+pymysql://{self.user}:{self.password}@{self.host}:3306/{self.database}'
            engine = create_engine(db_url)

            info = {
                'table_name': table_name,
                'structure': None,
                'sample_data': None,
                'row_count': 0,
                'column_count': 0
            }

            with engine.connect() as connection:
                # 1. è·å–è¡¨ç»“æ„
                try:
                    result = connection.execute(text(f"SHOW CREATE TABLE `{table_name}`"))
                    create_table_sql = result.fetchone()[1]
                    info['create_sql'] = create_table_sql
                except:
                    info['create_sql'] = None

                # 2. è·å–è¡¨æè¿°
                try:
                    result = connection.execute(text(f"DESCRIBE `{table_name}`"))
                    columns_info = []
                    for row in result:
                        col_info = {
                            'Field': row[0],
                            'Type': row[1],
                            'Null': row[2],
                            'Key': row[3],
                            'Default': row[4],
                            'Extra': row[5] if len(row) > 5 else ''
                        }
                        columns_info.append(col_info)
                    info['structure'] = columns_info
                    info['column_count'] = len(columns_info)
                except:
                    pass

                # 3. è·å–è¡Œæ•°
                try:
                    result = connection.execute(text(f"SELECT COUNT(*) FROM `{table_name}`"))
                    info['row_count'] = result.fetchone()[0]
                except:
                    pass

                # 4. è·å–æ ·ä¾‹æ•°æ®ï¼ˆæœ€å¤š5è¡Œï¼‰
                if info['row_count'] > 0:
                    try:
                        limit = min(5, info['row_count'])
                        query = text(f"SELECT * FROM `{table_name}` LIMIT {limit}")
                        df = pd.read_sql(query, connection)
                        info['sample_data'] = df
                    except:
                        pass

            return info

        except Exception as e:
            print(f"  è¡¨ {table_name} ä¿¡æ¯è·å–å¤±è´¥: {str(e)[:50]}...")
            return None

    def write_table_info(self, f, table_info, table_num, total_tables):
        """å†™å…¥å•ä¸ªè¡¨çš„å®Œæ•´ä¿¡æ¯åˆ°æ–‡ä»¶"""
        if not table_info:
            return

        table_name = table_info['table_name']

        f.write(f"\nã€è¡¨ {table_num}/{total_tables}ã€‘{table_name}\n")
        f.write("=" * 100 + "\n")

        # 1. åŸºæœ¬ä¿¡æ¯
        f.write(f"åŸºæœ¬ä¿¡æ¯:\n")
        f.write(f"  è¡Œæ•°: {table_info.get('row_count', 'æœªçŸ¥')}\n")
        f.write(f"  åˆ—æ•°: {table_info.get('column_count', 'æœªçŸ¥')}\n")
        f.write("\n")

        # 2. è¡¨ç»“æ„ï¼ˆå®Œæ•´ï¼‰
        if table_info.get('structure'):
            f.write("è¡¨ç»“æ„ï¼ˆå®Œæ•´ï¼‰:\n")
            f.write("-" * 80 + "\n")
            f.write(f"{'å­—æ®µå':<20} {'ç±»å‹':<20} {'å¯ç©º':<5} {'é”®':<5} {'é»˜è®¤å€¼':<15} {'é¢å¤–':<10}\n")
            f.write("-" * 80 + "\n")
            for col in table_info['structure']:
                field = col.get('Field', '')
                type_ = col.get('Type', '')
                null = col.get('Null', '')
                key = col.get('Key', '')
                default = str(col.get('Default', '')) if col.get('Default') is not None else 'NULL'
                extra = col.get('Extra', '')

                f.write(f"{field:<20} {type_:<20} {null:<5} {key:<5} {default:<15} {extra:<10}\n")
        f.write("\n")

        # 3. æ ·ä¾‹æ•°æ®ï¼ˆå®Œæ•´æ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼‰
        if table_info.get('sample_data') is not None and not table_info['sample_data'].empty:
            df = table_info['sample_data']
            f.write(f"æ•°æ®æ ·ä¾‹ï¼ˆå‰{len(df)}è¡Œï¼Œå®Œæ•´åˆ—ï¼‰:\n")
            f.write("-" * 80 + "\n")

            # æ˜¾ç¤ºæ‰€æœ‰åˆ—å
            columns = df.columns.tolist()
            f.write(f"æ‰€æœ‰åˆ—({len(columns)}ä¸ª):\n")
            for i, col in enumerate(columns, 1):
                f.write(f"  {i:2d}. {col}\n")
            f.write("\n")

            # æ˜¾ç¤ºæ•°æ®ï¼ˆè¡¨æ ¼æ ¼å¼ï¼‰
            # è®¾ç½®pandasæ˜¾ç¤ºé€‰é¡¹
            pd.set_option('display.max_columns', None)
            pd.set_option('display.width', None)
            pd.set_option('display.max_colwidth', 50)

            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
            data_str = df.to_string(index=False)

            # å¦‚æœæ•°æ®å¤ªé•¿ï¼Œåˆ†å—æ˜¾ç¤º
            if len(data_str) > 5000:
                f.write("æ•°æ®é¢„è§ˆï¼ˆå‰5000å­—ç¬¦ï¼‰:\n")
                f.write(data_str[:5000])
                f.write(f"\n... (æ•°æ®è¿‡é•¿ï¼Œå·²æˆªæ–­ï¼ŒåŸå§‹{len(data_str)}å­—ç¬¦)\n")
            else:
                f.write(data_str)
        else:
            f.write("æ•°æ®æ ·ä¾‹: è¡¨ä¸ºç©ºæˆ–æ— æ³•è¯»å–æ•°æ®\n")

        f.write("\n" * 2)

    def export_important_tables(self):
        """å¯¼å‡ºé‡è¦çš„è¡¨ï¼ˆæŒ‰å‰ç¼€ç­›é€‰ï¼‰"""
        print("å¼€å§‹å¯¼å‡ºæ•°æ®åº“è¡¨ä¿¡æ¯...")

        # æµ‹è¯•è¿æ¥
        if not self.test_connection():
            return

        # è·å–æ‰€æœ‰è¡¨
        tables = self.get_all_tables()
        if not tables:
            print("é”™è¯¯ï¼šæ•°æ®åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨")
            return

        # æŒ‰é‡è¦æ€§ç­›é€‰è¡¨ï¼ˆå…ˆå¯¼å‡ºå…³é”®è¡¨ï¼‰
        important_prefixes = ['ods_', 'dwd_', 'dmart_', 'dwt_']
        important_tables = []
        other_tables = []

        for table in tables:
            is_important = False
            for prefix in important_prefixes:
                if table.startswith(prefix):
                    important_tables.append(table)
                    is_important = True
                    break
            if not is_important:
                other_tables.append(table)

        print(f"æ‰¾åˆ° {len(tables)} å¼ è¡¨ï¼Œå…¶ä¸­:")
        print(f"  é‡è¦è¡¨ï¼ˆods/dwd/dmartï¼‰: {len(important_tables)} å¼ ")
        print(f"  å…¶ä»–è¡¨: {len(other_tables)} å¼ ")

        # è¯¢é—®ç”¨æˆ·è¦å¯¼å‡ºå“ªäº›è¡¨
        print("\nå¯¼å‡ºé€‰é¡¹:")
        print("1. åªå¯¼å‡ºé‡è¦è¡¨ï¼ˆods/dwd/dmartå¼€å¤´ï¼‰")
        print("2. å¯¼å‡ºæ‰€æœ‰è¡¨")
        print("3. å¯¼å‡ºæŒ‡å®šå‰ç¼€çš„è¡¨")

        choice = input("è¯·é€‰æ‹© (1/2/3, é»˜è®¤1): ").strip()

        if choice == '2':
            tables_to_export = important_tables + other_tables
        elif choice == '3':
            prefix = input("è¯·è¾“å…¥è¡¨å‰ç¼€ (å¦‚ ods_): ").strip()
            tables_to_export = [t for t in tables if t.startswith(prefix)]
            if not tables_to_export:
                print(f"æ²¡æœ‰ä»¥ {prefix} å¼€å¤´çš„è¡¨")
                return
        else:  # é»˜è®¤é€‰æ‹©1
            tables_to_export = important_tables

        print(f"\nå¼€å§‹å¯¼å‡º {len(tables_to_export)} å¼ è¡¨...")

        # æ³¨æ„ï¼šself.output_file æ˜¯Pathå¯¹è±¡ï¼Œopenæ—¶ä¼šè‡ªåŠ¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²è·¯å¾„ï¼Œå…¼å®¹Pythonå†…ç½®openå‡½æ•°
        with open(self.output_file, 'w', encoding='utf-8') as f:
            # å†™å…¥æ–‡ä»¶å¤´
            f.write("QUANTæ•°æ®åº“è¡¨ç»“æ„åŠæ•°æ®æ ·ä¾‹æŠ¥å‘Šï¼ˆå®Œæ•´ç‰ˆï¼‰\n")
            f.write("=" * 100 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æ•°æ®åº“: {self.database} @ {self.host}\n")
            f.write(f"æ€»è¡¨æ•°: {len(tables)}\n")
            f.write(f"æœ¬æ¬¡å¯¼å‡ºè¡¨æ•°: {len(tables_to_export)}\n")
            f.write("=" * 100 + "\n\n")

            # è¡¨ç›®å½•
            f.write("å¯¼å‡ºè¡¨ç›®å½•:\n")
            for i, table in enumerate(tables_to_export, 1):
                f.write(f"{i:3d}. {table}\n")
            f.write("\n" + "=" * 100 + "\n\n")

            # æŒ‰å‰ç¼€åˆ†ç»„å¯¼å‡º
            table_groups = {}
            for table in tables_to_export:
                if '_' in table:
                    prefix = table.split('_')[0]
                else:
                    prefix = 'å…¶ä»–'
                if prefix not in table_groups:
                    table_groups[prefix] = []
                table_groups[prefix].append(table)

            # å¯¼å‡ºæ¯ä¸ªè¡¨
            total_exported = 0
            for prefix in sorted(table_groups.keys()):
                f.write(f"\nã€{prefix.upper()}å±‚ã€‘({len(table_groups[prefix])}å¼ è¡¨)\n")
                f.write("=" * 80 + "\n\n")

                group_tables = sorted(table_groups[prefix])
                for i, table in enumerate(group_tables, 1):
                    print(f"å¤„ç†: {table} ({total_exported + 1}/{len(tables_to_export)})")

                    try:
                        # è·å–è¡¨ä¿¡æ¯
                        table_info = self.get_table_info(table)

                        if table_info:
                            # å†™å…¥æ–‡ä»¶
                            self.write_table_info(f, table_info, total_exported + 1, len(tables_to_export))
                            total_exported += 1

                    except Exception as e:
                        f.write(f"å¤„ç†è¡¨ {table} æ—¶å‡ºé”™: {str(e)[:100]}...\n\n")
                    print(f"  å®Œæˆ")

        # å®Œæˆæç¤ºï¼ˆä¼˜åŒ–ï¼šæ˜¾ç¤ºå®Œæ•´çš„è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼‰
        if self.output_file.exists():  # Pathå¯¹è±¡ç›´æ¥è°ƒç”¨exists()ï¼Œæ¯”os.path.existsæ›´ä¼˜é›…
            file_size = self.output_file.stat().st_size / 1024  # KBï¼ŒPathå¯¹è±¡ç›´æ¥è·å–æ–‡ä»¶ä¿¡æ¯
            print("\n" + "=" * 60)
            print("å¯¼å‡ºå®Œæˆï¼")
            print("=" * 60)
            print(f"è¾“å‡ºæ–‡ä»¶: {self.output_file}")
            print(f"æ–‡ä»¶å¤§å°: {file_size:.1f} KB")
            print(f"å¯¼å‡ºè¡¨æ•°: {total_exported}/{len(tables_to_export)}")
            print("=" * 60)

            # æ˜¾ç¤ºæ–‡ä»¶å†…å®¹å»ºè®®
            print("\næ–‡ä»¶å†…å®¹åŒ…å«:")
            print("1. å®Œæ•´çš„è¡¨ç»“æ„ï¼ˆæ‰€æœ‰å­—æ®µã€ç±»å‹ã€å¯ç©ºã€é»˜è®¤å€¼ç­‰ï¼‰")
            print("2. å®Œæ•´çš„æ•°æ®æ ·ä¾‹ï¼ˆæ‰€æœ‰åˆ—ï¼Œæœ€å¤š5è¡Œï¼‰")
            print("3. æ¯ä¸ªè¡¨çš„åŸºæœ¬ä¿¡æ¯ï¼ˆè¡Œæ•°ã€åˆ—æ•°ï¼‰")

            if file_size > 200:
                print(f"\nâš ï¸  æ–‡ä»¶è¾ƒå¤§ ({file_size:.1f}KB)ï¼Œå»ºè®®:")
                print("1. ç”¨Notepad++æˆ–VSCodeæ‰“å¼€æŸ¥çœ‹")
                print("2. å¯ä»¥åˆ†å¤šæ¬¡å‘é€å†…å®¹")
                print("3. æˆ–å‹ç¼©åå‘é€æ–‡ä»¶")
            else:
                print(f"\nâœ“ æ–‡ä»¶å¤§å°åˆé€‚ ({file_size:.1f}KB)ï¼Œå¯ç›´æ¥å¤åˆ¶ç²˜è´´")

            # æ˜¾ç¤ºæ–‡ä»¶å¤´
            print("\næ–‡ä»¶å¼€å¤´é¢„è§ˆ:")
            print("-" * 60)
            try:
                with open(self.output_file, 'r', encoding='utf-8') as f:
                    lines = []
                    for i in range(50):  # æ˜¾ç¤ºå‰50è¡Œ
                        line = f.readline()
                        if not line:
                            break
                        lines.append(line.rstrip())

                    for line in lines[:30]:  # åªæ˜¾ç¤ºå‰30è¡Œé¿å…å¤ªé•¿
                        if len(line) > 100:
                            print(line[:97] + "...")
                        else:
                            print(line)

                    if len(lines) > 30:
                        print("... (è¿˜æœ‰æ›´å¤šå†…å®¹)")
            except Exception as e:
                print(f"é¢„è§ˆå¤±è´¥: {str(e)}")

            print("\n" + "=" * 60)
            print("æ“ä½œè¯´æ˜:")
            print("1. æ‰“å¼€æ–‡ä»¶ï¼Œå¤åˆ¶éœ€è¦çš„å†…å®¹å‘é€ç»™æˆ‘")
            print("2. é‡è¦è¡¨ä¼˜å…ˆï¼šods_*, dwd_*, dmart_*")
            print("=" * 60)
        else:
            print("é”™è¯¯ï¼šæ–‡ä»¶æœªç”Ÿæˆ")


def main():
    """ä¸»å‡½æ•°"""
    print("QUANTæ•°æ®åº“è¡¨ç»“æ„å¯¼å‡ºå·¥å…·ï¼ˆå®Œæ•´ç‰ˆï¼‰")
    print("=" * 60)
    print("æœ¬å·¥å…·å°†å¯¼å‡ºå®Œæ•´çš„è¡¨ç»“æ„å’Œæ•°æ®")
    print("=" * 60)

    # åˆ›å»ºå¯¼å‡ºå™¨
    try:
        exporter = TableDataExporterFull()
        # å¯¼å‡ºè¡¨
        exporter.export_important_tables()
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()
```

--------------------------------------------------------------------------------
## datas_prepare\C06_data_transfer\put_df_to_mysql.py

```python

import pandas as pd
from yahoo_fin.stock_info import *

from CommonProperties.DateUtility import DateUtility
import CommonProperties.Base_Properties as base_properties
import CommonProperties.Mysql_Utils as mysql_utils
from CommonProperties.Base_utils import timing_decorator


def put_csv_to_mysql():

    #  è¯»å–csv
    # file_dir = r'F:\QDatas\vantage\USD_FX\USD_FX_2024081114.csv'
    # table_name = r'exchange_dxy_vantage'

    file_dir = r'F:\QDatas\vantage\USD_FX_detail\USD_FX_detail_2024081114.csv'
    table_name = r'exchange_rate_vantage_detail'

    df = pd.read_csv(file_dir)
    df.columns = ['name', 'ymd', 'open', 'high', 'low', 'close']

    mysql_utils.data_from_dataframe_to_mysql(df=df, table_name=table_name, database='quant')


if __name__ == "__main__":
    put_csv_to_mysql()


```

--------------------------------------------------------------------------------
## datas_prepare\C06_data_transfer\transfer_between_local_and_originMySQL.py

```python
# -*- coding: utf-8 -*-

import pandas as pd
from sqlalchemy import create_engine
import gc
from CommonProperties import Base_Properties
from CommonProperties.Base_utils import timing_decorator
import CommonProperties.Mysql_Utils as mysql_utils

local_user = Base_Properties.local_mysql_user
local_password = Base_Properties.local_mysql_password
local_database = Base_Properties.local_mysql_database
local_host = Base_Properties.local_mysql_host

origin_user = Base_Properties.origin_mysql_user
origin_password = Base_Properties.origin_mysql_password
origin_database = Base_Properties.origin_mysql_database
origin_host = Base_Properties.origin_mysql_host


@timing_decorator
def transfer_local_to_origin_mysql():
    """
    ä» æœ¬åœ° å‘ è¿œç«¯ æœåŠ¡å™¨åˆ·æ–° mysql æ•°æ®   å…¨åˆ å…¨æ’
    Returns:
    """

    local_db_url = f'mysql+pymysql://{local_user}:{local_password}@{local_host}:3306/{local_database}'
    origin_db_url = f'mysql+pymysql://{origin_user}:{origin_password}@{origin_host}:3306/{origin_database}'

    # 'stock_kline_daily_insight',

    table_all_list = ['ods_stock_code_daily_insight',
                      'ods_index_a_share_insight',
                      'ods_astock_industry_detail',
                      'ods_astock_industry_overview',
                      'ods_stock_limit_summary_insight',
                      'ods_future_inside_insight',
                      'ods_north_bound_daily',
                      'ods_shareholder_num',
                      'ods_stock_chouma_insight',
                      'ods_us_stock_daily_vantage',
                      'ods_exchange_rate_vantage_detail',
                      'ods_exchange_dxy_vantage',
                      'ods_tdx_stock_concept_plate',
                      'ods_tdx_stock_index_plate',
                      'ods_tdx_stock_industry_plate',
                      'ods_tdx_stock_region_plate',
                      'ods_tdx_stock_style_plate',
                      'ods_tdx_stock_pepb_info',
                      'ods_stock_kline_daily_insight',
                      'ods_stock_exchange_market',
                      'ods_stock_plate_redbook',
                      'dwd_stock_zt_list',
                      'dwd_stock_dt_list',
                      'dwd_stock_a_total_plate',
                      'dwd_ashare_stock_base_info'
                      ]

    table_temp_list = ['stock_chouma_insight']

    for tableName in table_temp_list:
        mysql_utils.full_replace_migrate(source_host=local_host,
                                         source_db_url=local_db_url,
                                         target_host=origin_host,
                                         target_db_url=origin_db_url,
                                         table_name=tableName)


# @timing_decorator
def transfer_origin_to_local_mysql():
    """
    ä» è¿œç«¯ å‘ æœ¬åœ° ä¸»æœºåˆ·æ–° mysql æ•°æ®   å…¨åˆ å…¨æ’
    Returns:

    """

    local_db_url = f'mysql+pymysql://{local_user}:{local_password}@{local_host}:3306/{local_database}'
    origin_db_url = f'mysql+pymysql://{origin_user}:{origin_password}@{origin_host}:3306/{origin_database}'

    table_all_list = [
        'ods_stock_code_daily_insight',
        'ods_stock_chouma_insight',
        'ods_shareholder_num',
        'ods_north_bound_daily',
        'ods_stock_exchange_market',
        'ods_tdx_stock_pepb_info',
        'ods_stock_kline_daily_insight',
        'ods_index_a_share_insight',
        'ods_future_inside_insight',
        'ods_us_stock_daily_vantage',
        'ods_exchange_rate_vantage_detail',
        'ods_exchange_dxy_vantage',
        'ods_stock_limit_summary_insight',
        'ods_astock_industry_overview',
        'ods_astock_industry_detail',
        'ods_tdx_stock_concept_plate',
        'ods_tdx_stock_region_plate',
        'ods_tdx_stock_industry_plate',
        'ods_tdx_stock_style_plate',
        'ods_tdx_stock_index_plate',
        'ods_stock_plate_redbook',
        'dwd_stock_a_total_plate',
        'dwd_ashare_stock_base_info',
        'dwd_stock_zt_list',
        'dwd_stock_dt_list',
        'dmart_stock_zt_details',
        'dmart_stock_zt_details_expanded'
    ]

    for tableName in table_all_list:
        mysql_utils.full_replace_migrate(source_host=origin_host,
                                         source_db_url=origin_db_url,
                                         target_host=local_host,
                                         target_db_url=local_db_url,
                                         table_name=tableName)

@timing_decorator
def append_origin_to_local_mysql():
    """
    ä» è¿œç«¯ å‘ æœ¬åœ° æœåŠ¡å™¨åˆ·æ–° mysql æ•°æ®   è¿½åŠ å½¢å¼
    Returns:
    """

    table_all_list = ['ods_stock_code_daily_insight',
                      'ods_index_a_share_insight',
                      'ods_astock_industry_detail',
                      'ods_astock_industry_overview',
                      'ods_stock_limit_summary_insight',
                      'ods_future_inside_insight',
                      'ods_north_bound_daily',
                      'ods_shareholder_num',
                      'ods_stock_chouma_insight',
                      'ods_us_stock_daily_vantage',
                      'ods_exchange_rate_vantage_detail',
                      'ods_exchange_dxy_vantage',
                      'ods_tdx_stock_concept_plate',
                      'ods_tdx_stock_index_plate',
                      'ods_tdx_stock_industry_plate',
                      'ods_tdx_stock_region_plate',
                      'ods_tdx_stock_style_plate',
                      'ods_tdx_stock_pepb_info',
                      'ods_stock_kline_daily_insight',
                      'ods_stock_exchange_market',
                      'ods_stock_plate_redbook',
                      'dwd_stock_zt_list',
                      'dwd_stock_dt_list',
                      'dwd_stock_a_total_plate',
                      'dwd_ashare_stock_base_info',
                      'dmart_stock_zt_details'
                      ]

    #  è®¾ç½®èµ·æ­¢æ—¶é—´ï¼Œä»source_table ä¸­æ‹‰å–æ•°æ®
    # start_date = '2024-12-11'
    # end_date = '2025-01-02'

    start_date = '2025-01-03'
    end_date = '2025-02-22'

    for tableName in table_all_list:
        sourceTable = tableName
        targetTable = tableName

        mysql_utils.cross_server_upsert_ymd(source_user=origin_user,
                                            source_password=origin_password,
                                            source_host=origin_host,
                                            source_database=origin_database,
                                            target_user=local_user,
                                            target_password=local_password,
                                            target_host=local_host,
                                            target_database=local_database,
                                            source_table=sourceTable,
                                            target_table=targetTable,
                                            start_date=start_date,
                                            end_date=end_date)

        # mysql_utils.cross_server_upsert_all(source_user=origin_user,
        #                                     source_password=origin_password,
        #                                     source_host=origin_host,
        #                                     source_database=origin_database,
        #                                     target_user=local_user,
        #                                     target_password=local_password,
        #                                     target_host=local_host,
        #                                     target_database=local_database,
        #                                     source_table=sourceTable,
        #                                     target_table=targetTable)


@timing_decorator
def append_local_to_origin_mysql():
    """
    ä» æœ¬åœ° å‘ è¿œç«¯ æœåŠ¡å™¨åˆ·æ–° mysql æ•°æ®   è¿½åŠ å½¢å¼
    Returns:
    """

    table_all_list = ['ods_stock_code_daily_insight',
                      'ods_index_a_share_insight',
                      'ods_astock_industry_detail',
                      'ods_astock_industry_overview',
                      'ods_stock_limit_summary_insight',
                      'ods_future_inside_insight',
                      'ods_north_bound_daily',
                      'ods_shareholder_num',
                      'ods_stock_chouma_insight',
                      'ods_us_stock_daily_vantage',
                      'ods_exchange_rate_vantage_detail',
                      'ods_exchange_dxy_vantage',
                      'ods_tdx_stock_concept_plate',
                      'ods_tdx_stock_index_plate',
                      'ods_tdx_stock_industry_plate',
                      'ods_tdx_stock_region_plate',
                      'ods_tdx_stock_style_plate',
                      'ods_tdx_stock_pepb_info',
                      'ods_stock_kline_daily_insight',
                      'ods_stock_exchange_market',
                      'ods_stock_plate_redbook',
                      'dwd_stock_zt_list',
                      'dwd_stock_dt_list',
                      'dwd_stock_a_total_plate',
                      'dwd_ashare_stock_base_info'
                      ]

    #  è®¾ç½®èµ·æ­¢æ—¶é—´ï¼Œä»source_table ä¸­æ‹‰å–æ•°æ®
    start_date = '2024-10-01'
    end_date = '2024-11-04'

    for tableName in table_all_list:
        sourceTable = tableName
        targetTable = tableName

        mysql_utils.cross_server_upsert_ymd(source_user=local_user,
                                            source_password=local_password,
                                            source_host=local_host,
                                            source_database=local_database,
                                            target_user=origin_user,
                                            target_password=origin_password,
                                            target_host=origin_host,
                                            target_database=origin_database,
                                            source_table=sourceTable,
                                            target_table=targetTable,
                                            start_date=start_date,
                                            end_date=end_date)

        # mysql_utils.cross_server_upsert_all(source_user=local_user,
        #                                     source_password=local_password,
        #                                     source_host=local_host,
        #                                     source_database=local_database,
        #                                     target_user=origin_user,
        #                                     target_password=origin_password,
        #                                     target_host=origin_host,
        #                                     target_database=origin_database,
        #                                     source_table=sourceTable,
        #                                     target_table=targetTable)


if __name__ == "__main__":
    #  ä» æœ¬åœ° å¾€ è¿œç«¯  msyqlè¿ç§»æ•°æ®          å…¨åˆ å…¨æ’   æ…é‡ä½¿ç”¨
    # transfer_local_to_origin_mysql()

    #  ä» è¿œç«¯ å¾€ æœ¬åœ°  msyqlè¿ç§»æ•°æ®          å…¨åˆ å…¨æ’   æ…é‡ä½¿ç”¨
    transfer_origin_to_local_mysql()

    #  ä» è¿œç«¯ å‘ æœ¬åœ° æœåŠ¡å™¨åˆ·æ–° mysql æ•°æ®    è¿½åŠ å½¢å¼
    # append_origin_to_local_mysql()

    #  ä» æœ¬åœ° å‘ è¿œç«¯ æœåŠ¡å™¨åˆ·æ–° mysql æ•°æ®    è¿½åŠ å½¢å¼
    # append_local_to_origin_mysql()

```

--------------------------------------------------------------------------------
## monitor\__init__.py

```python
from .realtime_monitor import RealtimeMonitor
from .alert_system import AlertSystem

__all__ = [
    'RealtimeMonitor',
    'AlertSystem'
]
```

--------------------------------------------------------------------------------
## monitor\alert_system.py

```python
import logging
import smtplib
from email.mime.text import MIMEText
from email.header import Header
from CommonProperties.Base_Properties import (
    smtp_host, smtp_port, smtp_user, smtp_password, alert_receivers
)
from CommonProperties.Base_utils import timing_decorator

logger = logging.getLogger(__name__)

# è¡¥å……é‚®ä»¶é…ç½®ï¼ˆå¦‚æœä½ çš„Base_Propertiesæ²¡æœ‰ï¼‰
if not 'smtp_host' in locals():
    smtp_host = "smtp.qq.com"
    smtp_port = 465
    smtp_user = "your_email@qq.com"
    smtp_password = "your_auth_code"
    alert_receivers = ["your_receiver@qq.com"]


class AlertSystem:
    """
    é¢„è­¦é€šçŸ¥ç³»ç»Ÿï¼š
    1. é‚®ä»¶é¢„è­¦
    2. æ—¥å¿—é¢„è­¦
    3. é¢„è­¦è®°å½•æŒä¹…åŒ–
    """

    def __init__(self):
        # é‚®ä»¶é…ç½®ï¼ˆä»Base_Propertiesè¯»å–ï¼‰
        self.smtp_host = smtp_host
        self.smtp_port = smtp_port
        self.smtp_user = smtp_user
        self.smtp_password = smtp_password
        self.receivers = alert_receivers  # é¢„è­¦æ¥æ”¶äººé‚®ç®±åˆ—è¡¨

    @timing_decorator
    def send_email_alert(self, alert_title, alert_content):
        """å‘é€é‚®ä»¶é¢„è­¦"""
        try:
            # æ„å»ºé‚®ä»¶å†…å®¹
            msg = MIMEText(alert_content, 'plain', 'utf-8')
            msg['From'] = Header("é‡åŒ–ç­–ç•¥ç›‘æ§ç³»ç»Ÿ", 'utf-8')
            msg['To'] = Header(",".join(self.receivers), 'utf-8')
            msg['Subject'] = Header(alert_title, 'utf-8')

            # å‘é€é‚®ä»¶
            smtp_obj = smtplib.SMTP_SSL(self.smtp_host, self.smtp_port)
            smtp_obj.login(self.smtp_user, self.smtp_password)
            smtp_obj.sendmail(self.smtp_user, self.receivers, msg.as_string())
            smtp_obj.quit()

            logger.info(f"âœ… é¢„è­¦é‚®ä»¶å‘é€æˆåŠŸ | æ ‡é¢˜ï¼š{alert_title} | æ¥æ”¶äººï¼š{self.receivers}")
            return True
        except Exception as e:
            logger.error(f"âŒ é¢„è­¦é‚®ä»¶å‘é€å¤±è´¥ï¼š{str(e)}")
            return False

    @timing_decorator
    def generate_alert_report(self, factor_alerts, position_alerts, price_alerts):
        """ç”Ÿæˆé¢„è­¦æ±‡æ€»æŠ¥å‘Š"""
        report = f"""
# ğŸš¨ é‡åŒ–ç­–ç•¥é¢„è­¦æŠ¥å‘Š
## ğŸ•’ ç”Ÿæˆæ—¶é—´ï¼š{logging.Formatter('%(asctime)s').formatTime(logging.LogRecord('', 0, '', 0, '', (), ()))}

### ğŸ” å› å­ä¿¡å·å˜åŒ–é¢„è­¦
{self._format_factor_alerts(factor_alerts)}

### ğŸ“‰ æŒä»“å›æ’¤é¢„è­¦
{self._format_position_alerts(position_alerts)}

### ğŸ“ˆ ä»·æ ¼æ³¢åŠ¨é¢„è­¦
{self._format_price_alerts(price_alerts)}

## âš ï¸ å¤„ç†å»ºè®®
1. å› å­ä¿¡å·å˜åŒ–ï¼šæ£€æŸ¥å› å­è®¡ç®—é€»è¾‘æ˜¯å¦å¼‚å¸¸
2. æŒä»“å›æ’¤è¶…é™ï¼šè€ƒè™‘æ­¢æŸæˆ–å‡ä»“
3. ä»·æ ¼æ³¢åŠ¨è¶…é™ï¼šæ ¸å®å¸‚åœºæ¶ˆæ¯ï¼Œç¡®è®¤æ˜¯å¦è°ƒä»“
        """
        return report

    def _format_factor_alerts(self, factor_alerts):
        """æ ¼å¼åŒ–å› å­é¢„è­¦"""
        if not factor_alerts:
            return "æ— å› å­ä¿¡å·å˜åŒ–é¢„è­¦"

        alert_text = []
        for alert in factor_alerts:
            alert_text.append(
                f"- {alert['stock_code']} | {alert['factor_type']}å› å­ | "
                f"ä¿¡å·å˜åŒ–ï¼š{alert['prev_signal']} â†’ {alert['curr_signal']} | "
                f"æ—¶é—´ï¼š{alert['change_time']}"
            )
        return "\n".join(alert_text)

    def _format_position_alerts(self, position_alerts):
        """æ ¼å¼åŒ–æŒä»“é¢„è­¦"""
        if not position_alerts:
            return "æ— æŒä»“å›æ’¤é¢„è­¦"

        alert_text = []
        for alert in position_alerts:
            alert_text.append(
                f"- {alert['stock_code']} | å›æ’¤ï¼š{alert['drawdown_rate']}% | "
                f"æˆæœ¬ï¼š{alert['cost_price']} | å½“å‰ï¼š{alert['current_price']}"
            )
        return "\n".join(alert_text)

    def _format_price_alerts(self, price_alerts):
        """æ ¼å¼åŒ–ä»·æ ¼é¢„è­¦"""
        if not price_alerts:
            return "æ— ä»·æ ¼æ³¢åŠ¨é¢„è­¦"

        alert_text = []
        for alert in price_alerts:
            alert_text.append(
                f"- {alert['stock_code']} | ä»·æ ¼å˜åŒ–ï¼š{alert['price_change']}% | "
                f"æ˜¨æ—¥ï¼š{alert['prev_price']} | ä»Šæ—¥ï¼š{alert['curr_price']}"
            )
        return "\n".join(alert_text)

    @timing_decorator
    def trigger_alert(self, alert_type, alert_data):
        """
        è§¦å‘é¢„è­¦
        :param alert_type: é¢„è­¦ç±»å‹ï¼ˆfactor/position/price/allï¼‰
        :param alert_data: é¢„è­¦æ•°æ®ï¼ˆå­—å…¸/åˆ—è¡¨ï¼‰
        """
        if alert_type == 'factor':
            title = "ã€é‡åŒ–ç­–ç•¥ã€‘å› å­ä¿¡å·å˜åŒ–é¢„è­¦"
            content = self._format_factor_alerts(alert_data)
        elif alert_type == 'position':
            title = "ã€é‡åŒ–ç­–ç•¥ã€‘æŒä»“å›æ’¤è¶…é™é¢„è­¦"
            content = self._format_position_alerts(alert_data)
        elif alert_type == 'price':
            title = "ã€é‡åŒ–ç­–ç•¥ã€‘ä»·æ ¼æ³¢åŠ¨è¶…é™é¢„è­¦"
            content = self._format_price_alerts(alert_data)
        elif alert_type == 'all':
            title = "ã€é‡åŒ–ç­–ç•¥ã€‘é¢„è­¦æ±‡æ€»æŠ¥å‘Š"
            content = self.generate_alert_report(
                alert_data.get('factor', []),
                alert_data.get('position', []),
                alert_data.get('price', [])
            )
        else:
            logger.warning(f"æœªçŸ¥é¢„è­¦ç±»å‹ï¼š{alert_type}")
            return False

        # æ—¥å¿—é¢„è­¦ + é‚®ä»¶é¢„è­¦
        logger.warning(f"\n{title}\n{content}")
        return self.send_email_alert(title, content)

```

--------------------------------------------------------------------------------
## monitor\realtime_monitor.py

```python
import time
import logging
from datetime import datetime, timedelta
from CommonProperties import Mysql_Utils
from CommonProperties.Base_utils import timing_decorator
from Others.strategy.factor_library import FactorLibrary

logger = logging.getLogger(__name__)


class RealtimeMonitor:
    """
    å®æ—¶ç›‘æ§æ¨¡å—ï¼š
    1. ç›‘æ§å› å­ä¿¡å·å˜åŒ–
    2. ç›‘æ§æŒä»“æ”¶ç›Š/å›æ’¤
    3. ç›‘æ§è‚¡ç¥¨æ± ä»·æ ¼æ³¢åŠ¨
    """

    def __init__(self, backtest_engine, stock_codes):
        self.engine = backtest_engine  # å›æµ‹å¼•æ“å®ä¾‹
        self.stock_codes = stock_codes  # ç›‘æ§è‚¡ç¥¨æ± 
        self.factor_lib = FactorLibrary()
        self.alert_thresholds = {
            'max_drawdown': 0.1,  # æœ€å¤§å›æ’¤é¢„è­¦é˜ˆå€¼ï¼ˆ10%ï¼‰
            'pb_change': 0.2,  # PBå› å­å˜åŒ–é¢„è­¦ï¼ˆ20%ï¼‰
            'price_change': 0.05  # ä»·æ ¼æ³¢åŠ¨é¢„è­¦ï¼ˆ5%ï¼‰
        }

    @timing_decorator
    def monitor_factor_signals(self):
        """ç›‘æ§å› å­ä¿¡å·å®æ—¶å˜åŒ–"""
        logger.info("======= å¼€å§‹ç›‘æ§å› å­ä¿¡å· =======")
        current_date = datetime.now().strftime('%Y%m%d')
        factor_changes = []

        for code in self.stock_codes:
            # æŸ¥è¯¢å½“æ—¥å› å­ä¿¡å·
            pb_signal = self.engine.get_factor_value(code, datetime.now().date(), 'pb')
            zt_signal = self.engine.get_factor_value(code, datetime.now().date(), 'zt')
            shareholder_signal = self.engine.get_factor_value(code, datetime.now().date(), 'shareholder')

            # æŸ¥è¯¢æ˜¨æ—¥å› å­ä¿¡å·ï¼ˆå¯¹æ¯”å˜åŒ–ï¼‰
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y%m%d')
            pb_signal_prev = self.engine.get_factor_value(code, (datetime.now() - timedelta(days=1)).date(), 'pb')

            # å› å­ä¿¡å·å˜åŒ–åˆ¤æ–­
            pb_changed = pb_signal != pb_signal_prev
            if pb_changed:
                factor_changes.append({
                    'stock_code': code,
                    'factor_type': 'PB',
                    'prev_signal': pb_signal_prev,
                    'curr_signal': pb_signal,
                    'change_time': current_date
                })
                logger.warning(f"âš ï¸ {code} PBå› å­ä¿¡å·å˜åŒ–ï¼š{pb_signal_prev} â†’ {pb_signal}")

            # è¾“å‡ºå½“å‰å› å­çŠ¶æ€
            logger.info(
                f"{code} å› å­çŠ¶æ€ | PBï¼š{pb_signal} | æ¶¨åœï¼š{zt_signal} | ç­¹ç ï¼š{shareholder_signal}"
            )

        return factor_changes

    @timing_decorator
    def monitor_position_performance(self, cerebro):
        """ç›‘æ§æŒä»“æ”¶ç›Š/å›æ’¤"""
        logger.info("======= å¼€å§‹ç›‘æ§æŒä»“ç»©æ•ˆ =======")
        position_alerts = []

        for data in cerebro.datas:
            code = data._name
            position = cerebro.broker.getposition(data)
            if position.size == 0:
                continue

            # è®¡ç®—æŒä»“æ”¶ç›Š/å›æ’¤
            cost_price = position.price
            current_price = data.close[0]
            profit_rate = (current_price - cost_price) / cost_price
            drawdown_rate = (cost_price - current_price) / cost_price if current_price < cost_price else 0

            # å›æ’¤é¢„è­¦
            if drawdown_rate > self.alert_thresholds['max_drawdown']:
                alert = {
                    'stock_code': code,
                    'alert_type': 'max_drawdown',
                    'drawdown_rate': round(drawdown_rate * 100, 2),
                    'cost_price': cost_price,
                    'current_price': current_price
                }
                position_alerts.append(alert)
                logger.error(
                    f"ğŸš¨ {code} å›æ’¤è¶…é™ | æˆæœ¬ï¼š{cost_price} | å½“å‰ï¼š{current_price} | å›æ’¤ï¼š{drawdown_rate * 100:.2f}%"
                )

            # è¾“å‡ºæŒä»“çŠ¶æ€
            logger.info(
                f"{code} æŒä»“çŠ¶æ€ | æˆæœ¬ï¼š{cost_price:.2f} | å½“å‰ï¼š{current_price:.2f} | "
                f"æ”¶ç›Šï¼š{profit_rate * 100:.2f}% | å›æ’¤ï¼š{drawdown_rate * 100:.2f}%"
            )

        return position_alerts

    @timing_decorator
    def monitor_price_volatility(self):
        """ç›‘æ§è‚¡ç¥¨ä»·æ ¼æ³¢åŠ¨"""
        logger.info("======= å¼€å§‹ç›‘æ§ä»·æ ¼æ³¢åŠ¨ =======")
        price_alerts = []
        current_date = datetime.now().strftime('%Y%m%d')

        for code in self.stock_codes:
            # è¯»å–å½“æ—¥/æ˜¨æ—¥ä»·æ ¼
            kline_df = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.engine.user,
                password=self.engine.password,
                host=self.engine.host,
                database=self.engine.database,
                table_name='ods_stock_kline_daily_insight',
                start_date=(datetime.now() - timedelta(days=1)).strftime('%Y%m%d'),
                end_date=current_date,
                cols=['htsc_code', 'ymd', 'close']
            )

            if len(kline_df) < 2:
                continue

            # è®¡ç®—ä»·æ ¼å˜åŒ–ç‡
            price_prev = kline_df.iloc[0]['close']
            price_curr = kline_df.iloc[1]['close']
            price_change = (price_curr - price_prev) / price_prev

            # ä»·æ ¼æ³¢åŠ¨é¢„è­¦
            if abs(price_change) > self.alert_thresholds['price_change']:
                alert = {
                    'stock_code': code,
                    'alert_type': 'price_volatility',
                    'price_change': round(price_change * 100, 2),
                    'prev_price': price_prev,
                    'curr_price': price_curr
                }
                price_alerts.append(alert)
                logger.warning(
                    f"âš ï¸ {code} ä»·æ ¼æ³¢åŠ¨è¶…é™ | æ˜¨æ—¥ï¼š{price_prev:.2f} | ä»Šæ—¥ï¼š{price_curr:.2f} | "
                    f"å˜åŒ–ï¼š{price_change * 100:.2f}%"
                )

        return price_alerts

    def run_monitor(self, cerebro, interval=3600):
        """
        å¯åŠ¨å®æ—¶ç›‘æ§
        :param cerebro: Backtrader Cerebroå®ä¾‹
        :param interval: ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤1å°æ—¶
        """
        logger.info(f"å¯åŠ¨å®æ—¶ç›‘æ§ | ç›‘æ§è‚¡ç¥¨æ± ï¼š{self.stock_codes} | é—´éš”ï¼š{interval / 3600}å°æ—¶")

        while True:
            try:
                # æ‰§è¡Œç›‘æ§
                self.monitor_factor_signals()
                self.monitor_position_performance(cerebro)
                self.monitor_price_volatility()

                # ç­‰å¾…ä¸‹ä¸€æ¬¡ç›‘æ§
                logger.info(f"ç›‘æ§å®Œæˆï¼Œç­‰å¾…{interval / 3600}å°æ—¶åç»§ç»­...\n")
                time.sleep(interval)

            except KeyboardInterrupt:
                logger.info("ç”¨æˆ·ç»ˆæ­¢ç›‘æ§")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§å¼‚å¸¸ï¼š{str(e)}")
                time.sleep(interval)


```

--------------------------------------------------------------------------------
## review\__init__.py

```python

```

--------------------------------------------------------------------------------
## review\daily_review.py

```python
import logging
import pandas as pd
from datetime import datetime, timedelta
from CommonProperties.Mysql_Utils import data_from_mysql_to_dataframe
from backtest.performance_analysis import PerformanceAnalyzer

logger = logging.getLogger(__name__)


class DailyReview:
    """
    æ¯æ—¥å¤ç›˜æ¨¡å—ï¼š
    1. å½“æ—¥äº¤æ˜“å¤ç›˜
    2. å› å­æ•ˆæœå¤ç›˜
    3. æ”¶ç›Š/é£é™©å¤ç›˜
    4. ç”Ÿæˆå¤ç›˜æŠ¥å‘Š
    """

    def __init__(self, backtest_engine, cerebro, strategy_type):
        self.engine = backtest_engine
        self.cerebro = cerebro
        self.strategy_type = strategy_type
        self.analyzer = PerformanceAnalyzer()
        self.review_date = datetime.now().date()

    def review_daily_trades(self):
        """å¤ç›˜å½“æ—¥äº¤æ˜“"""
        logger.info("======= å¼€å§‹å¤ç›˜å½“æ—¥äº¤æ˜“ =======")
        trade_data = []

        # è·å–ç­–ç•¥äº¤æ˜“è®°å½•
        strat = self.cerebro.runstrats[0][0] if self.cerebro.runstrats else None
        if not strat or not hasattr(strat, 'analyzers'):
            return "å½“æ—¥æ— äº¤æ˜“è®°å½•"

        trade_ana = strat.analyzers.trade_analyzer.get_analysis()
        if not hasattr(trade_ana, 'total') or trade_ana.total.closed == 0:
            return "å½“æ—¥æ— å®Œæˆäº¤æ˜“"

        # æå–å½“æ—¥äº¤æ˜“
        for trade in strat._trades:
            trade_date = trade.dtclose.date() if trade.dtclose else None
            if trade_date != self.review_date:
                continue

            trade_data.append({
                'stock_code': trade.data._name,
                'trade_type': 'ä¹°å…¥' if trade.size > 0 else 'å–å‡º',
                'price': trade.price,
                'size': abs(trade.size),
                'pnl': trade.pnl,
                'pnl_rate': (trade.pnl / (trade.price * abs(trade.size))) * 100,
                'trade_time': trade.dtclose.strftime('%Y-%m-%d %H:%M:%S')
            })

        if not trade_data:
            return "å½“æ—¥æ— äº¤æ˜“è®°å½•"

        # æ ¼å¼åŒ–äº¤æ˜“å¤ç›˜
        trade_df = pd.DataFrame(trade_data)
        review_text = f"""
### å½“æ—¥äº¤æ˜“æ±‡æ€»
- äº¤æ˜“æ¬¡æ•°ï¼š{len(trade_data)}
- ç›ˆåˆ©äº¤æ˜“ï¼š{len(trade_df[trade_df['pnl'] > 0])}
- äºæŸäº¤æ˜“ï¼š{len(trade_df[trade_df['pnl'] < 0])}
- æ€»ç›ˆäºï¼š{trade_df['pnl'].sum():.2f}å…ƒ

#### äº¤æ˜“æ˜ç»†
{trade_df.to_string(index=False)}
        """
        return review_text

    def review_factor_effectiveness(self):
        """å¤ç›˜å› å­æ•ˆæœ"""
        logger.info("======= å¼€å§‹å¤ç›˜å› å­æ•ˆæœ =======")
        if self.strategy_type != 'factor_driven':
            return "éå› å­é©±åŠ¨ç­–ç•¥ï¼Œè·³è¿‡å› å­å¤ç›˜"

        factor_review = []
        review_date_str = self.review_date.strftime('%Y%m%d')

        for data in self.cerebro.datas:
            code = data._name
            # æŸ¥è¯¢å½“æ—¥å› å­ä¿¡å·
            pb_signal = self.engine.get_factor_value(code, self.review_date, 'pb')
            zt_signal = self.engine.get_factor_value(code, self.review_date, 'zt')
            shareholder_signal = self.engine.get_factor_value(code, self.review_date, 'shareholder')

            # æŸ¥è¯¢å½“æ—¥æ”¶ç›Š
            position = self.cerebro.broker.getposition(data)
            if position.size == 0:
                profit_rate = 0
            else:
                profit_rate = (data.close[0] - position.price) / position.price * 100

            factor_review.append({
                'stock_code': code,
                'pb_signal': pb_signal,
                'zt_signal': zt_signal,
                'shareholder_signal': shareholder_signal,
                'profit_rate': round(profit_rate, 2),
                'is_profitable': profit_rate > 0
            })

        # å› å­æ•ˆæœç»Ÿè®¡
        factor_df = pd.DataFrame(factor_review)
        if len(factor_df) == 0:
            return "æ— å› å­æ•°æ®å¯å¤ç›˜"

        # è®¡ç®—å„å› å­ç›ˆåˆ©èƒœç‡
        pb_profit_win = len(factor_df[(factor_df['pb_signal'] == True) & (factor_df['is_profitable'] == True)])
        pb_total = len(factor_df[factor_df['pb_signal'] == True])
        pb_win_rate = (pb_profit_win / pb_total * 100) if pb_total > 0 else 0

        zt_profit_win = len(factor_df[(factor_df['zt_signal'] == True) & (factor_df['is_profitable'] == True)])
        zt_total = len(factor_df[factor_df['zt_signal'] == True])
        zt_win_rate = (zt_profit_win / zt_total * 100) if zt_total > 0 else 0

        # ç»„åˆå› å­æ•ˆæœ
        combo_profit_win = len(factor_df[
                                   (factor_df['pb_signal'] == True) &
                                   (factor_df['zt_signal'] == True) &
                                   (factor_df['shareholder_signal'] == True) &
                                   (factor_df['is_profitable'] == True)
                                   ])
        combo_total = len(factor_df[
                              (factor_df['pb_signal'] == True) &
                              (factor_df['zt_signal'] == True) &
                              (factor_df['shareholder_signal'] == True)
                              ])
        combo_win_rate = (combo_profit_win / combo_total * 100) if combo_total > 0 else 0

        review_text = f"""
### å½“æ—¥å› å­æ•ˆæœå¤ç›˜
#### å•å› å­ç›ˆåˆ©èƒœç‡
- PBå› å­ï¼š{pb_win_rate:.2f}%ï¼ˆ{pb_profit_win}/{pb_total}ï¼‰
- æ¶¨åœå› å­ï¼š{zt_win_rate:.2f}%ï¼ˆ{zt_profit_win}/{zt_total}ï¼‰

#### ç»„åˆå› å­ç›ˆåˆ©èƒœç‡
- PB+æ¶¨åœ+ç­¹ç ï¼š{combo_win_rate:.2f}%ï¼ˆ{combo_profit_win}/{combo_total}ï¼‰

#### å› å­ä¿¡å·ä¸æ”¶ç›Šæ˜ç»†
{factor_df.to_string(index=False)}
        """
        return review_text

    def review_risk_return(self):
        """å¤ç›˜å½“æ—¥æ”¶ç›Š/é£é™©"""
        logger.info("======= å¼€å§‹å¤ç›˜æ”¶ç›Šé£é™© =======")
        # è·å–è´¦æˆ·æ•´ä½“çŠ¶æ€
        total_cash = self.cerebro.broker.getcash()
        total_value = self.cerebro.broker.getvalue()
        total_position = total_value - total_cash

        # è®¡ç®—å½“æ—¥æ”¶ç›Š
        prev_date = self.review_date - timedelta(days=1)
        prev_value = self._get_historical_portfolio_value(prev_date)
        daily_return = (total_value - prev_value) / prev_value * 100 if prev_value > 0 else 0

        # è®¡ç®—æœ€å¤§å›æ’¤
        strat = self.cerebro.runstrats[0][0] if self.cerebro.runstrats else None
        max_drawdown = 0
        if strat and hasattr(strat.analyzers, 'drawdown'):
            max_drawdown = strat.analyzers.drawdown.get_analysis().get('max', {}).get('drawdown', 0)

        review_text = f"""
### å½“æ—¥æ”¶ç›Š/é£é™©å¤ç›˜
- è´¦æˆ·æ€»èµ„äº§ï¼š{total_value:.2f}å…ƒ
- æŒä»“å¸‚å€¼ï¼š{total_position:.2f}å…ƒ
- å¯ç”¨ç°é‡‘ï¼š{total_cash:.2f}å…ƒ
- å½“æ—¥æ”¶ç›Šï¼š{daily_return:.2f}%
- ç´¯è®¡æœ€å¤§å›æ’¤ï¼š{max_drawdown:.2f}%

#### é£é™©æç¤º
{self._generate_risk_tips(daily_return, max_drawdown)}
        """
        return review_text

    def _get_historical_portfolio_value(self, date):
        """è·å–å†å²è´¦æˆ·ä»·å€¼ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…éœ€ä»æ•°æ®åº“è¯»å–ï¼‰"""
        # æ­¤å¤„ä¸ºæ¨¡æ‹Ÿé€»è¾‘ï¼Œå®é™…éœ€å°†æ¯æ—¥è´¦æˆ·ä»·å€¼æŒä¹…åŒ–åˆ°æ•°æ®åº“
        date_str = date.strftime('%Y%m%d')
        try:
            # è¯»å–å½“æ—¥æ”¶ç›˜åè´¦æˆ·ä»·å€¼
            value_df = data_from_mysql_to_dataframe(
                user=self.engine.user,
                password=self.engine.password,
                host=self.engine.host,
                database=self.engine.database,
                table_name='ods_portfolio_daily_value',
                start_date=date_str,
                end_date=date_str,
                cols=['date', 'total_value']
            )
            return value_df['total_value'].iloc[0] if not value_df.empty else self.cerebro.broker.getvalue()
        except Exception:
            return self.cerebro.broker.getvalue()

    def _generate_risk_tips(self, daily_return, max_drawdown):
        """ç”Ÿæˆé£é™©æç¤º"""
        tips = []
        if daily_return < -5:
            tips.append("âš ï¸ å½“æ—¥äºæŸè¶…è¿‡5%ï¼Œå»ºè®®æ£€æŸ¥ç­–ç•¥é€»è¾‘æˆ–æš‚æ—¶å‡ä»“")
        if max_drawdown > 20:
            tips.append("âš ï¸ ç´¯è®¡æœ€å¤§å›æ’¤è¶…è¿‡20%ï¼Œç­–ç•¥é£é™©è¿‡é«˜ï¼Œéœ€ä¼˜åŒ–")
        if daily_return > 5:
            tips.append("âœ… å½“æ—¥æ”¶ç›Šè¶…è¿‡5%ï¼Œç­–ç•¥è¡¨ç°ä¼˜ç§€ï¼Œæ³¨æ„æ­¢ç›ˆ")
        if not tips:
            tips.append("ğŸ“Œ å½“æ—¥æ”¶ç›Š/é£é™©å¤„äºæ­£å¸¸èŒƒå›´ï¼Œç»§ç»­è§‚å¯Ÿ")
        return "\n".join(tips)

    def generate_daily_review_report(self):
        """ç”Ÿæˆå®Œæ•´çš„æ¯æ—¥å¤ç›˜æŠ¥å‘Š"""
        logger.info(f"======= ç”Ÿæˆ{self.review_date}å¤ç›˜æŠ¥å‘Š =======")

        # æ•´åˆå„éƒ¨åˆ†å¤ç›˜å†…å®¹
        report = f"""
# ğŸ“Š é‡åŒ–ç­–ç•¥æ¯æ—¥å¤ç›˜æŠ¥å‘Š
## ğŸ•’ å¤ç›˜æ—¥æœŸï¼š{self.review_date}
## ğŸ¯ ç­–ç•¥ç±»å‹ï¼š{self.strategy_type}

### 1. å½“æ—¥äº¤æ˜“å¤ç›˜
{self.review_daily_trades()}

### 2. å› å­æ•ˆæœå¤ç›˜
{self.review_factor_effectiveness()}

### 3. æ”¶ç›Š/é£é™©å¤ç›˜
{self.review_risk_return()}

### 4. ç­–ç•¥ä¼˜åŒ–å»ºè®®
{self.analyzer._generate_strategy_suggestion({
            'å¹´åŒ–æ”¶ç›Šç‡': daily_return * 252,  # å¹´åŒ–å½“æ—¥æ”¶ç›Š
            'æœ€å¤§å›æ’¤': max_drawdown,
            'èƒœç‡': self._get_daily_win_rate(),
            'ç›ˆäºæ¯”': self._get_daily_pl_ratio()
        })}

### 5. æ˜æ—¥æ“ä½œå»ºè®®
{self._generate_tomorrow_suggestion()}
        """
        # ä¿å­˜å¤ç›˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        self._save_report(report)
        logger.info("âœ… æ¯æ—¥å¤ç›˜æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report

    def _get_daily_win_rate(self):
        """è·å–å½“æ—¥èƒœç‡"""
        strat = self.cerebro.runstrats[0][0] if self.cerebro.runstrats else None
        if not strat or not hasattr(strat.analyzers, 'trade_analyzer'):
            return 0
        trade_ana = strat.analyzers.trade_analyzer.get_analysis()
        total = trade_ana.total.closed
        won = trade_ana.won.total if hasattr(trade_ana, 'won') else 0
        return (won / total * 100) if total > 0 else 0

    def _get_daily_pl_ratio(self):
        """è·å–å½“æ—¥ç›ˆäºæ¯”"""
        strat = self.cerebro.runstrats[0][0] if self.cerebro.runstrats else None
        if not strat or not hasattr(strat.analyzers, 'trade_analyzer'):
            return 0
        trade_ana = strat.analyzers.trade_analyzer.get_analysis()
        avg_win = trade_ana.won.pnl.average if hasattr(trade_ana, 'won') else 0
        avg_loss = abs(trade_ana.lost.pnl.average) if hasattr(trade_ana, 'lost') else 1
        return avg_win / avg_loss

    def _generate_tomorrow_suggestion(self):
        """ç”Ÿæˆæ˜æ—¥æ“ä½œå»ºè®®"""
        suggestions = []
        # åŸºäºå½“æ—¥å› å­ä¿¡å·
        for data in self.cerebro.datas:
            code = data._name
            pb_signal = self.engine.get_factor_value(code, self.review_date, 'pb')
            zt_signal = self.engine.get_factor_value(code, self.review_date, 'zt')
            shareholder_signal = self.engine.get_factor_value(code, self.review_date, 'shareholder')

            if pb_signal and zt_signal and shareholder_signal:
                suggestions.append(f"âœ… {code} å› å­ä¿¡å·å…¨éƒ¨æ»¡è¶³ï¼Œå»ºè®®ç»§ç»­æŒæœ‰")
            elif not pb_signal:
                suggestions.append(f"âš ï¸ {code} PBå› å­ä¿¡å·å¤±æ•ˆï¼Œå»ºè®®å…³æ³¨ä¼°å€¼å˜åŒ–")
            elif not zt_signal:
                suggestions.append(f"âš ï¸ {code} æ¶¨åœå› å­ä¿¡å·å¤±æ•ˆï¼Œå»ºè®®å…³æ³¨èµ„é‡‘åŠ¨å‘")

        if not suggestions:
            suggestions.append("ğŸ“Œ æ— æ˜ç¡®æ“ä½œå»ºè®®ï¼Œå»ºè®®ç»´æŒå½“å‰ä»“ä½")
        return "\n".join(suggestions)

    def _save_report(self, report):
        """ä¿å­˜å¤ç›˜æŠ¥å‘Šåˆ°æ–‡ä»¶"""
        report_path = f"review_reports/daily_review_{self.review_date}.md"
        try:
            import os
            os.makedirs(os.path.dirname(report_path), exist_ok=True)
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(report)
            logger.info(f"å¤ç›˜æŠ¥å‘Šå·²ä¿å­˜è‡³ï¼š{report_path}")
        except Exception as e:
            logger.error(f"ä¿å­˜å¤ç›˜æŠ¥å‘Šå¤±è´¥ï¼š{str(e)}")


```

--------------------------------------------------------------------------------
## strategy\__init__.py

```python
from Others.strategy.factor_library import FactorLibrary

__all__ = ['FactorLibrary']
```

--------------------------------------------------------------------------------
## strategy\factor_library.py

```python
# strategy/factor_library.py
import pandas as pd
import logging
from CommonProperties import Mysql_Utils
from CommonProperties.Base_utils import timing_decorator, convert_ymd_format

logger = logging.getLogger(__name__)


class FactorLibrary:
    """å› å­è®¡ç®—åº“ï¼šåŸºäºç°æœ‰MySQLæ•°æ®è®¡ç®—PB/æ¶¨åœ/ç­¹ç ç­‰å› å­ï¼ˆæ”¯æŒå¤šæ—¥å›æµ‹ï¼‰"""

    def __init__(self):
        # å¤ç”¨MySQLé…ç½®
        self.user = Mysql_Utils.origin_user
        self.password = Mysql_Utils.origin_password
        self.host = Mysql_Utils.origin_host
        self.database = Mysql_Utils.origin_database

    @timing_decorator
    def pb_factor(self, start_date, end_date, pb_percentile=0.3):
        """
        è®¡ç®—PBå› å­ï¼šä¸ºæ—¥æœŸèŒƒå›´å†…çš„æ¯ä¸€å¤©è®¡ç®—PBä¿¡å·

        è¿”å›:
            DataFrame: ymd, stock_code, pb, pb_signal
        """
        try:
            # ä»DWDå±‚è¯»å–PBæ•°æ®
            pb_df = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.user,
                password=self.password,
                host=self.host,
                database=self.database,
                table_name='dwd_ashare_stock_base_info',
                start_date=start_date,
                end_date=end_date,
                cols=['ymd', 'stock_code', 'pb']
            )

            if pb_df.empty:
                logger.warning(f"PBå› å­æ•°æ®ä¸ºç©º: {start_date}~{end_date}")
                return pd.DataFrame(columns=['ymd', 'stock_code', 'pb', 'pb_signal'])

            # æ•°æ®é¢„å¤„ç†
            pb_df = convert_ymd_format(pb_df, 'ymd')
            pb_df = pb_df.dropna(subset=['pb'])

            # è½¬æ¢pbåˆ—ä¸ºæ•°å€¼ç±»å‹
            try:
                pb_df['pb'] = pd.to_numeric(pb_df['pb'], errors='coerce')
            except:
                pb_df['pb'] = pb_df['pb'].astype(str).str.extract(r'([\d\.]+)')[0].astype(float)

            pb_df = pb_df.dropna(subset=['pb'])

            # æŒ‰æ—¥è®¡ç®—åˆ†ä½æ•°ï¼Œæ ‡è®°ä½PBè‚¡ç¥¨
            # æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªdataframe
            result_dfs = []

            # æŒ‰æ—¥æœŸåˆ†ç»„å¤„ç†
            pb_df['ymd_dt'] = pd.to_datetime(pb_df['ymd'])
            unique_dates = pb_df['ymd_dt'].unique()

            for date in unique_dates:
                date_str = date.strftime('%Y%m%d')
                date_df = pb_df[pb_df['ymd_dt'] == date].copy()

                if len(date_df) > 0:
                    pb_threshold = date_df['pb'].quantile(pb_percentile)
                    date_df['pb_signal'] = date_df['pb'] < pb_threshold
                    date_df['ymd'] = date_str

                    result_dfs.append(date_df[['ymd', 'stock_code', 'pb', 'pb_signal']])

            if result_dfs:
                result_df = pd.concat(result_dfs, ignore_index=True)
                logger.info(f"PBå› å­è®¡ç®—å®Œæˆï¼šå…±{len(result_df)}æ¡è®°å½•ï¼Œæ—¥æœŸèŒƒå›´{start_date}~{end_date}")
                return result_df
            else:
                return pd.DataFrame(columns=['ymd', 'stock_code', 'pb', 'pb_signal'])

        except Exception as e:
            logger.error(f"è®¡ç®—PBå› å­å¤±è´¥ï¼š{str(e)}")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'pb', 'pb_signal'])

    # @timing_decorator
    def zt_factor(self, start_date, end_date, lookback_days=5):
        """
        è®¡ç®—æ¶¨åœå› å­ï¼šä¸ºæ—¥æœŸèŒƒå›´å†…çš„æ¯ä¸€å¤©è®¡ç®—æ¶¨åœä¿¡å·
        è¿”å›:
            DataFrame: ymd, stock_code, zt_signal, latest_zt_date
        """
        try:
            # 1. è¯»å–æ—¥æœŸèŒƒå›´å†…çš„æ‰€æœ‰æ¶¨åœè®°å½•
            zt_df = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.user,
                password=self.password,
                host=self.host,
                database=self.database,
                table_name='dwd_stock_zt_list',
                start_date=start_date,
                end_date=end_date,
                cols=['ymd', 'stock_code']
            )

            if zt_df.empty:
                logger.warning(f"æ¶¨åœå› å­æ•°æ®ä¸ºç©º: {start_date}~{end_date}")
                # è¿”å›ç©ºDataFrameï¼Œä½†åŒ…å«æ­£ç¡®çš„åˆ—ç»“æ„
                return pd.DataFrame(columns=['ymd', 'stock_code', 'zt_signal'])

            # 2. æ•°æ®é¢„å¤„ç†
            zt_df = convert_ymd_format(zt_df, 'ymd')
            zt_df['ymd_dt'] = pd.to_datetime(zt_df['ymd'])

            # 3. è·å–éœ€è¦è®¡ç®—çš„æ‰€æœ‰æ—¥æœŸ
            start_dt = pd.to_datetime(start_date, format='%Y%m%d')
            end_dt = pd.to_datetime(end_date, format='%Y%m%d')

            # ä»PBæ•°æ®æˆ–Kçº¿æ•°æ®è·å–å®é™…äº¤æ˜“æ—¥
            # ç®€åŒ–ç‰ˆï¼šå…ˆç”Ÿæˆæ‰€æœ‰æ—¥æœŸï¼Œåç»­å¯ä»¥ä¼˜åŒ–
            all_dates = pd.date_range(start=start_dt, end=end_dt, freq='D')

            # 4. è·å–æ‰€æœ‰æœ‰æ¶¨åœè®°å½•çš„è‚¡ç¥¨
            all_zt_stocks = zt_df['stock_code'].unique()

            # 5. ä¸ºæ¯åªè‚¡ç¥¨æ„å»ºæ¶¨åœæ—¥æœŸåˆ—è¡¨
            stock_zt_dates = {}
            for stock in all_zt_stocks:
                stock_dates = zt_df[zt_df['stock_code'] == stock]['ymd_dt'].tolist()
                stock_zt_dates[stock] = sorted(stock_dates)

            # 6. è®¡ç®—æ¯æ—¥æ¶¨åœä¿¡å·
            result_data = []

            for current_date in all_dates:
                date_str = current_date.strftime('%Y%m%d')

                for stock in all_zt_stocks:
                    if stock in stock_zt_dates and stock_zt_dates[stock]:
                        # æ‰¾åˆ°å°äºç­‰äºå½“å‰æ—¥æœŸçš„æ¶¨åœè®°å½•
                        zt_dates = [d for d in stock_zt_dates[stock] if d <= current_date]

                        if zt_dates:
                            latest_zt_date = max(zt_dates)
                            days_since_zt = (current_date - latest_zt_date).days

                            # åˆ¤æ–­æ˜¯å¦åœ¨lookback_daysçª—å£å†…
                            zt_signal = 0 <= days_since_zt <= lookback_days

                            result_data.append({
                                'ymd': date_str,
                                'stock_code': stock,
                                'zt_signal': zt_signal,
                                'latest_zt_date': latest_zt_date.strftime('%Y%m%d')
                            })

            # 7. è½¬æ¢ä¸ºDataFrame
            result_df = pd.DataFrame(result_data) if result_data else pd.DataFrame(
                columns=['ymd', 'stock_code', 'zt_signal', 'latest_zt_date']
            )

            # 8. æŒ‰æ—¥æœŸå’Œè‚¡ç¥¨ä»£ç æ’åº
            result_df = result_df.sort_values(['ymd', 'stock_code']).reset_index(drop=True)

            logger.info(
                f"æ¶¨åœå› å­è®¡ç®—å®Œæˆï¼šæ—¥æœŸèŒƒå›´ {start_date}~{end_date}ï¼Œ"
                f"å…±{len(all_dates)}å¤©ï¼Œ{len(all_zt_stocks)}åªè‚¡ç¥¨æœ‰æ¶¨åœè®°å½•ï¼Œ"
                f"æ€»è®°å½•æ•°ï¼š{len(result_df)}ï¼Œ"
                f"æ¶¨åœä¿¡å·Trueå æ¯”ï¼š{result_df['zt_signal'].mean() * 100:.2f}%"
            )

            return result_df[['ymd', 'stock_code', 'zt_signal']]

        except Exception as e:
            logger.error(f"è®¡ç®—æ¶¨åœå› å­å¤±è´¥ï¼š{str(e)}")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'zt_signal'])

    @timing_decorator
    def shareholder_factor(self, start_date, end_date):
        """
        è®¡ç®—ç­¹ç å› å­ï¼šä¸ºæ—¥æœŸèŒƒå›´å†…çš„æ¯ä¸€å¤©è®¡ç®—è‚¡ä¸œæ•°ä¿¡å·

        è¿”å›:
            DataFrame: ymd, stock_code, shareholder_signal, total_sh, pct_of_total_sh
        """
        try:
            # ä»ODSå±‚è¯»å–è‚¡ä¸œæ•°æ®
            shareholder_df = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.user,
                password=self.password,
                host=self.host,
                database=self.database,
                table_name='ods_shareholder_num',
                start_date=start_date,
                end_date=end_date,
                cols=['htsc_code', 'ymd', 'total_sh', 'pct_of_total_sh']
            )

            if shareholder_df.empty:
                logger.warning(f"è‚¡ä¸œå› å­æ•°æ®ä¸ºç©º: {start_date}~{end_date}")
                return pd.DataFrame(columns=['ymd', 'stock_code', 'shareholder_signal'])

            # æ•°æ®é¢„å¤„ç†
            shareholder_df = convert_ymd_format(shareholder_df, 'ymd')
            shareholder_df.rename(columns={'htsc_code': 'stock_code'}, inplace=True)

            # æ¸…ç†è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆç§»é™¤åç¼€ï¼‰
            shareholder_df['stock_code'] = shareholder_df['stock_code'].astype(str)
            shareholder_df['stock_code'] = shareholder_df['stock_code'].str.split('.').str[0]

            # è½¬æ¢æ•°æ®ä¸ºæ•°å€¼ç±»å‹
            shareholder_df['total_sh'] = pd.to_numeric(shareholder_df['total_sh'], errors='coerce')
            shareholder_df['pct_of_total_sh'] = pd.to_numeric(shareholder_df['pct_of_total_sh'], errors='coerce')
            shareholder_df = shareholder_df.dropna(subset=['total_sh', 'pct_of_total_sh'])

            # è‚¡ä¸œæ•°ç¯æ¯”ä¸‹é™æ ‡è®°ä¸ºTrue
            shareholder_df['shareholder_signal'] = shareholder_df['pct_of_total_sh'] < 0

            # æŒ‰æ—¥æœŸæ’åº
            shareholder_df = shareholder_df.sort_values(['ymd', 'stock_code'])

            logger.info(
                f"ç­¹ç å› å­è®¡ç®—å®Œæˆï¼šå…±{len(shareholder_df)}æ¡è®°å½•ï¼Œ"
                f"è‚¡ä¸œæ•°ä¸‹é™å æ¯”ï¼š{shareholder_df['shareholder_signal'].mean() * 100:.2f}%"
            )

            return shareholder_df[['ymd', 'stock_code', 'shareholder_signal', 'total_sh', 'pct_of_total_sh']]

        except Exception as e:
            logger.error(f"è®¡ç®—ç­¹ç å› å­å¤±è´¥ï¼š{str(e)}")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'shareholder_signal'])

    @timing_decorator
    def get_stock_kline_data(self, stock_code, start_date, end_date):
        """
        è·å–è‚¡ç¥¨Kçº¿æ•°æ®ï¼ˆç”¨äºå›æµ‹ï¼‰
        ä½¿ç”¨ ods_stock_kline_daily_insight è¡¨
        """
        try:
            # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼
            stock_code_clean = stock_code.split('.')[0] if '.' in stock_code else stock_code

            # è¯»å–Kçº¿æ•°æ®
            kline_df = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.user,
                password=self.password,
                host=self.host,
                database=self.database,
                table_name='ods_stock_kline_daily_insight',
                start_date=start_date,
                end_date=end_date,
                cols=['htsc_code', 'ymd', 'open', 'high', 'low', 'close', 'volume']
            )

            if kline_df.empty:
                return pd.DataFrame()

            # è¿‡æ»¤æŒ‡å®šè‚¡ç¥¨ä»£ç 
            kline_df = kline_df[kline_df['htsc_code'].str.contains(stock_code_clean)]

            # æ•°æ®é¢„å¤„ç†
            kline_df = convert_ymd_format(kline_df, 'ymd')
            kline_df.rename(columns={'htsc_code': 'stock_code'}, inplace=True)

            return kline_df

        except Exception as e:
            logger.error(f"è·å–Kçº¿æ•°æ®å¤±è´¥ {stock_code}: {str(e)}")
            return pd.DataFrame()

    @timing_decorator
    def get_trading_days(self, start_date, end_date):
        """
        è·å–äº¤æ˜“æ—¥åˆ—è¡¨ï¼ˆä¼˜åŒ–ç‰ˆï¼‰
        """
        try:
            # ä»Kçº¿æ•°æ®ä¸­è·å–å®é™…çš„äº¤æ˜“æ—¥
            kline_dates = Mysql_Utils.data_from_mysql_to_dataframe(
                user=self.user,
                password=self.password,
                host=self.host,
                database=self.database,
                table_name='ods_stock_kline_daily_insight',
                cols=['ymd']
            )['ymd'].unique()

            # è½¬æ¢ä¸ºæ—¥æœŸæ ¼å¼
            kline_dates = pd.to_datetime(kline_dates, format='%Y%m%d')

            # ç­›é€‰æ—¥æœŸèŒƒå›´
            start_dt = pd.to_datetime(start_date, format='%Y%m%d')
            end_dt = pd.to_datetime(end_date, format='%Y%m%d')

            trading_days = sorted([d for d in kline_dates if start_dt <= d <= end_dt])

            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            trading_days_str = [d.strftime('%Y%m%d') for d in trading_days]

            logger.info(f"è·å–äº¤æ˜“æ—¥ï¼š{len(trading_days_str)}å¤©ï¼Œä»{trading_days_str[0]}åˆ°{trading_days_str[-1]}")
            return trading_days_str

        except Exception as e:
            logger.error(f"è·å–äº¤æ˜“æ—¥å¤±è´¥ï¼š{str(e)}")
            # è¿”å›æ‰€æœ‰æ—¥æœŸä½œä¸ºåå¤‡
            start_dt = pd.to_datetime(start_date, format='%Y%m%d')
            end_dt = pd.to_datetime(end_date, format='%Y%m%d')
            all_dates = pd.date_range(start=start_dt, end=end_dt, freq='D')
            return [d.strftime('%Y%m%d') for d in all_dates]

if __name__=='__main__':
    factorlib = FactorLibrary()
    res = factorlib.zt_factor(start_date='20260101', end_date='20260109')



```

--------------------------------------------------------------------------------
## strategy\strategy_engine.py

```python
# strategy/strategy_engine.py
import pandas as pd
import logging
from CommonProperties.Base_utils import timing_decorator

logger = logging.getLogger(__name__)


class StrategyEngine:
    """ç­–ç•¥å¼•æ“ï¼šæ”¯æŒå¤šæ—¥å›æµ‹çš„ç­–ç•¥æ‰§è¡Œå™¨"""

    def __init__(self, factor_lib):
        self.factor_lib = factor_lib  # æ³¨å…¥å› å­åº“å®ä¾‹
        self.strategies = {}  # å­˜å‚¨å·²æ³¨å†Œçš„ç­–ç•¥

    def register_strategy(self, name, func, params=None):
        """æ³¨å†Œç­–ç•¥"""
        self.strategies[name] = {
            'func': func,
            'params': params or {}
        }
        logger.info(f"ç­–ç•¥[{name}]æ³¨å†ŒæˆåŠŸ")

    @timing_decorator
    def value_chip_zt_strategy(self, start_date=None, end_date=None, pb_quantile=0.3, zt_window=5,
                               min_factor_count=2):
        """
        ä½PB+ç­¹ç é›†ä¸­+æ¶¨åœ ç»„åˆå› å­ç­–ç•¥ï¼ˆæ”¯æŒå¤šæ—¥å›æµ‹ï¼‰

        å‚æ•°:
            start_date: å¼€å§‹æ—¥æœŸ
            end_date: ç»“æŸæ—¥æœŸ
            pb_quantile: PBåˆ†ä½æ•°é˜ˆå€¼
            zt_window: æ¶¨åœçª—å£å¤©æ•°
            min_factor_count: æœ€å°‘æ»¡è¶³çš„å› å­æ•°é‡ï¼ˆ1-3ï¼‰
        """
        logger.info(f"å¼€å§‹æ‰§è¡Œä¸‰å› å­ç­–ç•¥ï¼š{start_date} ~ {end_date}")

        # 1. è·å–äº¤æ˜“æ—¥åˆ—è¡¨
        trading_days = self.factor_lib.get_trading_days(start_date, end_date)

        if not trading_days:
            logger.error("æ²¡æœ‰æ‰¾åˆ°äº¤æ˜“æ—¥æ•°æ®")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'stock_name'])

        all_selected = []

        # 2. æŒ‰æ¯ä¸ªäº¤æ˜“æ—¥å¤„ç†
        for i, current_date in enumerate(trading_days):
            logger.debug(f"å¤„ç†äº¤æ˜“æ—¥ {i + 1}/{len(trading_days)}: {current_date}")

            try:
                # 3. è·å–å½“æ—¥çš„å› å­æ•°æ®
                # 3.1 PBå› å­
                pb_df_day = self.factor_lib.pb_factor(
                    start_date=current_date,
                    end_date=current_date,
                    pb_percentile=pb_quantile
                )

                if pb_df_day.empty:
                    logger.warning(f"{current_date}: PBå› å­æ•°æ®ä¸ºç©º")
                    continue

                # 3.2 æ¶¨åœå› å­
                zt_df_day = self.factor_lib.zt_factor(
                    start_date=current_date,
                    end_date=current_date,
                    lookback_days=zt_window
                )

                # 3.3 ç­¹ç å› å­
                shareholder_df_day = self.factor_lib.shareholder_factor(
                    start_date=current_date,
                    end_date=current_date
                )

                # 4. åˆå¹¶å› å­æ•°æ®ï¼ˆå·¦è¿æ¥ï¼Œä»¥PBæ•°æ®ä¸ºåŸºå‡†ï¼‰
                merged = pb_df_day[['stock_code', 'pb_signal']].copy()

                # 4.1 åˆå¹¶æ¶¨åœå› å­
                if not zt_df_day.empty:
                    merged = merged.merge(
                        zt_df_day[['stock_code', 'zt_signal']],
                        on='stock_code',
                        how='left'
                    )
                else:
                    merged['zt_signal'] = False

                # 4.2 åˆå¹¶ç­¹ç å› å­
                if not shareholder_df_day.empty:
                    merged = merged.merge(
                        shareholder_df_day[['stock_code', 'shareholder_signal']],
                        on='stock_code',
                        how='left'
                    )
                else:
                    merged['shareholder_signal'] = False

                # 5. å¤„ç†ç¼ºå¤±å€¼
                merged['zt_signal'] = merged['zt_signal'].fillna(False)
                merged['shareholder_signal'] = merged['shareholder_signal'].fillna(False)

                # 6. è®¡ç®—å› å­å¾—åˆ†
                merged['factor_count'] = (
                        merged['pb_signal'].astype(int) +
                        merged['zt_signal'].astype(int) +
                        merged['shareholder_signal'].astype(int)
                )

                # 7. ç­›é€‰è‚¡ç¥¨
                selected_day = merged[merged['factor_count'] >= min_factor_count].copy()

                if not selected_day.empty:
                    # æ·»åŠ æ—¥æœŸä¿¡æ¯
                    selected_day['ymd'] = current_date

                    # æ·»åŠ è‚¡ç¥¨åç§°ï¼ˆä»PBæ•°æ®è·å–ï¼‰
                    if 'stock_name' in pb_df_day.columns:
                        stock_names = pb_df_day.set_index('stock_code')['stock_name'].to_dict()
                        selected_day['stock_name'] = selected_day['stock_code'].map(stock_names)

                    all_selected.append(selected_day[['ymd', 'stock_code', 'stock_name', 'factor_count']])

                    logger.debug(f"{current_date}: é€‰ä¸­ {len(selected_day)} åªè‚¡ç¥¨")

            except Exception as e:
                logger.error(f"å¤„ç†äº¤æ˜“æ—¥ {current_date} å¤±è´¥: {str(e)}")
                continue

        # 8. åˆå¹¶æ‰€æœ‰äº¤æ˜“æ—¥ç»“æœ
        if all_selected:
            final_result = pd.concat(all_selected, ignore_index=True)

            # ç»Ÿè®¡ä¿¡æ¯
            unique_stocks = final_result['stock_code'].nunique()
            avg_selected_per_day = len(final_result) / len(trading_days)

            logger.info(
                f"ç­–ç•¥æ‰§è¡Œå®Œæˆï¼š\n"
                f"  - å›æµ‹æœŸé—´ï¼š{start_date} ~ {end_date}ï¼Œå…±{len(trading_days)}ä¸ªäº¤æ˜“æ—¥\n"
                f"  - é€‰ä¸­è‚¡ç¥¨æ€»æ•°ï¼š{len(final_result)}æ¡è®°å½•\n"
                f"  - å”¯ä¸€è‚¡ç¥¨æ•°ï¼š{unique_stocks}åª\n"
                f"  - å¹³å‡æ¯æ—¥é€‰ä¸­ï¼š{avg_selected_per_day:.1f}åª\n"
                f"  - ç­›é€‰æ¡ä»¶ï¼šè‡³å°‘æ»¡è¶³{min_factor_count}ä¸ªå› å­"
            )

            return final_result
        else:
            logger.warning("ç­–ç•¥æœªé€‰ä¸­ä»»ä½•è‚¡ç¥¨")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'stock_name', 'factor_count'])

    @timing_decorator
    def run_strategy_combination(self, strategy_names, start_date=None, end_date=None,
                                 weight_threshold=0.5, min_factor_count=2):
        """
        å¤šç­–ç•¥åŠ æƒç»„åˆé€‰è‚¡

        å‚æ•°:
            strategy_names: ç­–ç•¥åç§°åˆ—è¡¨
            weight_threshold: æƒé‡é˜ˆå€¼
            min_factor_count: æœ€å°‘æ»¡è¶³çš„å› å­æ•°é‡
        """
        if not strategy_names:
            raise ValueError("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªç­–ç•¥")

        logger.info(f"å¼€å§‹æ‰§è¡Œç»„åˆç­–ç•¥ï¼š{strategy_names}")

        # 1. æ‰§è¡Œæ¯ä¸ªç­–ç•¥
        strategy_results = {}
        for name in strategy_names:
            if name not in self.strategies:
                raise ValueError(f"ç­–ç•¥[{name}]æœªæ³¨å†Œ")

            strat = self.strategies[name]
            logger.info(f"æ‰§è¡Œç­–ç•¥: {name}")

            # æ‰§è¡Œç­–ç•¥
            selected = strat['func'](
                start_date=start_date,
                end_date=end_date,
                min_factor_count=min_factor_count,
                **strat['params']
            )

            strategy_results[name] = selected

        # 2. åˆå¹¶ç­–ç•¥ç»“æœ
        all_dates = self.factor_lib.get_trading_days(start_date, end_date)
        combined_results = []

        for current_date in all_dates:
            date_results = []

            for strategy_name, result_df in strategy_results.items():
                # è·å–è¯¥ç­–ç•¥åœ¨å½“å‰æ—¥æœŸçš„é€‰è‚¡
                day_stocks = result_df[result_df['ymd'] == current_date]['stock_code'].tolist()

                for stock in day_stocks:
                    date_results.append({
                        'ymd': current_date,
                        'stock_code': stock,
                        'strategy_name': strategy_name
                    })

            if date_results:
                date_df = pd.DataFrame(date_results)

                # è®¡ç®—æƒé‡
                strategy_count = len(strategy_names)
                date_df['weight'] = 1.0 / strategy_count

                # æŒ‰è‚¡ç¥¨æ±‡æ€»æƒé‡
                stock_weights = date_df.groupby(['ymd', 'stock_code'])['weight'].sum().reset_index()

                # æŒ‰æƒé‡é˜ˆå€¼ç­›é€‰
                selected_stocks = stock_weights[stock_weights['weight'] >= weight_threshold]

                if not selected_stocks.empty:
                    combined_results.append(selected_stocks)

        # 3. åˆå¹¶æœ€ç»ˆç»“æœ
        if combined_results:
            final_result = pd.concat(combined_results, ignore_index=True)

            # æ·»åŠ è‚¡ç¥¨åç§°
            try:
                # ä»ä»»æ„ç­–ç•¥ç»“æœè·å–è‚¡ç¥¨åç§°
                sample_strategy = list(strategy_results.values())[0]
                stock_names = sample_strategy.drop_duplicates('stock_code').set_index('stock_code')[
                    'stock_name'].to_dict()
                final_result['stock_name'] = final_result['stock_code'].map(stock_names)
            except:
                final_result['stock_name'] = ''

            logger.info(f"ç»„åˆç­–ç•¥å®Œæˆï¼šé€‰ä¸­ {len(final_result)} åªè‚¡ç¥¨")
            return final_result[['ymd', 'stock_code', 'stock_name', 'weight']]
        else:
            logger.warning("ç»„åˆç­–ç•¥æœªé€‰ä¸­ä»»ä½•è‚¡ç¥¨")
            return pd.DataFrame(columns=['ymd', 'stock_code', 'stock_name', 'weight'])

    @timing_decorator
    def run_backtest_for_strategy(self, strategy_name, start_date, end_date,
                                  initial_cash=100000, commission=0.0003):
        """
        ä¸ºç­–ç•¥è¿è¡Œå›æµ‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        å®é™…å›æµ‹åº”è¯¥ä½¿ç”¨ä¸“é—¨çš„backtestæ¨¡å—
        """
        logger.info(f"ä¸ºç­–ç•¥ {strategy_name} è¿è¡Œå›æµ‹")

        if strategy_name not in self.strategies:
            raise ValueError(f"ç­–ç•¥[{strategy_name}]æœªæ³¨å†Œ")

        # æ‰§è¡Œç­–ç•¥è·å–é€‰è‚¡
        strat = self.strategies[strategy_name]
        selected_stocks = strat['func'](
            start_date=start_date,
            end_date=end_date,
            **strat['params']
        )

        if selected_stocks.empty:
            logger.warning("ç­–ç•¥æœªé€‰ä¸­ä»»ä½•è‚¡ç¥¨ï¼Œæ— æ³•å›æµ‹")
            return None

        # è¿™é‡Œåº”è¯¥è°ƒç”¨backtestæ¨¡å—è¿›è¡Œå®é™…å›æµ‹
        # ç›®å‰åªè¿”å›é€‰è‚¡ç»Ÿè®¡ä¿¡æ¯

        stats = {
            'strategy_name': strategy_name,
            'backtest_period': f"{start_date} ~ {end_date}",
            'total_selected': len(selected_stocks),
            'unique_stocks': selected_stocks['stock_code'].nunique(),
            'trading_days': selected_stocks['ymd'].nunique(),
            'avg_stocks_per_day': len(selected_stocks) / selected_stocks['ymd'].nunique(),
            'selected_stocks_sample': selected_stocks.head(10).to_dict('records')
        }

        logger.info(f"å›æµ‹ç»Ÿè®¡ï¼š{stats}")
        return stats
```
